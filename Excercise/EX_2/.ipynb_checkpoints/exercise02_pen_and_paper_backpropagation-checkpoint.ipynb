{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pen & Paper: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a forward and backward pass to calculate the gradients for the weights $w_0, w_1, w_2, w_s$ in the following MLP. Each node represents one unit with a weight $w_i, i \\in \\{0, 1, 2\\}$ connecting it to the previous node. The connection from unit 0 to unit 2 is called a _skip connection_, which means unit 2 receives input from two sources and thus has an additional weight $w_s$. The weighted inputs are added before the nonlinearity is applied.\n",
    "\n",
    "**![There should be an image here. If you can't see it, you probably forgot to download the mlp.png!](mlp.png)**\n",
    "\n",
    "We assume that we want to solve a regression task. We use an L1-loss $L(\\hat{y}, y) = |y - \\hat{y}|$\n",
    "\n",
    "The nonlinearities for the first two units are rectified linear functions/units (ReLU): $g_0(z) = g_1(z) = \\begin{cases} 0, z<0\\\\ z, else \\end{cases}$.\n",
    "\n",
    "We do not use a nonlinearity for the second unit: $g_2(z_2) = z_2$.\n",
    "\n",
    "**Note:** We use the notation of the Deep Learning book here, i.e. $z = Wx+b$. If you attended the Machine Learning course, you might be used to the different notation used in the Bishop Book, where $z$ denotes the value after applying the activation function. Here, $z$ is the value before applying the activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Complete the backpropagation algorithm**\n",
    "The equations of the network:\n",
    "\\begin{align*}\n",
    "   L(\\hat{y}, y) &= |y - \\hat{y}| \\\\\n",
    "    \\hat{y} &= g_2(z_2) = z_2 \\\\\n",
    "    z_2 &= w_2 \\cdot h_1 + w_S \\cdot h_0\\\\\n",
    "    h_1 &= g_1(z_1)\\\\\n",
    "    z_1 &= w_1 \\cdot h_0\\\\\n",
    "    h_0 &= g_0(z_0)\\\\\n",
    "    z_0 &= w_0 \\cdot x \\\\\n",
    "\\end{align*}\n",
    "Reuse equations/values when possible.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} &= -1 \\\\\n",
    "    \\frac{\\partial L}{\\partial z_2} &= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} = -1 \\cdot 1 = -1\\\\\n",
    "    \\frac{\\partial L}{\\partial h_1} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_1} = -1 \\cdot w_2 = -w_2\\\\\n",
    "    \\frac{\\partial L}{\\partial z_1} &= \\frac{\\partial L}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_1} =  \\begin{cases} 0, z_1<0 \\\\ -w_2, else \\end{cases} \\\\\n",
    "    \\frac{\\partial L}{\\partial h_0} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_0} + \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial h_0} = -1 \\cdot w_s  + \\frac{\\partial L}{\\partial z_1} \\cdot w_1= -w_s+ \\frac{\\partial L}{\\partial z_1} \\cdot w_1\\\\\n",
    "    \\frac{\\partial L}{\\partial z_0} &= \\frac{\\partial L}{\\partial h_0} \\cdot \\frac{\\partial h_0}{\\partial z_0} = \\begin{cases} 0, z_0<0 \\\\ -w_s, else \\end{cases} \\\\\n",
    "    \\frac{\\partial L}{\\partial w_s} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_s} = -h_0 \\\\\n",
    "    \\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_2} = -h_1\\\\\n",
    "    \\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1} =  \\frac{\\partial L}{\\partial z_1} \\cdot h_0\\\\\n",
    "    \\frac{\\partial L}{\\partial w_0} &= \\frac{\\partial L}{\\partial z_0} \\cdot \\frac{\\partial z_0}{\\partial w_0} =  \\frac{\\partial L}{\\partial z_0} \\cdot x\\\\\n",
    "\\end{align*}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: What difference does the skip connection make when propagating back the error?** \n",
    "\n",
    "Answer: The skip connection used results in additional term in the equations which helps to decrease vanishing gradient problem, especially for complex neural networks (with large number of layers).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3: Calculate the gradients for the given datapoint and the given initial weights (calculating the gradients requires to calculate a forward pass first). Also calculate the weights and the loss after one gradient descent step.**\n",
    "\n",
    "$$(x_1, y_1) = (1, -3) \\\\\n",
    "w_0 = w_1 = w_2 = w_s = 0.5 \\\\\n",
    "Learning Rate = 1 \\\\  $$\n",
    "\n",
    "Forward Pass:\n",
    "\\begin{align*}\n",
    "z_0 &= w_0 \\cdot x = 0.5 \\cdot 1 = 0.5\\\\\n",
    "h_0 &= g_0(z_0) = 0.5\\\\\n",
    "z_1 &= w_1 \\cdot h_0 = 0.5 \\cdot 0.5 = 0.25\\\\\n",
    "h_1 &= g_1(z_1) = 0.25\\\\\n",
    "z_2 &= w_2 \\cdot h_1 + w_S \\cdot h_0 = 0.5 \\cdot 0.25 + 0.5 \\cdot 0.5 = 0.375 \\\\\\\n",
    "\\hat{y} &= g_2(z_2) = 0.375 \\\\\n",
    "L(\\hat{y}, y) &= |y - \\hat{y}| = |-3 - 0.375| = 3.375 \\\\\n",
    "\\end{align*}\n",
    "Backward Pass:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} &= -1 \\\\\n",
    "    \\frac{\\partial L}{\\partial z_2} &= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} = -1 \\cdot 1 = -1\\\\\n",
    "    \\frac{\\partial L}{\\partial h_1} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_1} = -1 \\cdot w_2 = -0.5\\\\\n",
    "    \\frac{\\partial L}{\\partial z_1} &= \\frac{\\partial L}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_1} =  -0.5 \\\\\n",
    "    \\frac{\\partial L}{\\partial h_0} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_0} + \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial h_0}= -w_s+ \\frac{\\partial L}{\\partial z_1} \\cdot w_1= -0.75\\\\\n",
    "    \\frac{\\partial L}{\\partial z_0} &= \\frac{\\partial L}{\\partial h_0} \\cdot \\frac{\\partial h_0}{\\partial z_0} = -0.75 \\\\\n",
    "    \\frac{\\partial L}{\\partial w_s} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_s} = -0.5 \\\\\n",
    "    \\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_2} = -0.25\\\\\n",
    "    \\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1} =  \\frac{\\partial L}{\\partial z_1} \\cdot h_0 = 0.5 \\cdot 0.5 = -0.25\\\\\n",
    "    \\frac{\\partial L}{\\partial w_0} &= \\frac{\\partial L}{\\partial z_0} \\cdot \\frac{\\partial z_0}{\\partial w_0} =  \\frac{\\partial L}{\\partial z_0} \\cdot x = -0.75\\\\\n",
    "\\end{align*}   \n",
    "\n",
    "One gradient descent step:\n",
    "\n",
    "Learning rate $(\\epsilon) = 1$\n",
    "\n",
    "General formula: $w = w - \\epsilon \\frac{\\partial L}{\\partial w}  $\n",
    "\n",
    "Calculations:\n",
    "\n",
    "\\begin{align*}\n",
    "w_0 &= w_0 - \\frac{\\partial L}{\\partial w_0} = 0.5 + 0.75 = 1.25\\\\\n",
    "w_1 &= w_1 - \\frac{\\partial L}{\\partial w_1} = 0.5 + 0.25 = 0.75\\\\\n",
    "w_2 &= w_2 - \\frac{\\partial L}{\\partial w_2} = 0.5 + 0.25 = 0.75\\\\\n",
    "w_s &= w_s - \\frac{\\partial L}{\\partial w_s} = 0.5 + 0.5 = 1\\\\\n",
    "\\end{align*} \n",
    "\n",
    "Forward Pass to calculate the loss after 1 gradient descent step:\n",
    "\\begin{align*}\n",
    "z_0 &= w_0 \\cdot x = 1.25 \\cdot 1 = 1.25\\\\\n",
    "h_0 &= g_0(z_0) = 1.25\\\\\n",
    "z_1 &= w_1 \\cdot h_0 = 0.75 \\cdot 1.25 = 0.9375\\\\\n",
    "h_1 &= g_1(z_1) = 0.9375\\\\\n",
    "z_2 &= w_2 \\cdot h_1 + w_s \\cdot h_0 = 0.75 \\cdot 0.9375 + 1 \\cdot 1.25 = 1.95 \\\\\\\n",
    "\\hat{y} &= g_2(z_2) = 1.95 \\\\\n",
    "L(\\hat{y}, y) &= |y - \\hat{y}| = |-3 - 1.95| = 4.95 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your feedback on exercise 2.1: ** \n",
    "It was a good exercise to see backpropagation in action. It took 3 hours to solve it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
