{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itHvRwgoj6Av"
   },
   "source": [
    "# Sixth Exercise (HPO)\n",
    "\n",
    "This exercise focuses on hyperparameter optimization with neural networks.\n",
    "\n",
    "We will\n",
    "- define hyperparameter-configuration search-spaces\n",
    "- train deep learning models with various hyper parameters\n",
    "- use random search as a basic hyperparameter optimizer\n",
    "- use BOHB as an advanced hyperparameter optimizer\n",
    "\n",
    "It's in the nature of hyperparameter optimization, that you'll have to train a lot of models. \n",
    "Therefore execution time will be longer in this exercise, running the completed notebook takes about 30 minutes on a tutor's 3 year old laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENBt1a7vj6Ax"
   },
   "source": [
    "#### Install HpBandster and torchvision\n",
    "\n",
    "You need to install two more python packages for this exercise.\n",
    "\n",
    "- [torchvision](https://pytorch.org/docs/stable/torchvision/) provides utility methods for pytorch.\n",
    "- [HpBandSter](https://github.com/automl/HpBandSter) is a fast, parallel implementation of several hyperparameter optimizers.  \n",
    "   We can define even complex hyperparameter search spaces with [ConfigSpace](https://github.com/automl/ConfigSpace), which comes along HpBandSter as dependency.\n",
    "\n",
    "\n",
    "Install with anaconda/conda\n",
    "```\n",
    "conda install torchvision\n",
    "conda install hpbandster\n",
    "```\n",
    "\n",
    "or with plain python\n",
    "```\n",
    "pip3 install torchvision\n",
    "pip3 install hpbandster\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "zWrvmGu8kpE4",
    "outputId": "f23bd4aa-d1a0-4bac-ec2c-5407b2fbd461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
      "Collecting torch (from torchvision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 519.5MB 29kB/s \n",
      "tcmalloc: large alloc 1073750016 bytes == 0x58630000 @  0x7f850e65d2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
      "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 4.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Installing collected packages: torch, pillow, torchvision\n",
      "  Found existing installation: Pillow 4.0.0\n",
      "    Uninstalling Pillow-4.0.0:\n",
      "      Successfully uninstalled Pillow-4.0.0\n",
      "Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1\n",
      "Collecting hpbandster\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/3e/62192b0bb527d9353d222b4b6df14400b3c4f36a92a2b138f11a5eafe000/hpbandster-0.7.4.tar.gz (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.2MB/s \n",
      "\u001b[?25hCollecting Pyro4 (from hpbandster)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/97/d489cf7aebf77c4bfa245b26f446c5e3a1644c11ba41f871c08ed32da379/Pyro4-4.74-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 4.1MB/s \n",
      "\u001b[?25hCollecting serpent (from hpbandster)\n",
      "  Downloading https://files.pythonhosted.org/packages/55/9f/8e6b4519af9eabde9fa29e6853b865e07d89e220035f34cca914507e17e7/serpent-1.27-py2.py3-none-any.whl\n",
      "Collecting ConfigSpace (from hpbandster)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/53/e54835d444153ba461fe8aec28533a1ffc3c5531b89509c8029b6b57a7e7/ConfigSpace-0.4.7.tar.gz (913kB)\n",
      "\u001b[K    100% |████████████████████████████████| 921kB 6.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hpbandster) (1.14.6)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from hpbandster) (0.8.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hpbandster) (1.1.0)\n",
      "Collecting netifaces (from hpbandster)\n",
      "  Downloading https://files.pythonhosted.org/packages/99/9e/ca74e521d0d8dcfa07cbfc83ae36f9c74a57ad5c9269d65d1228c5369aff/netifaces-0.10.7-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace->hpbandster) (2.3.0)\n",
      "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace->hpbandster) (3.6.6)\n",
      "Collecting Cython (from ConfigSpace->hpbandster)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/e0/0592be5b851c8013aa253592606ca265862d27444d908e029fd75d563c9c/Cython-0.29.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.1MB 7.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from statsmodels->hpbandster) (0.22.0)\n",
      "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from statsmodels->hpbandster) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->statsmodels->hpbandster) (2018.7)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->statsmodels->hpbandster) (2.5.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy->statsmodels->hpbandster) (1.11.0)\n",
      "Building wheels for collected packages: hpbandster, ConfigSpace\n",
      "  Running setup.py bdist_wheel for hpbandster ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9d/57/62/6b00c8011bac96e0c404adc5be4e16964ba4544614240b4e23\n",
      "  Running setup.py bdist_wheel for ConfigSpace ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fb/05/bc/ace7b3602c315c402176a9280296a59275aa826476517faa24\n",
      "Successfully built hpbandster ConfigSpace\n",
      "Installing collected packages: serpent, Pyro4, Cython, ConfigSpace, netifaces, hpbandster\n",
      "Successfully installed ConfigSpace-0.4.7 Cython-0.29.1 Pyro4-4.74 hpbandster-0.7.4 netifaces-0.10.7 serpent-1.27\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchvision\n",
    "!pip3 install hpbandster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqru5aa9kdOb"
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBl9q5DLj6A1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "from ConfigSpace.conditions import NotEqualsCondition\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def load_mnist_minibatched(batch_size: int, n_train: int = 8192, n_valid: int = 1024,\n",
    "                           valid_test_batch_size: int = 1024) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='../data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='../data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(range(n_train))\n",
    "    validation_sampler = SubsetRandomSampler(range(n_train, n_train+n_valid))\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    validation_loader = DataLoader(dataset=train_dataset, batch_size=valid_test_batch_size,\n",
    "                                   sampler=validation_sampler)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=valid_test_batch_size, \n",
    "                                              shuffle=False)\n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model: nn.Module, data_loader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            output = model(x)\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "    accuracy = correct / len(data_loader.sampler)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJnQl-Cyj6A8"
   },
   "source": [
    "## Random Search\n",
    "\n",
    "Here we get hands on hyperparameter optimization using random search.\n",
    "\n",
    "### Model and Hyperparameter Space\n",
    "\n",
    "First we define a configurable model and a hyperparameter space. You learn how to use *ConfigSpace* to define the hyperparameters, by looking at [this example](https://automl.github.io/SMAC3/stable/quickstart.html#using-smac-in-python-svm).   \n",
    "\n",
    "**Task:** Complete the functions as described in the docstrings.\n",
    "\n",
    "**Hint:** The `CS.GreaterThanCondition(conditioned_hyperparameter, lefthand_side, righthand_side)` method might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-_12G9Yj6A-"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "  def forward(self, input):\n",
    "    self.input_shape = input.shape\n",
    "    return input.view(input.size(0), -1)\n",
    "    \n",
    "def get_conv_model(num_filters_per_layer: List[int]) -> nn.Module:\n",
    "    \"\"\"Builds a deep convolutional model with various number of convolution\n",
    "       layers for MNIST input using pytorch.\n",
    "    \n",
    "    for each element in num_filters_per_layer:\n",
    "        convolution (conv_kernel_size, num_filters, stride=1, padding=0)\n",
    "        relu\n",
    "        max pool    (pool_kernel_size, stride=1)\n",
    "    linear\n",
    "    log softmax\n",
    "    \"\"\"\n",
    "    assert len(num_filters_per_layer) > 0, \"len(num_filters_per_layer) should be greater than 0\"\n",
    "    pool_kernel_size = 2\n",
    "    conv_kernel_size = 3\n",
    "    #print(len(num_filters_per_layer))\n",
    "    # START TODO ################\n",
    "    img_size = 28\n",
    "    layers = []\n",
    "    old_num_filters = 1\n",
    "    for  num_filters in num_filters_per_layer:\n",
    "      layers += [nn.Conv2d(old_num_filters, num_filters,conv_kernel_size, stride=1, padding=0)]\n",
    "      layers += [nn.ReLU(True)]\n",
    "      layers += [nn.MaxPool2d(pool_kernel_size, stride=1)]\n",
    "      img_size = (img_size - conv_kernel_size + 1 ) - pool_kernel_size + 1\n",
    "      old_num_filters = num_filters\n",
    "      #print(f\"img_size = {img_size}\")\n",
    "    layers += [Flatten()]\n",
    "    layers += [nn.Linear(old_num_filters*img_size*img_size, 10)]\n",
    "    layers += [nn.LogSoftmax(dim=1)]\n",
    "    return nn.Sequential(*layers)\n",
    "    # End TODO ################\n",
    "\n",
    "\n",
    "def get_configspace() -> CS.ConfigurationSpace:\n",
    "    \"\"\" Define a conditional hyperparameter search-space.\n",
    "    \n",
    "    hyperparameters:\n",
    "      lr              from 1e-6 to 1e-0 (log, float)\n",
    "      num_filters_1   from    2 to    8 (int)\n",
    "      num_filters_2   from    2 to    8 (int)\n",
    "      num_conv_layers from    1 to    2 (int)\n",
    "    \n",
    "    conditions: \n",
    "      include num_filters_2 only if num_conv_layers > 1\n",
    "    \"\"\"\n",
    "    cs = CS.ConfigurationSpace()\n",
    "    # START TODO ################\n",
    "    \n",
    "    hyper_list = []\n",
    "    hyper_list.append(CSH.UniformFloatHyperparameter(name = 'lr', lower = 1e-6, upper = 1,log=True))\n",
    "    hyper_list.append(CSH.UniformIntegerHyperparameter(name = 'num_filters_1', lower = 2, upper = 8))\n",
    "    \n",
    "    num_filters_2 = CSH.UniformIntegerHyperparameter(name = 'num_filters_2', lower = 2, upper = 8)\n",
    "    hyper_list.append(num_filters_2) \n",
    "    \n",
    "    num_conv_layers = CSH.UniformIntegerHyperparameter(name = 'num_conv_layers', lower = 1, upper = 2)\n",
    "    hyper_list.append(num_conv_layers)\n",
    "    \n",
    "    cs.add_hyperparameters(hyper_list)\n",
    "    \n",
    "    use_2 = NotEqualsCondition(child=num_filters_2, parent=num_conv_layers, value=1)\n",
    "    cs.add_conditions([use_2])\n",
    "    \n",
    "    # End TODO ################\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bYBDA9h9O-7"
   },
   "outputs": [],
   "source": [
    "cs = get_configspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aP8fcoYGj6BD"
   },
   "source": [
    "### Run model with configuration\n",
    "\n",
    "A single sample from your hyperparameter space is a *Configuration*. You can use the configuration similar to a dictionary, it supports *config.keys(), config.values(), value = config[key], key in config, …* .\n",
    "You can iterate a *DataLoader* to access (data, label) batches.\n",
    "\n",
    "**Note:** If a condition isn't met, the conditional hyperparameter isn't included in the configuration.\n",
    "\n",
    "**Task:** Complete the function to run a model like defined by the configuration. The function should return the model and the *validation error* for each epoch. You can use *evaluate_accuracy* (defined above), don't forget to switch between train and eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7olxg8owj6BE"
   },
   "outputs": [],
   "source": [
    "def run_conv_model(config: CS.Configuration, epochs: int, train_loader: DataLoader,\n",
    "                   validation_loader: DataLoader) -> Tuple[nn.Module, List[float]]:    \n",
    "    \"\"\" Run and evaluate a model from get_conv_model with NLLLoss and SGD.\n",
    "    \"\"\"\n",
    "    # START TODO ################\n",
    "    # retrieve the number of filters from the config and create the model\n",
    "    model = get_conv_model([config['num_filters_1'], config['num_filters_2']] \n",
    "                            if ('num_filters_2') in config else [config['num_filters_1']] )  \n",
    "    # define loss and optimizer\n",
    "    #loss_fn = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = config['lr'])\n",
    "    \n",
    "    # train the model for `epochs` and save the validation error for each epoch in\n",
    "    val_errors = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "      model.train()\n",
    "      for batch_idx, (data, target) in enumerate(train_loader):\n",
    "         \n",
    "          optimizer.zero_grad()\n",
    "          output = model(data)\n",
    "          #print(output.shape)\n",
    "          #print(target.shape)\n",
    "          loss = F.nll_loss(output, target)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          if batch_idx % 10 == 0:\n",
    "              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                  epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                  100. * batch_idx / len(train_loader), loss.item()))\n",
    "      val_errors.append(evaluate_accuracy(model,validation_loader))\n",
    "    # End TODO ################\n",
    "    return model, val_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OX6pl5wHj6BJ"
   },
   "source": [
    "Now let's run models with various, random hyperparameter configurations. Don't forget to store all the configuration and validation errors for further evaluation.\n",
    "\n",
    "**Tipp:** *ConfigSpace* objects have a *.sample_configuration()* function to sample a random configuration.\n",
    "\n",
    "**Task:** Run *n_random_samples* models for *n_epochs* and store the tuple `(model, config, val_errors)` in `results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37893
    },
    "colab_type": "code",
    "id": "bZ2hZjjGj6BK",
    "outputId": "b1791a3a-5ae2-4594-fa6a-351cf64d9302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Configuration Sample: 0/18\n",
      "Configuration:\n",
      "  lr, Value: 0.00014246674013680232\n",
      "  num_conv_layers, Value: 2\n",
      "  num_filters_1, Value: 3\n",
      "  num_filters_2, Value: 7\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.320747\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.333996\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.318193\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.306520\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.297177\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.332345\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.315163\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.299136\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.324717\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.320172\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.330994\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.319759\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.297582\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.320140\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.322342\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.315386\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.314440\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.301311\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.307869\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.288868\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.303608\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.303707\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.307534\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.294760\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.296614\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.301705\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.313549\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.313001\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.320797\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.315674\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.309820\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.295178\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.299417\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.316744\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.297127\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.313521\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.283515\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.299188\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.304353\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.310367\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.275813\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.290930\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.307679\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.311091\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.296365\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.293811\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.294316\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.315473\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.300052\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.289853\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.300485\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.292973\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.315543\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.292053\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.305300\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.305260\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.304220\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.279288\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.303075\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.305365\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.308730\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.304892\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.307585\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.303521\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.305101\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.308009\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.294569\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.293101\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.303378\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.296952\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.299696\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.299867\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.289927\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.289360\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.303258\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.317165\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.302904\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.290936\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.309489\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.295170\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.292614\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.290626\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.293449\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.295597\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.302610\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.308290\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.306933\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.302566\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.296804\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.296182\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.301304\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.298081\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.302248\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.300790\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.304338\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.296485\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.305980\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.297017\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.286178\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.304465\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.298644\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.308643\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.301093\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.303433\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.310387\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.296793\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.309379\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.298966\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.301128\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.307073\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.296666\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.286790\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.287136\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.298640\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.303910\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.298591\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.294907\n",
      "Configuration Sample: 1/18\n",
      "Configuration:\n",
      "  lr, Value: 0.34548919590212673\n",
      "  num_conv_layers, Value: 1\n",
      "  num_filters_1, Value: 8\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.317652\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.075887\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.302172\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.309790\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.311226\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.331470\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.276160\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.278777\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.308938\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.301340\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.301387\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.287317\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.293226\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.288007\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.302784\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.280663\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.329790\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.289405\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.302862\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.293172\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.324027\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.314369\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.322360\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.296587\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.317924\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.301078\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.284287\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.318451\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.315115\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.275040\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.323299\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.305573\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.293520\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.279975\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.336731\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.310020\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.313601\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.294128\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.312376\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.322803\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.326293\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.300173\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.313591\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.265417\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.350049\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.303714\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.318642\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.328787\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.262952\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.338312\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.309579\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.296283\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.310198\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.310126\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.302137\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.335154\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.311261\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.311190\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.318475\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.285255\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.305527\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.305005\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.316435\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.300276\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.307436\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.300378\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.305228\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.309903\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.290781\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.300122\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.289350\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.316347\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.306895\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.303167\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.282420\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.327635\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.298408\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.317042\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.306729\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.312174\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.285790\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.279236\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.302635\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.335382\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.284309\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.298951\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.297971\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.321300\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.290833\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.302544\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.317789\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.297642\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.303472\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.308622\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.291829\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.323575\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.358777\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.302379\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.297289\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.297105\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.302995\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.297572\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.262075\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.324171\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.298118\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.281189\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.325981\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.320153\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.312282\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.319942\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.305081\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.282833\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.315606\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.281972\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.284099\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.306895\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.304682\n",
      "Configuration Sample: 2/18\n",
      "Configuration:\n",
      "  lr, Value: 1.6939587646805e-05\n",
      "  num_conv_layers, Value: 2\n",
      "  num_filters_1, Value: 5\n",
      "  num_filters_2, Value: 4\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.270152\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.307700\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.289034\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.306893\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.290943\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.293671\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.346686\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.297967\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.279852\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.347743\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.290960\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.277506\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.245795\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.265525\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.299086\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.286831\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.284089\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.295066\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.324234\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.281079\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.280847\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.279195\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.323372\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.315602\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.303536\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.280873\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.245781\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.265098\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.261028\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.287664\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.282206\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.272262\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.304979\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.285398\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.303827\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.322749\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.260937\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.308538\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.333219\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.280186\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.288842\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.283371\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.277122\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.258641\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.275258\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.247216\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.298306\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.326172\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.280660\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.282183\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.308611\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.278532\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.273307\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.317568\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.322491\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.260972\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.287532\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.261656\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.276508\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.247927\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.285571\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.285094\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.318384\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.242605\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.303567\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.294557\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.276850\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.301199\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.296502\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.254717\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.298271\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.300623\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.294914\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.291937\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.309694\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.280832\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.249637\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.252229\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.309678\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.316195\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.279628\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.308669\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.313873\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.267851\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.260694\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.296086\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.246764\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.249408\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.287348\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.291417\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.271994\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.310648\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.268079\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.253206\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.284898\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.276024\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.276469\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.286362\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.263299\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.235043\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.318312\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.296796\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.302663\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.300300\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.326007\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.258982\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.264860\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.300969\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.275298\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.274855\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.265304\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.267268\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.273386\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.327940\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.292352\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.281771\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.264309\n",
      "Configuration Sample: 3/18\n",
      "Configuration:\n",
      "  lr, Value: 1.732150894834876e-05\n",
      "  num_conv_layers, Value: 1\n",
      "  num_filters_1, Value: 3\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.315018\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.315310\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.329415\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.331061\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.311101\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.319432\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.310934\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.314952\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.330460\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.313826\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.316244\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.319199\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.332180\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.312952\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.304926\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.318015\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.296092\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.309340\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.309372\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.332817\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.332435\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.310704\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.302090\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.313514\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.299875\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.321430\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.319865\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.311082\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.299308\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.324452\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.310140\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.309805\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.328512\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.309085\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.325425\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.327561\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.304854\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.310908\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.311960\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.316805\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.285449\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.326373\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.333628\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.319809\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.324180\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.296420\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.311835\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.293658\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.309883\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.317601\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.325855\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.324583\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.305474\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.323139\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.324860\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.295143\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.305462\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.316885\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.306436\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.328963\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.312746\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.321479\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.313115\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.301474\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.317484\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.307647\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.329358\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.303025\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.324559\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.306067\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.321040\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.309577\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.294652\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.281532\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.321408\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.316168\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.313396\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.321877\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.323716\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.297408\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.312012\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.339014\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.327083\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.304006\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.321753\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.299455\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.317821\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.322159\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.330952\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.305338\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.319529\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.320738\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.319660\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.314584\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.328242\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.303736\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.317928\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.298488\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.325890\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.326894\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.316522\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.326383\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.300542\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.320167\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.322715\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.306066\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.308188\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.308005\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.312110\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.304149\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.309789\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.308150\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.303230\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.308972\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.296525\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.307192\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.313951\n",
      "Configuration Sample: 4/18\n",
      "Configuration:\n",
      "  lr, Value: 0.00012923541974186147\n",
      "  num_conv_layers, Value: 2\n",
      "  num_filters_1, Value: 5\n",
      "  num_filters_2, Value: 8\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.290777\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.305093\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.308099\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.307113\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.305830\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.305486\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.304186\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.306940\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.302438\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.300466\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.314013\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.292298\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.280345\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.307955\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.300251\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.303190\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.303084\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.310948\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.301521\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.300096\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.303718\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.298861\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.302452\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.292522\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.287644\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.259049\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.286709\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.301914\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.295655\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.298684\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.292837\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.290880\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.286182\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.289090\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.284566\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.301510\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.298914\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.284961\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.288911\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.293009\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.301009\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.289274\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.300448\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.282550\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.286485\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.288432\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.289753\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.294083\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.297615\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.298187\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.287309\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.273502\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.263910\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.285620\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.298648\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.308375\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.287489\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.302943\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.285798\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.284001\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.298976\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.292677\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.277829\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.289029\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.281859\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.264885\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.295544\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.286943\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.269812\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.268064\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.284613\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.273196\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.269956\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.269786\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.277444\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.285390\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.297389\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.270090\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.297207\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.279700\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.286672\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.286350\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.262417\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.281268\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.273120\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.261276\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.272307\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.282296\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.274809\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.266557\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.243737\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.261472\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.269604\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.270759\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.261283\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.291527\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.259653\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.272130\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.254423\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.255720\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.267212\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.249837\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.264411\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.250854\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.282970\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.252123\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.264259\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.262434\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.266410\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.254410\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.287707\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.261275\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.255272\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.262131\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.247444\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.287344\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.253054\n",
      "Configuration Sample: 5/18\n",
      "Configuration:\n",
      "  lr, Value: 3.0434749746506927e-05\n",
      "  num_conv_layers, Value: 1\n",
      "  num_filters_1, Value: 4\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.310349\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.294154\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.328167\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.293188\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.296853\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.300587\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.270869\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.308761\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.284330\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.327998\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.294428\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.321542\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.312832\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.312542\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.309709\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.320972\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.304433\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.293897\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.284605\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.302731\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.304284\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.307540\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.304424\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.294951\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.308082\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.308846\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.301589\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.270898\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.281937\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.300569\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.306232\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.310657\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.318598\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.296693\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.282192\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.317887\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.298509\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.309710\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.300164\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.304466\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.311929\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.300172\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.292262\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.279199\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.299393\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.289521\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.313954\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.300978\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.292408\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.300982\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.299577\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.289172\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.297726\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.308510\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.304651\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.279168\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.344252\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.282619\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.310448\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.295401\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.300694\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.291394\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.290364\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.309712\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.308585\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.310404\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.329450\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.304199\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.301264\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.302269\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.318725\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.309381\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.300379\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.316195\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.290382\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.291903\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.301626\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.295889\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.287648\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.305781\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.263400\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.296760\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.308327\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.332986\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.312037\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.328523\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.323588\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.317673\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.304931\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.303881\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.274895\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.282630\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.292535\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.306315\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.289483\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.261140\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.329094\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.260862\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.286088\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.297562\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.316163\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.293032\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.317991\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.308838\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.296332\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.301098\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.310591\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.323970\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.311521\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.289105\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.321780\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.277279\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.305963\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.300930\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.315385\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.334211\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.304349\n",
      "Configuration Sample: 6/18\n",
      "Configuration:\n",
      "  lr, Value: 1.3445511687242624e-06\n",
      "  num_conv_layers, Value: 1\n",
      "  num_filters_1, Value: 8\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.331413\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.310540\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.319652\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.295436\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.345109\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.317645\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.300170\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.317504\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.307426\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.318907\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.346919\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.303590\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.301018\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.320192\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.322767\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.304086\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.304767\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.310521\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.292825\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.319458\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.274721\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.315522\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.306717\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.279120\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.292193\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.297904\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.316519\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.335635\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.309265\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.280368\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.324115\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.313920\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.321258\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.327683\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.305448\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.277045\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.290464\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.279019\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.297523\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.303961\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.255090\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.294611\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.323742\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.277860\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.297454\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.354145\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.297614\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.324192\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.292097\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.322134\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.317578\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.313285\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.314738\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.310816\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.306628\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.265935\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.326355\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.329111\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.305267\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.309731\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.297985\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.287671\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.303468\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.300557\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.302549\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.280216\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.332576\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.276677\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.312876\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.320240\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.316774\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.301433\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.326943\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.286928\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.289719\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.314675\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.305223\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.265975\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.290863\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.306609\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.324837\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.312973\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.325020\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.301641\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.294407\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.327660\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.347923\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.278291\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.325997\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.299356\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.315036\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.300891\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.325013\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.322919\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.333231\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.320085\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.323006\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.307227\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.299591\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.304955\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.317946\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.318157\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.294550\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.317842\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.306043\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.311300\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.316319\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.316519\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.315251\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.300471\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.294845\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.309440\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.297113\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.293667\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.321226\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.312896\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.301207\n",
      "Configuration Sample: 7/18\n",
      "Configuration:\n",
      "  lr, Value: 0.03633768066215828\n",
      "  num_conv_layers, Value: 2\n",
      "  num_filters_1, Value: 2\n",
      "  num_filters_2, Value: 6\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311965\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.077205\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 1.590265\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 1.046301\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 1.149366\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 0.758437\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 0.969637\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 0.445544\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 0.528724\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 0.694626\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 0.828487\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 0.274331\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 0.347803\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.542633\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 0.325104\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 0.472973\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 0.523222\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 0.326884\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 0.203815\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 0.514802\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 0.239666\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 0.542978\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 0.240635\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 0.435021\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 0.343201\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 0.233132\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.150750\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 0.262588\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 0.350509\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 0.297498\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 0.496145\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 0.414599\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 0.070312\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 0.172216\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 0.153262\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 0.468378\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 0.260862\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 0.164853\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 0.373470\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.440125\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 0.158300\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 0.301771\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 0.397703\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 0.048749\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 0.141797\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 0.273513\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 0.087769\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 0.296216\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 0.184060\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 0.131933\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 0.173600\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 0.202601\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.135544\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 0.266394\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 0.317747\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 0.179213\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 0.153080\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 0.129066\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 0.179922\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 0.129418\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 0.292199\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 0.122507\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 0.169920\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 0.226878\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 0.254962\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.101787\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 0.154093\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 0.055906\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 0.162782\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 0.191157\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 0.312627\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 0.077430\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 0.174923\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 0.044499\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 0.226003\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 0.338211\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 0.076922\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 0.098976\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.089977\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 0.081345\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 0.111532\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 0.044245\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 0.096632\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 0.062507\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 0.149587\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 0.053080\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 0.146456\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 0.058876\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 0.241808\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 0.130175\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 0.143952\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.105478\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 0.226198\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 0.089054\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 0.056468\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 0.217421\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 0.051182\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 0.035800\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 0.143137\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 0.023461\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 0.032985\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 0.055223\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 0.069195\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 0.054523\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.099389\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 0.171469\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 0.078068\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 0.023224\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 0.299535\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 0.029737\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 0.060307\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 0.018908\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 0.029465\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 0.062635\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 0.244959\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 0.067834\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 0.185824\n",
      "Configuration Sample: 8/18\n",
      "Configuration:\n",
      "  lr, Value: 0.003965835110765956\n",
      "  num_conv_layers, Value: 1\n",
      "  num_filters_1, Value: 7\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.281689\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.225665\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.148790\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.084830\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 1.883981\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 1.833670\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 1.804126\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 1.712163\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 1.409627\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 1.385500\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 1.276965\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 1.237431\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 1.024450\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.084517\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 0.949376\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 0.983909\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 0.974231\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 0.731965\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 0.906037\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 0.583603\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 0.726467\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 0.459259\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 0.498330\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 0.694878\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 0.653292\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 0.627168\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.760179\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 0.532117\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 0.455322\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 0.485332\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 0.395921\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 0.385616\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 0.501503\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 0.420070\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 0.484455\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 0.506883\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 0.651665\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 0.434548\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 0.398235\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.401402\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 0.479412\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 0.467417\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 0.502305\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 0.506128\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 0.377491\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 0.557390\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 0.395552\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 0.331724\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 0.378892\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 0.547641\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 0.597833\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 0.325106\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.389416\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 0.206690\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 0.756957\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 0.504803\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 0.469415\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 0.436390\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 0.587640\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 0.398692\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 0.315521\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 0.499658\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 0.382057\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 0.537181\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 0.391655\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.394119\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 0.431690\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 0.580585\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 0.270380\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 0.286853\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 0.198151\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 0.286899\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 0.835808\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 0.263705\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 0.434207\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 0.196146\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 0.404350\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 0.337651\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.196523\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 0.408072\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 0.189256\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 0.273340\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 0.488068\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 0.151474\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 0.284785\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 0.361160\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 0.210216\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 0.357317\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 0.276078\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 0.155194\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 0.376050\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.150386\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 0.208692\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 0.208292\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 0.139190\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 0.283577\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 0.189934\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 0.597452\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 0.376052\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 0.330047\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 0.221046\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 0.421923\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 0.153514\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 0.298567\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.374314\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 0.191467\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 0.232801\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 0.414762\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 0.574748\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 0.362843\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 0.254838\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 0.451563\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 0.439122\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 0.257890\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 0.316261\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 0.146628\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 0.338069\n",
      "Configuration Sample: 9/18\n",
      "Configuration:\n",
      "  lr, Value: 4.987891579705589e-05\n",
      "  num_conv_layers, Value: 1\n",
      "  num_filters_1, Value: 8\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311456\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.305722\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.310397\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.330851\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.305141\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.290654\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.299259\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.294431\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.290133\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.298811\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.312416\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.286573\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.296177\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.269286\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.283588\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.306008\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.294643\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.315002\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.308570\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.284787\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.286893\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.284331\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.305259\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.291803\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.277376\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.278299\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.296721\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.281561\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.287978\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.279472\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.295287\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.309855\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.305654\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.293937\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.276099\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.297357\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.277915\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.305900\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.292184\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.291396\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.273701\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.296626\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.290503\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.288778\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.291854\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.288432\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.292328\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.298361\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.296775\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.297615\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.294842\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.288586\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.289985\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.277841\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.283515\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.280517\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.301683\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.282799\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.304294\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.287251\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.300880\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.292854\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.292574\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.287648\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.291447\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.298586\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.288344\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.285111\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.284157\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.282536\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.298217\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.283231\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.284603\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.284221\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.286735\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.285564\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.283453\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.289509\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.295599\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.302715\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.296028\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.285556\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.280402\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.287908\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.302008\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.292222\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.277368\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.296268\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.282997\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.282052\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.300416\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.276037\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.277273\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.292037\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.300170\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.277875\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.276319\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.294243\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.288539\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.301454\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.270515\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.298021\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.282672\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.278645\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.273598\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.284944\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.283784\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.262903\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.277658\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.285355\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.274772\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.276469\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.296632\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.289398\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.281794\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.291423\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.274607\n",
      "Configuration Sample: 10/18\n",
      "Configuration:\n",
      "  lr, Value: 0.5575956853160674\n",
      "  num_conv_layers, Value: 2\n",
      "  num_filters_1, Value: 6\n",
      "  num_filters_2, Value: 8\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301423\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.291321\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.301923\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.283217\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.329591\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.322545\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.309285\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.277460\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.312245\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.302674\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.293693\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.300036\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.316541\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.324195\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.307636\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.305214\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.308471\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.303044\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.278326\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.304564\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.308302\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.306148\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.306335\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.310760\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.313529\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.295090\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.308364\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.312109\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.303907\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.303041\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.308536\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.307676\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.326569\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.333978\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.276467\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.298738\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.264412\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.361061\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.295683\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.300301\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.288391\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.302768\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.328590\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.345935\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.297534\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.312737\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.329808\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.299444\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.286778\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.288738\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.305614\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.291409\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.301940\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.291652\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.295691\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.323316\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.331621\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.306314\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.295260\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.310766\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.267035\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.304507\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.318321\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.290299\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.300220\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.273197\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.312265\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.302937\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.309443\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.303559\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.282170\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.289943\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.288826\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.298534\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.315381\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.304294\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.291808\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.302582\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.300929\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.321306\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.336177\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.314354\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.303891\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.346447\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.315299\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.321137\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.325611\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.324276\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.265537\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.331444\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.304595\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.300255\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.303828\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.295130\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.309589\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.252605\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.294835\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.312414\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.309933\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.303232\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.302673\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.315183\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.305578\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.324030\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.276110\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.301733\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.295673\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.281198\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.318687\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.293897\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.282525\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.326061\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.290793\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.300583\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.323179\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.304501\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.311071\n",
      "Configuration Sample: 11/18\n",
      "Configuration:\n",
      "  lr, Value: 1.1354327112592066e-06\n",
      "  num_conv_layers, Value: 1\n",
      "  num_filters_1, Value: 7\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.275840\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.303203\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.324828\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.279863\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.265366\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.321620\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.270102\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.305933\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.278347\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.311803\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.287226\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.291832\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.323591\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.304451\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.298982\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.329395\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.324883\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.309525\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.304466\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.306790\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.329124\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.315577\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.294559\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.328452\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.325782\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.295754\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.283134\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.327707\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.281817\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.344953\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.271124\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.300401\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.333495\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.328658\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.335827\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.299970\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.329809\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.318847\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.318669\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.277036\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.315820\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.298741\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.314025\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.312856\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.290960\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.316532\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.338615\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.305620\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.294891\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.339858\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.334418\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.336792\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.315683\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.325785\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.306175\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.287609\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.331849\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.308813\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.318829\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.304209\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.268424\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.323777\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.370398\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.340950\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.263434\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.306824\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.310665\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.343800\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.310798\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.298712\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.333430\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.317582\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.348580\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.331633\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.296889\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.327737\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.307588\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.296818\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.260048\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.317726\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.292537\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.305655\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.321316\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.277551\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.299314\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.296077\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.335097\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.283105\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.300817\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.329304\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.281062\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.321170\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.282501\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.310942\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.295903\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.287866\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.232908\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.301510\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.302320\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.302994\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.325712\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.266262\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.316809\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.329896\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.347059\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.287634\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.310616\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.307277\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.340966\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.259267\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.340427\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.299364\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.305102\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.319811\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.328528\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.317852\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.286307\n",
      "Configuration Sample: 12/18\n",
      "Configuration:\n",
      "  lr, Value: 0.38194210500820003\n",
      "  num_conv_layers, Value: 1\n",
      "  num_filters_1, Value: 6\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.333097\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.297420\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.307000\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.281404\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.275942\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.300255\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.279366\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.297990\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.300155\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.293547\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.319566\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.302176\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.333197\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.305796\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.297148\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.288559\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.314023\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.324277\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.293393\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.307909\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.301417\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.289997\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.300965\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.310499\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.304875\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.308609\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.285696\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.284979\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.319786\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.314203\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.279341\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.292347\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.284806\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.324327\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.305325\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.323136\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.300704\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.301707\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.308802\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.276148\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.319117\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.302880\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.310332\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.301463\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.295788\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.337647\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.295008\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.293686\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.310497\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.309231\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.330981\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.297296\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.281441\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.283496\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.303288\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.294182\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.316626\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.293435\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.339548\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.308947\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.310503\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.323402\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.293289\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.298749\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.273699\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.290225\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.297771\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.308494\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.312443\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.268611\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.307087\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.318709\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.297069\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.303722\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.293141\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.291990\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.292379\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.312842\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.301292\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.299200\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.287850\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.293478\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.307361\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.300218\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.317953\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.322408\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.303867\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.304985\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.305805\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.330880\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.312616\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.318574\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.302436\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.293096\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.291240\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.286826\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.292161\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.270782\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.310319\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.301487\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.297290\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.293905\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.303612\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.322204\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.303111\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.295344\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.310109\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.302577\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.312473\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.290473\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.301425\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.355204\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.328426\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.291438\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.304453\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.302730\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.324359\n",
      "Configuration Sample: 13/18\n",
      "Configuration:\n",
      "  lr, Value: 1.569637735126036e-05\n",
      "  num_conv_layers, Value: 2\n",
      "  num_filters_1, Value: 4\n",
      "  num_filters_2, Value: 2\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304090\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.301301\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.306734\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.300813\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.303555\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.301520\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.304441\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.307750\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.302428\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.303063\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.307245\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.301891\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.305707\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.304043\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.301610\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.294993\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.296765\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.303447\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.299699\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.304229\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.302006\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.294619\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.301113\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.301419\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.302735\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.298170\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.306404\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.303049\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.301835\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.303327\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.314965\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.308947\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.303046\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.305251\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.299717\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.305630\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.304286\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.303339\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.304044\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.308268\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.299292\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.306059\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.298649\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.297109\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.303617\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.307327\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.297522\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.303880\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.312983\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.309946\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.299328\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.305219\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.311090\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.296839\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.303047\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.294883\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.299278\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.305747\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.307590\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.299629\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.307354\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.299450\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.313742\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.301521\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.309799\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.307407\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.310266\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.308982\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.299274\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.314283\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.305074\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.303671\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.308767\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.304186\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.299980\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.305157\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.302266\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.300480\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.303343\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.295816\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.310061\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.312020\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.303544\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.306535\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.315001\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.305532\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.300450\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.309414\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.298292\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.297777\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.311876\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.311155\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.308648\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.304601\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.308226\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.302531\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.298576\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.303190\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.301379\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.295067\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.310288\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.308195\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.304992\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.306676\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.306644\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.296523\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.299331\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.306725\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.310205\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.305036\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.304456\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.302381\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.306954\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.308599\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.301498\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.310046\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.306462\n",
      "Configuration Sample: 14/18\n",
      "Configuration:\n",
      "  lr, Value: 0.0009426553648122567\n",
      "  num_conv_layers, Value: 2\n",
      "  num_filters_1, Value: 6\n",
      "  num_filters_2, Value: 4\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303233\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.334301\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.320968\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.288906\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.269439\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.250177\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.276284\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.264883\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.257212\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.239766\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.238000\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.235745\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.234634\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.224377\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.196769\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.205858\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.147130\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.158645\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.189462\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.116671\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.077015\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.110134\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.160559\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.062855\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.055234\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.001631\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.969471\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 1.960337\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 1.938227\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 1.866973\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 1.925599\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 1.930438\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 1.748383\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 1.819540\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 1.835731\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 1.727143\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 1.665703\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 1.551732\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 1.505634\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.536314\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 1.503253\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 1.472369\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 1.345458\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 1.277549\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 1.149901\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 1.116073\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 1.083190\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 1.184918\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 1.001115\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 0.912967\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 1.193738\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 0.955700\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.778467\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 0.938818\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 1.043579\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 0.780924\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 0.859635\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 0.914021\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 0.989123\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 0.766720\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 0.805977\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 0.646848\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 0.585654\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 0.502847\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 0.773443\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.749745\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 0.696400\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 0.725664\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 0.656239\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 0.427406\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 0.648155\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 0.543121\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 0.416840\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 0.571067\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 0.392787\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 0.553965\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 0.713113\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 0.480138\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.602461\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 0.367680\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 0.473499\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 0.402567\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 0.388267\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 0.399798\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 0.444995\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 0.592790\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 0.599428\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 0.383307\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 0.436119\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 0.498937\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 0.470460\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.700931\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 0.415058\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 0.566453\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 0.516036\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 0.531388\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 0.537332\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 0.528589\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 0.375009\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 0.423040\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 0.604801\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 0.451753\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 0.424746\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 0.277731\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.354073\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 0.490626\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 0.527003\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 0.531170\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 0.355763\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 0.327278\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 0.375542\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 0.539193\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 0.513214\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 0.579424\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 0.324919\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 0.442105\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 0.481759\n",
      "Configuration Sample: 15/18\n",
      "Configuration:\n",
      "  lr, Value: 0.03821899137939451\n",
      "  num_conv_layers, Value: 1\n",
      "  num_filters_1, Value: 8\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.282511\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 1.720463\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 0.971868\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 0.746944\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 0.475766\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 0.667568\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 0.511247\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 0.519634\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 0.336423\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 0.616443\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 0.382608\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 0.584765\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 0.352718\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.432339\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 0.210093\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 0.408142\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 0.360776\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 0.300131\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 0.506499\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 0.239640\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 0.424784\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 0.312771\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 0.396799\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 0.254746\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 0.072681\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 0.717258\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.219243\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 0.389668\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 0.288285\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 0.286750\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 0.240543\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 0.185996\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 0.093774\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 0.142870\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 0.520417\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 0.377132\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 0.313841\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 0.223378\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 0.261326\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.139587\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 0.306762\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 0.191717\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 0.160623\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 0.200645\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 0.275442\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 0.279359\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 0.253354\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 0.235018\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 0.169179\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 0.257692\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 0.219798\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 0.083144\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.331813\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 0.246362\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 0.304596\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 0.170181\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 0.113564\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 0.232167\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 0.087513\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 0.179568\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 0.318289\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 0.445655\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 0.090880\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 0.141469\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 0.151971\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.249759\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 0.396847\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 0.087105\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 0.156060\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 0.045369\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 0.170142\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 0.301322\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 0.153263\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 0.342254\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 0.250958\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 0.333865\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 0.075995\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 0.522140\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.253379\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 0.227223\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 0.204233\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 0.152828\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 0.430595\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 0.272728\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 0.252551\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 0.033639\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 0.387974\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 0.193725\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 0.197854\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 0.240807\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 0.341353\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.144081\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 0.236427\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 0.282369\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 0.217345\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 0.325078\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 0.164525\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 0.065155\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 0.122676\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 0.484293\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 0.269400\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 0.194578\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 0.106558\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 0.145027\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.078189\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 0.293467\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 0.075194\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 0.024594\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 0.164978\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 0.111390\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 0.148175\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 0.269343\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 0.060026\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 0.113757\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 0.404724\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 0.128566\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 0.176534\n",
      "Configuration Sample: 16/18\n",
      "Configuration:\n",
      "  lr, Value: 0.07758952962514781\n",
      "  num_conv_layers, Value: 1\n",
      "  num_filters_1, Value: 3\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.328791\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 1.714550\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 0.845485\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 0.656736\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 1.220456\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 0.629661\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 0.484535\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 0.800606\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 0.732465\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 0.406830\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 0.678909\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 0.465551\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 0.379873\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.320386\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 0.200847\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 0.238290\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 0.259336\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 0.614504\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 0.154143\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 0.389011\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 0.112552\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 0.439334\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 0.102242\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 0.437866\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 0.403846\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 0.258300\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.403827\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 0.123250\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 0.165626\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 0.082930\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 0.248048\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 0.289468\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 0.361996\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 0.359368\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 0.518519\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 0.339092\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 0.125455\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 0.420772\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 0.344317\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.221679\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 0.400605\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 0.163651\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 0.120240\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 0.297933\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 0.475134\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 0.218582\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 0.168720\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 0.436013\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 0.111718\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 0.205465\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 0.230058\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 0.328188\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.234852\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 0.187410\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 0.163863\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 0.310017\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 0.104522\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 0.187884\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 0.238912\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 0.142705\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 0.256838\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 0.134421\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 0.069713\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 0.186570\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 0.085709\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.158952\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 0.077970\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 0.044123\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 0.055176\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 0.089612\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 0.324983\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 0.177729\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 0.168714\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 0.333624\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 0.120013\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 0.181995\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 0.270433\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 0.228096\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.123874\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 0.049803\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 0.200621\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 0.212024\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 0.157928\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 0.166740\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 0.207656\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 0.119275\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 0.104191\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 0.055754\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 0.205045\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 0.109130\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 0.206915\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.069598\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 0.196015\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 0.265653\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 0.081922\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 0.028020\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 0.149256\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 0.253237\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 0.161331\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 0.167829\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 0.412935\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 0.288935\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 0.063415\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 0.046285\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.196200\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 0.072870\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 0.261866\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 0.109207\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 0.136278\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 0.041133\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 0.076937\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 0.208938\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 0.103606\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 0.166879\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 0.024353\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 0.126280\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 0.103116\n",
      "Configuration Sample: 17/18\n",
      "Configuration:\n",
      "  lr, Value: 1.6171259699739759e-06\n",
      "  num_conv_layers, Value: 2\n",
      "  num_filters_1, Value: 4\n",
      "  num_filters_2, Value: 7\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303167\n",
      "Train Epoch: 1 [320/60000 (8%)]\tLoss: 2.313583\n",
      "Train Epoch: 1 [640/60000 (16%)]\tLoss: 2.307971\n",
      "Train Epoch: 1 [960/60000 (23%)]\tLoss: 2.296556\n",
      "Train Epoch: 1 [1280/60000 (31%)]\tLoss: 2.303147\n",
      "Train Epoch: 1 [1600/60000 (39%)]\tLoss: 2.298846\n",
      "Train Epoch: 1 [1920/60000 (47%)]\tLoss: 2.320563\n",
      "Train Epoch: 1 [2240/60000 (55%)]\tLoss: 2.307153\n",
      "Train Epoch: 1 [2560/60000 (62%)]\tLoss: 2.307177\n",
      "Train Epoch: 1 [2880/60000 (70%)]\tLoss: 2.293532\n",
      "Train Epoch: 1 [3200/60000 (78%)]\tLoss: 2.296273\n",
      "Train Epoch: 1 [3520/60000 (86%)]\tLoss: 2.296586\n",
      "Train Epoch: 1 [3840/60000 (94%)]\tLoss: 2.301656\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.297203\n",
      "Train Epoch: 2 [320/60000 (8%)]\tLoss: 2.312106\n",
      "Train Epoch: 2 [640/60000 (16%)]\tLoss: 2.299866\n",
      "Train Epoch: 2 [960/60000 (23%)]\tLoss: 2.306890\n",
      "Train Epoch: 2 [1280/60000 (31%)]\tLoss: 2.298100\n",
      "Train Epoch: 2 [1600/60000 (39%)]\tLoss: 2.313806\n",
      "Train Epoch: 2 [1920/60000 (47%)]\tLoss: 2.300239\n",
      "Train Epoch: 2 [2240/60000 (55%)]\tLoss: 2.299032\n",
      "Train Epoch: 2 [2560/60000 (62%)]\tLoss: 2.308795\n",
      "Train Epoch: 2 [2880/60000 (70%)]\tLoss: 2.310631\n",
      "Train Epoch: 2 [3200/60000 (78%)]\tLoss: 2.298977\n",
      "Train Epoch: 2 [3520/60000 (86%)]\tLoss: 2.307461\n",
      "Train Epoch: 2 [3840/60000 (94%)]\tLoss: 2.297225\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.317307\n",
      "Train Epoch: 3 [320/60000 (8%)]\tLoss: 2.312304\n",
      "Train Epoch: 3 [640/60000 (16%)]\tLoss: 2.302887\n",
      "Train Epoch: 3 [960/60000 (23%)]\tLoss: 2.299397\n",
      "Train Epoch: 3 [1280/60000 (31%)]\tLoss: 2.308111\n",
      "Train Epoch: 3 [1600/60000 (39%)]\tLoss: 2.301748\n",
      "Train Epoch: 3 [1920/60000 (47%)]\tLoss: 2.295230\n",
      "Train Epoch: 3 [2240/60000 (55%)]\tLoss: 2.297347\n",
      "Train Epoch: 3 [2560/60000 (62%)]\tLoss: 2.301950\n",
      "Train Epoch: 3 [2880/60000 (70%)]\tLoss: 2.313129\n",
      "Train Epoch: 3 [3200/60000 (78%)]\tLoss: 2.306582\n",
      "Train Epoch: 3 [3520/60000 (86%)]\tLoss: 2.298235\n",
      "Train Epoch: 3 [3840/60000 (94%)]\tLoss: 2.306119\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.295012\n",
      "Train Epoch: 4 [320/60000 (8%)]\tLoss: 2.306143\n",
      "Train Epoch: 4 [640/60000 (16%)]\tLoss: 2.313843\n",
      "Train Epoch: 4 [960/60000 (23%)]\tLoss: 2.307871\n",
      "Train Epoch: 4 [1280/60000 (31%)]\tLoss: 2.307721\n",
      "Train Epoch: 4 [1600/60000 (39%)]\tLoss: 2.295742\n",
      "Train Epoch: 4 [1920/60000 (47%)]\tLoss: 2.305998\n",
      "Train Epoch: 4 [2240/60000 (55%)]\tLoss: 2.309345\n",
      "Train Epoch: 4 [2560/60000 (62%)]\tLoss: 2.305576\n",
      "Train Epoch: 4 [2880/60000 (70%)]\tLoss: 2.309927\n",
      "Train Epoch: 4 [3200/60000 (78%)]\tLoss: 2.303504\n",
      "Train Epoch: 4 [3520/60000 (86%)]\tLoss: 2.300607\n",
      "Train Epoch: 4 [3840/60000 (94%)]\tLoss: 2.299429\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.299853\n",
      "Train Epoch: 5 [320/60000 (8%)]\tLoss: 2.320921\n",
      "Train Epoch: 5 [640/60000 (16%)]\tLoss: 2.306537\n",
      "Train Epoch: 5 [960/60000 (23%)]\tLoss: 2.290492\n",
      "Train Epoch: 5 [1280/60000 (31%)]\tLoss: 2.288900\n",
      "Train Epoch: 5 [1600/60000 (39%)]\tLoss: 2.305148\n",
      "Train Epoch: 5 [1920/60000 (47%)]\tLoss: 2.301444\n",
      "Train Epoch: 5 [2240/60000 (55%)]\tLoss: 2.311009\n",
      "Train Epoch: 5 [2560/60000 (62%)]\tLoss: 2.305280\n",
      "Train Epoch: 5 [2880/60000 (70%)]\tLoss: 2.294059\n",
      "Train Epoch: 5 [3200/60000 (78%)]\tLoss: 2.306885\n",
      "Train Epoch: 5 [3520/60000 (86%)]\tLoss: 2.308682\n",
      "Train Epoch: 5 [3840/60000 (94%)]\tLoss: 2.297483\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.305761\n",
      "Train Epoch: 6 [320/60000 (8%)]\tLoss: 2.308130\n",
      "Train Epoch: 6 [640/60000 (16%)]\tLoss: 2.303993\n",
      "Train Epoch: 6 [960/60000 (23%)]\tLoss: 2.293271\n",
      "Train Epoch: 6 [1280/60000 (31%)]\tLoss: 2.302181\n",
      "Train Epoch: 6 [1600/60000 (39%)]\tLoss: 2.304138\n",
      "Train Epoch: 6 [1920/60000 (47%)]\tLoss: 2.296548\n",
      "Train Epoch: 6 [2240/60000 (55%)]\tLoss: 2.309635\n",
      "Train Epoch: 6 [2560/60000 (62%)]\tLoss: 2.293193\n",
      "Train Epoch: 6 [2880/60000 (70%)]\tLoss: 2.310865\n",
      "Train Epoch: 6 [3200/60000 (78%)]\tLoss: 2.311185\n",
      "Train Epoch: 6 [3520/60000 (86%)]\tLoss: 2.292854\n",
      "Train Epoch: 6 [3840/60000 (94%)]\tLoss: 2.313800\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.304378\n",
      "Train Epoch: 7 [320/60000 (8%)]\tLoss: 2.308306\n",
      "Train Epoch: 7 [640/60000 (16%)]\tLoss: 2.303648\n",
      "Train Epoch: 7 [960/60000 (23%)]\tLoss: 2.305061\n",
      "Train Epoch: 7 [1280/60000 (31%)]\tLoss: 2.306293\n",
      "Train Epoch: 7 [1600/60000 (39%)]\tLoss: 2.297426\n",
      "Train Epoch: 7 [1920/60000 (47%)]\tLoss: 2.312126\n",
      "Train Epoch: 7 [2240/60000 (55%)]\tLoss: 2.300345\n",
      "Train Epoch: 7 [2560/60000 (62%)]\tLoss: 2.305356\n",
      "Train Epoch: 7 [2880/60000 (70%)]\tLoss: 2.311968\n",
      "Train Epoch: 7 [3200/60000 (78%)]\tLoss: 2.301639\n",
      "Train Epoch: 7 [3520/60000 (86%)]\tLoss: 2.292514\n",
      "Train Epoch: 7 [3840/60000 (94%)]\tLoss: 2.302397\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.306194\n",
      "Train Epoch: 8 [320/60000 (8%)]\tLoss: 2.297899\n",
      "Train Epoch: 8 [640/60000 (16%)]\tLoss: 2.302558\n",
      "Train Epoch: 8 [960/60000 (23%)]\tLoss: 2.297624\n",
      "Train Epoch: 8 [1280/60000 (31%)]\tLoss: 2.297017\n",
      "Train Epoch: 8 [1600/60000 (39%)]\tLoss: 2.295297\n",
      "Train Epoch: 8 [1920/60000 (47%)]\tLoss: 2.289128\n",
      "Train Epoch: 8 [2240/60000 (55%)]\tLoss: 2.298301\n",
      "Train Epoch: 8 [2560/60000 (62%)]\tLoss: 2.307665\n",
      "Train Epoch: 8 [2880/60000 (70%)]\tLoss: 2.303175\n",
      "Train Epoch: 8 [3200/60000 (78%)]\tLoss: 2.294902\n",
      "Train Epoch: 8 [3520/60000 (86%)]\tLoss: 2.296151\n",
      "Train Epoch: 8 [3840/60000 (94%)]\tLoss: 2.290773\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.299817\n",
      "Train Epoch: 9 [320/60000 (8%)]\tLoss: 2.307109\n",
      "Train Epoch: 9 [640/60000 (16%)]\tLoss: 2.308120\n",
      "Train Epoch: 9 [960/60000 (23%)]\tLoss: 2.294017\n",
      "Train Epoch: 9 [1280/60000 (31%)]\tLoss: 2.292255\n",
      "Train Epoch: 9 [1600/60000 (39%)]\tLoss: 2.302352\n",
      "Train Epoch: 9 [1920/60000 (47%)]\tLoss: 2.289461\n",
      "Train Epoch: 9 [2240/60000 (55%)]\tLoss: 2.298510\n",
      "Train Epoch: 9 [2560/60000 (62%)]\tLoss: 2.297087\n",
      "Train Epoch: 9 [2880/60000 (70%)]\tLoss: 2.301002\n",
      "Train Epoch: 9 [3200/60000 (78%)]\tLoss: 2.295329\n",
      "Train Epoch: 9 [3520/60000 (86%)]\tLoss: 2.294483\n",
      "Train Epoch: 9 [3840/60000 (94%)]\tLoss: 2.321011\n"
     ]
    }
   ],
   "source": [
    "n_random_samples = 18\n",
    "n_epochs = 9\n",
    "cs = get_configspace()\n",
    "train_loader, validation_loader, _ = load_mnist_minibatched(batch_size=32, n_train=4096, n_valid=512)\n",
    "\n",
    "# START TODO ################\n",
    "results = []\n",
    "for i in range(n_random_samples):\n",
    "  print(f\"Configuration Sample: {i}/{n_random_samples}\")\n",
    "  config = cs.sample_configuration()\n",
    "  print(config)\n",
    "  #for epoch in range(1, n_epochs + 1):\n",
    "  #    models, val_errors = run_conv_model(config, epoch, train_loader, validation_loader)\n",
    "  model, val_errors = run_conv_model(config, n_epochs, train_loader, validation_loader) \n",
    "  results.append((model, config, val_errors))  \n",
    "# End TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SK_wPkt0j6BR"
   },
   "source": [
    "### Evaluate \n",
    "\n",
    "Now we should evaluate the previous runs. Evaluation in hyperparameter optimization can mean two different things: On the one hand, we might be only interested in the model with the best performance. On the other hand, we might want to find the best hyperparameter configuration to then train a model with these hyper-parameters (but with e.g. more epochs).\n",
    "\n",
    "**Task:** Print the model and final validation error of the best model in `results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "VTFoJe8Dj6BT",
    "outputId": "e6c4fe18-59f6-4db1-e6e9-65e3250b48a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Validation Accuracy = 0.9453125\n",
      "Configuration:\n",
      "  lr, Value: 0.03633768066215828\n",
      "  num_conv_layers, Value: 2\n",
      "  num_filters_1, Value: 2\n",
      "  num_filters_2, Value: 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03633768066215828"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# START TODO ################\n",
    "\n",
    "val_acc = [results[best_model_idx][-1][-1] for best_model_idx in range(len(results))]\n",
    "max_val_acc = max(val_acc)\n",
    "idx = val_acc.index(max_val_acc)\n",
    "print(f\"Maximum Validation Accuracy = {max_val_acc}\")\n",
    "print(results[idx][1])\n",
    "results[idx][1][\"lr\"]\n",
    "# END TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TrHrSi5Nj6BW"
   },
   "source": [
    "Let's further investigate which hyperparameters work well and which don't.\n",
    "\n",
    "**Task:** Print a scatter plot of learning rate (x) and number of filters (sum over layers, y). Scale the size of the scatter points by the error in the last epoch (10 to 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "JnR6EbUNj6BX",
    "outputId": "d34532ca-1660-4b54-82e6-0743b0a8d13e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEGCAYAAACJnEVTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHaZJREFUeJzt3XmYHHW97/F3z5JMJpkkk8wkZBkS\nsvBNICFsQWQNuxLUwypeUHG/yvV49T76HI/Ioh713gPH56CeK94jKh49R2VxQwQBAYHITsKSfCEJ\nISHbzCSTZZJZMpm+f1QndJKZ7pqlqqe7Pq/nyUN3dXXV90cnn/71r6p+lUqn04iISDKUFboAERGJ\nj0JfRCRBFPoiIgmi0BcRSRCFvohIglQUuoBcmpp2RnZqUW1tNS0tu6PafOxKrT2gNhWLUmtTKbSn\nvr4m1dtrie3pV1SUF7qEQVVq7QG1qViUWptKrT0HS2zoi4gkkUJfRCRBFPoiIgmi0BcRSRCFvohI\ngkQa+mY2z8xWmdn/OGj5BWammd5ERGIW2Xn6ZjYS+C7w0EHLq4AvAxuj2reISDF5ZkUjS17exLDK\nMi4+/QgmjhsZ2b6i7Ol3ABcCGw5a/o/A94HOCPctIlIU1je18h8POC+ubObp5Y3c/scVke4vsp6+\nu3cBXWa2f5mZHQkscPfrzeyf822jtrY60gsl6utrItt2IZRae0BtKhal1qY427P0ja3s3L1n//OW\n1k7q6kaRSvV6Ue2AxD0Nw3eAvw+7cpSXQtfX19DUtDOy7cet1NoDalOxKLU2xd2ew+uqmTB2BI3b\n2gCYUldNc3PrgLaZ60srttA3synAHODnmd7/JDN71N3PjKsGEZGhpramimsvmcdjSzdSNayci06Z\nHun+Ygt9d18PzNz33MzWKPBFRKBhQg1XnRfPkFKUZ++cANwCTAf2mNllwCXuvjWqfYqISG5RHsh9\nDliU4/XpUe1bRER6pityRUQSRKEvIpIgCn0RkQRR6IuIJIhCX0QkQRT6IiIJotAXEUkQhb6ISIIk\nLvTT6TQtOzvY1bYn/8oiIiUm7lk2C+p5b+S+p9ayrrGV6qoKZk8dywcvOJJRI4YVujQRkVgkJvTX\nN7XyswdeY/uu4N4tna2dPLOikc6uvXzusgUFrk5EJB6JGd55dOmG/YGfbcWbLaxvGtjc1SIixSIx\nod/bGH7Hnm42t7TFXI2ISGEkJvQn1/V8o+HamuHMOXxszNWIiBRGYkL/3BMbmDl59AHLysvg1PmH\nUV1VWaCqRETilZgDucMry/ncFQu4b8mbrG1sZfSo4Rx1+BhOnT+50KWJiMQmMaEPMKqqksvPmgWU\n3s2cRUTCSMzwjoiIKPRFRBJFoS8ikiAKfRGRBFHoi4gkiEJfRCRBFPoiIgmi0BcRSRCFvohIgij0\nRUQSJNJpGMxsHvBb4Dvu/j0zawB+DFQCe4Cr3X1TlDWIiMjbIuvpm9lI4LvAQ1mLvwH80N3PBO4B\nvhDV/kVE5FBRDu90ABcCG7KWfQa4K/O4CRgf4f5FROQgqXQ6HekOzOxGoNndv5e1rBx4GPiauz/U\n23u7uvamKyrKI61PRKQEpXp7IfaplTOB/zPg4VyBD9DSsjuyOkptauVSaw+oTcWi1NpUCu2pr6/p\n9bVCnL3zY+B1d7+pAPsWEUm0WEPfzK4COt39hjj3KyIigciGd8zsBOAWYDqwx8wuAyYA7Wb2SGa1\nV939M1HVICIiB4os9N39OWBRVNsXEZG+0xW5IiIJotAXEUkQhb6ISIIo9EVEEkShLyKSIAp9EZEE\nUeiLiCSIQl9EJEEU+iIiCaLQFxFJkNinVi60F19v5omXNzKpfhTvOXkalRX63hOR5EhU6Ld1dPHz\nPztbdnSAN7G3ay+XL5pV6LJERGKTqG5uZ1c3u9q79j/f3bangNWIiMQvUaE/ZuQwzlgwmbGjhjNr\n6ljOXdhQ6JJERGKVqOEdgCvPmc3lZ81k4oTRNDe3FrocEZFYJaqnv095WRmpVK/3DRYRKVmJDH0R\nkaRS6IuIJEjeMX0zmw5McfcnzOwTwMnAze6+POriRERkcIXp6f8Y6DSz44CPA3cBt0ZalYiIRCJM\n6Kfd/RngYuB77v5HQEdBRUSKUJhTNkeZ2ULgMuBMMxsO1EZbloiIRCFMT/9m4P8Bt7l7E3Aj8Iso\nixIRkWiE6envdPdjs55/xd27oypIRESiE6an/wUz2//loMAXESleYXr624BXzex5oHPfQnf/UGRV\nRaR5ext/fnYdazbuJJ2C0SMqWWgTOOmoibpCV0QSIUzo/yHzp6i9tHoLd/zJ2bKj/YDlL7zWzLLV\nW/jYRUdRpuAXkRKXN/Td/admNg+Y5e6/MbOx7r4tzMYz7/st8B13/56ZNQA/A8qBjcAH3b1jAPWH\n8vY8+u2HvJYGlryymQm11bzvtCOiLkVEpKDyjumb2eeB24GbMou+ambXhXjfSOC7wENZi78GfN/d\nTwdWAh/tc8X98NBz62hsOTTws73wehPd3ek4yhERKZgwB3I/QDD1wtbM8y8CF4V4XwdwIbAha9ki\n4HeZx78Hzg1V5QCt3Zx/CuW1m1vZtHV3DNWIiBRO2FM2u80MCM7eMbO8Z/C4exfQte99GSOzhnMa\ngUm5tlFbW01FRXmIEnOrrAx324AxY0ZQX18z4P0VSjHX3hu1qTiUWptKrT3ZwqThKjO7Aag1s0uA\n9wODMdla3qOmLS2D0/MeO7Iy7zoTx41gWAqamnYOyj7jVl9fU7S190ZtKg6l1qZSaE+uL60wwzvX\nAruA9cDVwFPAp/tZS6uZjcg8nsKBQz+ROe/EBkbnCf55R4ynskIzTYtIaQsV+u5+s7svdvdL3P1f\ngK/0c38PApdmHl8K/Kmf2+mTsTXDufj0GVRX9fzDZu60Wi5bNDOOUkRECqrX4R0zOws4G7jazMZl\nvVQJfAS4IdeGzewE4BZgOrDHzC4DrgJ+YmafAt4Efjqg6vvgzGOnUDd2BI+9uIHVG3bQnU4zdtQw\n5s0Yz+KTpzGscuDHDkREhrpcY/orgMmZx3uzlu8Brsy3YXd/juBsnYOdF7a4wXb09HEcPX0c6XSa\nceNH0bJ1V6FKEREpiF5D3903Aj83s8fd/c0Ya4pcKpWiolzj9yKSPLmGd9YRXLDKQaddAuDuh0dX\nloiIRCHX8M5psVUhIiKxyBX6R7n7fWbW21QJt0dRkIiIRCdX6M8H7gNO7+G1NAp9EZGikyv09722\nyt2/EUcxIiISrVyh/zEzqwGuNLNhB7/o7tdHV5aIiEQh13mLVxNMvwDBefoH/xERkSKT6zz9JcAS\nM/uLuz8RY00iIhKRvFcolVrgt7Z18viyDby8qrnQpYiIxC7cRPMlonl7G7feuYy3mnZRWVnGBQsb\nuOQMTbQmIsnRa0/fzD6S+e/H4ysnWn9+dh1vNQWHKfbs6ebxZZvo2pv3fjAiIiUjV0//usxZO/+z\npztlubvO0xcRKTK5Qv+LBPe4HcuhF2gV5cVZ553YwPI1LfuHd047ZpImXhORRMl19s7dwN1mdqm7\n3xVjTZGpGzOCL111PEtfb+bI6eOprznk8gMRkZIW5kDuEjP7EbCQoIf/N+A6d2+KtLKIjKqq5NT5\nk0riPpgiIn0VZmzjNuB54AMEd75aDvwoyqJERCQaYXr61e7+/aznL5vZe6MqSEREohOmpz/SzCbt\ne2JmU4Gq6EoSEZGohOnpfx14zsw2ASmgHvhYpFWJiEgk8oa+u99rZjOBIwkO5L7m7u2RVyYiIoMu\n1DQM7t4GLI24FhERiZiuTBIRSZC8oW9mqTgKERGR6IXp6T8ceRVFqq2ji6Urm/jL82+xav129nZr\n8jYRGdrCjOm/aGZfA54EOvctdPfEfhlsa23nPx98naUrt9DZ9XbQj6yq4LT5k7h00UzN6SMiQ1KY\n0D8289/sSdfSJPQXwLbWdm7+zxfZsGX3Ia/tau/i/mfWsbllN9deMp/yMgW/iAwtYU7ZPAuCsX13\nTw9kZ2Y2CrgDqAWGAze5+/0D2WZfbWtt56lXNnNEQy2zJ9WQSvXtkMVdj6zuMfCzvbhyCw88s453\nv2PaQEoVCS2dTrNqww7KUzB90ug+/72W5Mgb+ma2gGCunVHAHDP7KvCAuz/Vj/1dA7i7f9nMJhP8\nWpjTj+30y+atu/n6Hc+yu70LgHfMncCn3jcv9Pt3t3fxypqtodZdtnKLQl9ikU6nuf3e5Tz58iZS\nKTjz2Ml88ILY/llJkQkz/vA94KPAxszzXwL/0s/9NQPjM49rM89jc++SNfsDH+C515v7dOes1Ru3\ns621M/+KwOaW3XR3D+iHkUgom1va+Nurm0kD3WlY8spmduwK9/dUkifMmP4ed19mZgC4+2tm1pXn\nPT1y9/8ys2vMbCVB6C/OtX5tbTUVFeX92VWPRtccOGVQRVmKCRNGU14W7qfw+O3hL0QuLy+jvr6G\nspDbHgz19TWx7SsualN+qcpyKivL2NuxF4BhleVMnDiaUSMqB3U/uZTa51Rq7ckWJvS7zOwIgoO3\nmNm7Cebg6TMzuxpY6+7vyho2OrG39Vtaco+d99WFJzXw9CubaN7eTnlZigtOamDrltbQ768bOYwJ\ntVU0tuQP/8l11Wzpw7YHqhTvD6A2hXfBwgYeeWEDqVSK806cSltrO22t8cyWUmqfUym0J9eXVpjQ\n/1/AbwEzsx3AG8CH+1nLqcD9AO6+1Mwmm1m5u+/t5/b6pLqqkm9+8mTWN7UyY9p49nbs6dP7KyvK\nOGZmHQ8++1bO9VLASXMmDqBSkb5532kzuPDkaUCKygqdNSa9C3P2zkvAMWZWD3S4+44B7G8l8A7g\nLjObBrTGFfj7VJSXMe2w0YwbXUVTU99CH+DyRTNZ39zK8jXbel3nrOOncOr8Sb2+LhKFykEcCpXS\nFebsnaOAG4GjgbSZvQTc6O7ej/3dBtxuZo9m9v3f+7GNgqqsKOfzlx/LH554g7++tJHtuzrp7s4c\nHxg3ggsWNnD6gimFLlNEpEdhhnfuAP4NuJ5g5OI04D8I7pnbJ+7eClzR1/cNNRXlZfzdGTN53+kz\n6Ozqprs7zbDKMl2MJSJDXpjQb3X327OeLzezS6MqqJikUimGV+ontYgUj15D38z2dVsfNLNLgAeB\nbuAc4LEYahMRkUGWq6ffRXCaZk+nZ3YB34ykIhERiUyvoe/uGqAWESkxYc7emQxcBowhq9fv7l+L\nsC4REYlAmN78fcBxwDCgMuuPiIgUmTBn72xx949EXknMdrfv4fV12xhWWUbDhHjnyBERKZQwoX+P\nmV0FLCE4gAuAu6+NrKoIbd3Rzj2PrWbFum1syUygdvjEURx/ZD0XnTKdMs1DLiIlLEzoHwNcBWzJ\nWpYGDo+koght3dHOv965jHWNB06EtnZzK2s3t9K0rY2PXjhXN6AQkZIVJvRPBmrdvSPqYqJ292Or\nDwn8bE++tIkFM+s4cc6EGKsSEYlPmAO5zwBVedca4to6uli6Kvc9W9LAU8s3x1OQiEgBhOnpTwXW\nmNlyDhzTPyOyqiKwrnEnu9ry3/uluQ83ShERKTZhQv+fIq8iBmHnyAl7Fy0RkWIUZninvJc/RaVh\nQg1T6qrzrjdtYuneJk1EJExP/6tZj4cRzKv/BPBwJBVFpKwsxYlzJ7L+r2/0uk5NdSVnH6+58EWk\ndIW5c9ZZ2c/NbALwrcgqitB7TplOc0sbT768Kbjhb5aa6kquOHsWU+pHFaQ2EZE4hOnpH8DdG81s\nbhTFRK0sleKji+dyzMzxLH1jKxsaWylLwbTDajj7+KkKfBEpeWEmXPsZHNAxbgBiva/tYEqlUiyc\nO5ELz5hV9He8FxHpqzA9/QezHqeBHcAD0ZQjIiJRCjOm/9M4ChERkejlul3iGxw4rJPKPB8OHObu\nRXfapohI0uW6c9YRBy8zs78jOHPn9kPfISIiQ12os3fMbDZwK9AJLHb31ZFWJSIikcgZ+mY2Erge\nWAx80d3vi6UqERGJRK/TMJjZB4DngK3AcQp8EZHil6un/3PgNeBdwAVmtm95Cki7+9kR1yYiIoMs\nV+gfciBXRESKW66zd96MYoeZ++1+iWBu/uvd/d4o9pNLd/rgmXdEZJ/u7jQPPb+O1t17OPnow5g0\nfmShS5JB1Oe5dwbCzMYDNwAnAKOAm4BYQ/83f13N48s2Ul9bzccWz6FuzIg4dy8y5P37H17lb68G\nd5B76tVGPn/FMUwcp+AvFWHm0x9M5wIPuvtOd9/o7p+Mc+fd6TSPL9vI1p0d+NoWHn5ufZy7Fxny\nurvTrFi7bf/zxm1tPLOiqYAVyWCLtacPTAeqzex3QC1wo7s/1NvKtbXVVFQM7oW/9bXVbN3ZQXkZ\nzJpWS3196dw0pZTaso/aFK90Os2YmmFsa+0AIJWCwyePyVvzUG5Tf5Rae7Kl0jGOb5vZPwCnAhcD\n04C/ANPcvccimpp2DnpxjS1tPPLCemZNq+W4GeNIpUrj9oj19TUlN2uo2lQYL61q5u7HVtPWsZd5\nM8Zx1XlH5vx3Ugxt6otSaE99fU2vH1jcPf3NwJPu3gWsMrOdQD3QGFcBE2pHcMXZs0rigxWJwvyZ\ndcyfWVfoMiQicY/pPwCcbWZlmYO6o4DmmGsQEUmsWEPf3dcDdwJ/A+4DPuvu3XHWICKSZHEP7+Du\ntwG3xb1fERGJf3hHREQKSKEvIpIgCn0RkQRR6IuIJIhCX0QkQRT6IiIJotAXEUkQhb6ISIIo9EVE\nEkShLyKSIAp9EZEEUeiLiCSIQl9EJEEU+iIiCaLQFxFJEIW+iEiCKPRFRBJEoS8ikiAKfRGRBFHo\ni4gkiEJfRCRBFPoiIgmi0BcRSRCFvohIgij0RUQSRKEvIpIgBQl9MxthZqvM7JpC7F9EJKkqCrTf\n64Ctce+0O53mVw+vZOVb2xhdU8W7T2pg9tSxcZchIlIwsYe+mc0BjgLujXvfv39iDQ88sy54snEn\nm5pa+eo1CxkxvFDffSIi8SrE8M4twBcKsF/WN7Ue8HxTSxsbt+wqRCkiIgURaxfXzD4ELHH3N8ws\n7/q1tdVUVJQP2v4nTagBb3p7+zXDmTOznjGjhg/aPgqpvr6m0CUMOrWpOJRam0qtPdlS6XQ6tp2Z\n2S+BGcBeYCrQAXzK3R/saf2mpp2DWlzHnr388HevsHrjDmqqh/Gukw7nlHmHDeYuCqa+voampp2F\nLmNQqU3FodTaVArtqa+vSfX2Wqw9fXd//77HZnYjsKa3wI/C8MpyPnvpMXTt7eawiaNpbm7N/yYR\nkRKSyPP0K8rLSKV6/SIUESlZBTttxd1vLNS+RUSSKpE9fRGRpFLoi4gkiEJfRCRBFPoiIgmi0BcR\nSRCFvohIgij0RUQSRNNLDjGbW3bzyAvr2bGrk2GV5Rw7czzHzKrTxWQiMigU+kNEdzrNLx58jade\n3sSujr37l/916QaOPLyWjy+ey7jRVQWsUERKgYZ3hog7H1nFw8+tPyDwAbrTsOLNFn7w25fp2ttd\noOpEpFQkLvS702lWrd9Oy472Qpey3+72Lp5+dXPOdVau38HjyzbEVJGIlKrEDe/cfu9ynnx5E3Vj\nq/jUe49m5uQxhS6Jx5ZuYOvOjrzrvbR6K4uOmxpDRSJSqhLX03/9re0ANG9rZ+nK5gJXE9jVtifc\neu3h1hMR6U3iQn/21KBnP35MFcfMrCtwNYHqEeF+cFVXJe6HmYgMssSlyEcXz+XMBZOZM7OOvZ1d\nhS4HgDMXTOGhZ9/KO8Qz/4jxMVUkIqUqcT39slSK2Q1jGTdmRKFL2a+6qoKFcyfkXGfm5NGcvmBy\nTBWJSKlKXE9/qLr8rFl0dnXz1Cub2J112mZZCmY3jOXji+dSUZ6472gRGWQK/SGiLJXig+cb5504\nlUde2MCO3Z0MqyhnwazxHKsrckVkkCj0h5jDxo3kynNmF7oMESlRGi8QEUkQhb6ISIIo9EVEEkSh\nLyKSIKl0Ol3oGkREJCbq6YuIJIhCX0QkQRT6IiIJotAXEUkQhb6ISIIo9EVEEkShLyKSICU/4ZqZ\nfQc4GUgDn3P3Z7JeOxf4JrAX+KO7f70wVfZNnjZVAbcBR7v7iQUqsc/ytOks4FsEn5MDH3f37oIU\nGlKe9nwC+BhBe5YC17r7kL9gJlebstb5FvBOd18Uc3n9kudzWgOsI/icAK5y9/Vx1zjYSrqnb2Zn\nArPd/Z0E/8huPWiVW4FLgVOB883sqJhL7LMQbfpn4MXYCxuAEG36IXCZu58K1ADvirnEPsnVHjOr\nBq4ETs+0Zw7wzoIU2gchPiMy/37OiLu2/grTJuDd7r4o86foAx9KPPSBc4DfALj7cqDWzEYDmNkM\nYKu7r8v0Gv+YWX+o67VNGf8I3FOIwgYgX5tOcPe3Mo+bgKF+38he2+Puu939HHffk/kCGANsKlyp\noeX7jABuAb4Sd2EDEKZNJafUQ/8wgpDYpymzrKfXGoFJMdU1ELnahLvvjL2igcvXph0AZjYJOJ/g\nC3ooy9keADP7B2AV8Ct3Xx1jbf2Vs01mdg3wKLAm1qoGJu/nBPzAzB43s2+bWUncyajUQ/9guT60\nYv1Ai7XuXA5pk5lNAH4PfMbdt8Rf0oAc0h53/zYwA3iXmZ0af0kDtr9NZjYO+AhBT7+YHfw5XQ98\nAVgEzCMYCi56pR76Gzjwm3sysLGX16Zklg11udpUrHK2KfOT+z7gOnd/IOba+qPX9pjZODM7A8Dd\n2wjaVQyhn+szOhuoB/5KMLR4fOYA6VCX8++du9/h7o3u3kXw63J+zPVFotRD/wHgMgAzOx7YsG/4\nw93XAKPNbLqZVQAXZdYf6nptUxHL16ZbgO+4+58KUVw/5GpPJfATMxuVeX4SwRlJQ12uf0t3uvtR\n7n4ycDHwvLt/vnClhtZrm8xsjJndb2bDMuueCbxcmDIHV8lPrWxm3yY4o6AbuBY4Dtju7vdkelz/\nO7PqXe5+c4HK7JM8bfo10AAcDTwH/NDdf1GwYkPqrU3A/UALsCRr9V+4+w9jL7IP8nxG12SWdRGc\nsvnpIjlls9c2Za0zHfhJEZ2ymetz+hzwYaANeAH4bDF8TvmUfOiLiMjbSn14R0REsij0RUQSRKEv\nIpIgCn0RkQRR6IuIJEjJz7IpxS9zGuDj7j41xn0+Apzj7nvzrZtnO2ngMYJZHAGqgP/j7nfned9/\nA/5rqM8mKsVHoS/Sg0E+z/yczFWdmNlEYKmZPeLuW3O85ybgVwTnj4sMGoW+FDUzuwL4LMG8KU0E\nc+1vMbNPAx8COoF24P3uvi0zR/ovCea9+SLwO4ILwN5BMG3zYnffkOmhVwLXEczqORWYDfzF3T+b\nuW/BT4HpwFsEF1r92d3/PVe97r7ZzDYCM81sG/ADgumVhwNPufvfm9lNwCzgITO7GFgA3JBp4x7g\nE+7+RubCorOBDmA98GF37+j//01JAo3pS9EyswaCqXzPdffTgEcIppYGGAGc7+5nEsz8eHXWW193\n98szj48iuIL0DIL7ELy/h10dR3C5/kLgI2ZWm9lepbu/g+BKzvND1nwCwRwvy4FaYJm7n5HZzvlm\nNs/db8isfg7BF9YPgEsybfkucHOmhmsJblhyOnA3MDFMDZJs6ulLMXsnwXTY95sZBL3lNzKvbQH+\naGbdBL3x7Enpnsx63Ozur2QevwmM62E/j2fG9tvMrDmzzrEEXzK4+yYzezxHnQ9lfjlMJLik/z3u\n3mpmbUCDmS0h6K1PAuoOeu+8zPK7M20sB9Lu3mJm9wOPmtk9wC+z7jkg0iuFvhSzDuBpd78oe6GZ\nTQVuJrhlZKOZHTynUmfW466DXutpquqe1injwPH2XAd8z3H3LjNbCNwBvJRZfiXBr4fTM68/28N7\nO4C1PR1jcPfLzGwOsJgg/C9196K6a5rET8M7UsyeAU4ys8MAzOxyM3sfMIGgB9+Ymev9fIJfAYNp\nBXBKZr8TgNPyvSFz/9X7gW9kFk0MFntXZthnVlad+44pvAbUmdm8zL7OMLNPmtkMM/u8u69w91sI\nhncWDF7zpFSppy/Foj5zGuU+T7v7lzIzIf7BzHYDuwlmRWwCXjezpwnuTnUD8H/N7N5BrOcnwEWZ\noZk3COaSP/gXQU+uA5aZ2Z3Ar4Hfm9mjwBMEv05uNbOTgT8BzwLvJTh+8CMza89s45MEB4+Py7Rx\nJ8FMpDcNUtukhGmWTZF+MLMpwCnu/mszKwOeJ5gieUmet4oUlEJfpB/MbCTB+HwDwVDMw+7+5cJW\nJZKfQl9EJEF0IFdEJEEU+iIiCaLQFxFJEIW+iEiCKPRFRBLk/wP6+DPuozW0yQAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f934dddecc0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# START TODO ################\n",
    "val_acc = [results[best_model_idx][-1][-1] for best_model_idx in range(len(results))]\n",
    "lrs =  [results[best_model_idx][1][\"lr\"] for best_model_idx in range(len(results))]\n",
    "\n",
    "num_filters = [[results[best_model_idx][1]['num_filters_1'] + results[best_model_idx][1]['num_filters_2']] if ('num_filters_2') in results[best_model_idx][1] else [results[best_model_idx][1]['num_filters_1']] for best_model_idx in range(len(results))]\n",
    "#print(len(lrs))\n",
    "#print(len(num_filters))\n",
    "#Scale the values from 10 to 100\n",
    "scaled = [((100-10)/(max(val_acc) -min(val_acc)))*(v - max(val_acc)) + 100 for v in val_acc]\n",
    "#print(scaled)\n",
    "plt.scatter(lrs, num_filters, s=scaled)\n",
    "plt.xlabel('Learning Rates')\n",
    "plt.ylabel('Number of filters')  \n",
    "plt.show()\n",
    "# END TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qg38XoGXj6Ba"
   },
   "source": [
    "**Question:** What pattern do you see? Why might it occur?\n",
    "\n",
    "**Answer:**  We see that large and very low learning rates show poor performance. It happens as the gradient decend algorithm cannot converge for very low learning rates and jumps between the values and missed the minimum point for large learning rates. We also see that number of filters have a lower significance on the result compared with the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVhA3lxxj6Bb"
   },
   "source": [
    "After looking at the final error, let's now have a look at the training error.\n",
    "\n",
    "**Task:** Plot error curves (error per epoch) for all your configurations in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "aGzbl3F_j6Bc",
    "outputId": "c9819131-453e-47ec-8342-d346f886039c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhGIKwDAMY0L5f3UcdG5OAf1yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f934ddde390>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# START TODO ################\n",
    "#val_errors_p_ep = [results[best_model_idx][-1][:] for best_model_idx in range(len(results))]\n",
    "for model_idx in range(len(results)):\n",
    "  plt.plot(results[model_idx][-1][:])\n",
    "\n",
    "plt.ylabel('Validation Error per Epoch')\n",
    "plt.xlabel('Epochs')  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# END TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVNXym3Aj6Bg"
   },
   "source": [
    "**Questions:** How could you detect configurations with a low error earlier/faster? Why could this be problematic? \n",
    "\n",
    "**Answers:** The configurations with lower accuracies can be stopped on earlier epochs. On one hand, it decreases the computational loss since we wouldnt evaluate the models which do not improve at all, but on the other hand, it also stops some models that started with low accuracy but can converge to the best accuracy value, later. Therefore, this technique does not guarantee finding the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ygjq6CYCj6Bh"
   },
   "source": [
    "### Your Feedback on Exercise 6.1\n",
    "\n",
    "Wonderful exercise, optimization was always a problem when developing deep learning codes. I personally, didn't know that we could use these environments to check the values efficiently. It took more than 10 hours to solve this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W42L2-eHj6Bi"
   },
   "source": [
    "## BOHB\n",
    "\n",
    "Here we will use the more advanced hyperparameter optimizer [BOHB](https://www.automl.org/blog_bohb/) (Bayesian Optimization with Hyperband).\n",
    "Based on [Hyperband](https://arxiv.org/pdf/1603.06560.pdf), BOHB evaluates configurations on your model with increasing budgets. In the context of Deep Learning, budget can be the number of epochs or the number of training samples. In lower budget evaluations, BOHB can look at more configurations. Full budget evaluations avoid missing configurations which are poor at the beginnning but good at the end (and vice versa). \n",
    "At the start of a run, BOHB samples configurations randomly. After some time, BOHB then uses a bayesian model (based on Parzen Tree Estimators), sampling only promising configs.\n",
    "\n",
    "This exercise part is based on the [HpBandSter Examples](https://automl.github.io/HpBandSter/build/html/auto_examples/index.html) and the [HpBandSter Documentation](https://automl.github.io/HpBandSter/). *HpBandSter* provides a fast implementation of *Randomsearch*, *Hyperband* and *BOHB*. The optimization can easily be distributed between multiple cores or even multiple computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "yeDLITs3j6Bj",
    "outputId": "3f2b043d-5a6a-4b24-b7ba-6e02a09668b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import logging\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "from hpbandster.optimizers import BOHB\n",
    "\n",
    "logging.getLogger('hpbandster').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pMiCn0Xj6Bl"
   },
   "source": [
    "### Implement Worker\n",
    "\n",
    "The worker defines the hyperparameter problem which we try to optimize.\n",
    "*compute(...)* should - for a given configuration and budget - return a loss which the hyperparameter optimizer tries to minimize. In our case, we can use the number of epochs as budget and the validation error as loss. \n",
    "As best practice, we define the configuration space also in the worker.\n",
    "\n",
    "If you need help, you might get some inspiration from the [HpBandSter Pytorch Worker Example](https://automl.github.io/HpBandSter/build/html/auto_examples/example_5_pytorch_worker.html#sphx-glr-auto-examples-example-5-pytorch-worker-py).\n",
    "\n",
    "**Task:** Complete the methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdRmso17j6Bm"
   },
   "outputs": [],
   "source": [
    "class PyTorchWorker(Worker):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_loader, self.validation_loader, self.test_loader =\\\n",
    "            load_mnist_minibatched(batch_size=32, n_train=4096, n_valid=512)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model(config: CS.Configuration) -> nn.Module:\n",
    "        \"\"\" Define a configurable convolution model.\n",
    "            \n",
    "        See description of get_conv_model above for more details on the model.\n",
    "        \"\"\"\n",
    "        # START TODO ################\n",
    "        pool_kernel_size = 2\n",
    "        conv_kernel_size = 3\n",
    "        #print(len(num_filters_per_layer))\n",
    "        # START TODO ################\n",
    "        str_flt = ['num_filters_1','num_filters_2','num_filters_3']\n",
    "        num_filters_per_layer = [config[str_flt[index]] for index in  range(config['num_conv_layers'])]\n",
    "        \n",
    "        img_size = 28\n",
    "        layers = []\n",
    "        old_num_filters = 1\n",
    "        for  num_filters in num_filters_per_layer:\n",
    "          layers += [nn.Conv2d(old_num_filters, num_filters,conv_kernel_size, stride=1, padding=0)]\n",
    "          layers += [nn.ReLU(True)]\n",
    "          layers += [nn.MaxPool2d(pool_kernel_size, stride=1)]\n",
    "          img_size = (img_size - conv_kernel_size + 1 ) - pool_kernel_size + 1\n",
    "          old_num_filters = num_filters\n",
    "          #print(f\"img_size = {img_size}\")\n",
    "        layers += [Flatten()]\n",
    "        layers += [nn.Linear(old_num_filters*img_size*img_size, 10)]\n",
    "        layers += [nn.LogSoftmax(dim=1)]\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "  \n",
    "        # END TODO ################\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_configspace() -> CS.Configuration:\n",
    "        \"\"\" Define a conditional hyperparameter search-space.\n",
    "    \n",
    "        hyperparameters:\n",
    "          num_filters_1   from    4 to   32 (int)\n",
    "          num_filters_2   from    4 to   32 (int)\n",
    "          num_filters_3   from    4 to   32 (int)\n",
    "          num_conv_layers from    1 to    3 (int)\n",
    "          lr              from 1e-6 to 1e-1 (float, log)\n",
    "          sgd_momentum    from 0.00 to 0.99 (float)\n",
    "          optimizer            Adam or  SGD (categoric)\n",
    "          \n",
    "        conditions: \n",
    "          include num_filters_2 only if num_conv_layers > 1\n",
    "          include num_filters_3 only if num_conv_layers > 2\n",
    "          include sgd_momentum  only if       optimizer = SGD\n",
    "        \"\"\"\n",
    "        # START TODO ################\n",
    "        cs = CS.ConfigurationSpace()\n",
    "        \n",
    "        hyper_list = []\n",
    "        hyper_list.append(CSH.UniformIntegerHyperparameter(name = 'num_filters_1', lower = 4, upper = 32))\n",
    "\n",
    "        num_filters_2 = CSH.UniformIntegerHyperparameter(name = 'num_filters_2', lower = 4, upper = 32)\n",
    "        hyper_list.append(num_filters_2) \n",
    "\n",
    "        num_filters_3 = CSH.UniformIntegerHyperparameter(name = 'num_filters_3', lower = 4, upper = 32)\n",
    "        hyper_list.append(num_filters_3)\n",
    "        \n",
    "        num_conv_layers = CSH.UniformIntegerHyperparameter(name = 'num_conv_layers', lower = 1, upper = 3)\n",
    "        hyper_list.append(num_conv_layers)\n",
    "        \n",
    "        hyper_list.append(CSH.UniformFloatHyperparameter(name = 'lr', lower = 1e-6, upper = 1,log=True))\n",
    "        \n",
    "        sgd_momentum = CSH.UniformFloatHyperparameter(name = 'sgd_momentum', lower = 0.00, upper = 0.99)\n",
    "        hyper_list.append(sgd_momentum)\n",
    "        \n",
    "        optimizer = CSH.CategoricalHyperparameter('optimizer',['Adam', 'SGD']) #categoric\n",
    "        hyper_list.append(optimizer)\n",
    "        \n",
    "        cs.add_hyperparameters(hyper_list)\n",
    "        \n",
    "        cond1 = CS.conditions.GreaterThanCondition(num_filters_2, num_conv_layers, 1)\n",
    "        cond2 = CS.conditions.GreaterThanCondition(num_filters_3, num_conv_layers, 2)\n",
    "        cond3 = CS.conditions.EqualsCondition(sgd_momentum, optimizer, 'SGD')\n",
    "        \n",
    "        cs.add_conditions([cond1, cond2, cond3])\n",
    "        \n",
    "        return cs\n",
    "        # END TODO ################\n",
    "\n",
    "    def compute(self, config: CS.Configuration, budget: float, working_directory: str,\n",
    "                *args, **kwargs) -> float:\n",
    "      \"\"\"Evaluate a function with the given config and budget and return a loss.\n",
    "\n",
    "      Bohb tries to minimize the returned loss.\n",
    "\n",
    "      In our case the function is the training and validation of a model,\n",
    "      the budget is the number of epochs and the loss is the validation error.\n",
    "      \"\"\"\n",
    "      model = self.get_model(config)\n",
    "\n",
    "      # START TODO ################\n",
    "      if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(),lr=config['lr'])\n",
    "      else:\n",
    "        optimizer = optim.SGD(model.parameters(),lr=config['lr'], momentum=config['sgd_momentum'])\n",
    "\n",
    "      #val_errors = []\n",
    "      for epoch in range(1, int(budget) + 1):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "          optimizer.zero_grad()\n",
    "          output = model(data)\n",
    "          loss = F.nll_loss(output, target)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "      # END TODO ###############\n",
    "      train_accuracy = evaluate_accuracy(model, self.train_loader)\n",
    "      validation_accuracy = evaluate_accuracy(model, self.validation_loader)\n",
    "      test_accuracy = evaluate_accuracy(model, self.test_loader)\n",
    "\n",
    "      return ({\n",
    "              'loss': 1 - validation_accuracy,  # remember: HpBandSter minimizes the loss!\n",
    "              'info': {'test_accuracy': test_accuracy,\n",
    "                       'train_accuracy': train_accuracy,\n",
    "                       'valid_accuracy': validation_accuracy,\n",
    "                       'model': str(model)}\n",
    "              })\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqNOpdUyj6Bo"
   },
   "source": [
    "It's best practice to do a quick sanity check of our worker with a low budget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdC1XYC-j6Bp"
   },
   "outputs": [],
   "source": [
    "working_dir = os.curdir\n",
    "# minimum budget that BOHB uses\n",
    "min_budget = 1\n",
    "# largest budget BOHB will use\n",
    "max_budget = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "id": "CZeAxIHLj6Br",
    "outputId": "651bc157-16ba-4771-c0bb-363c20b3858d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 5.458013447893234e-06, 'num_conv_layers': 2, 'num_filters_1': 21, 'optimizer': 'SGD', 'num_filters_2': 26, 'sgd_momentum': 0.5318108650894474}\n",
      "{'loss': 0.923828125, 'info': {'test_accuracy': 0.0774, 'train_accuracy': 0.066650390625, 'valid_accuracy': 0.076171875, 'model': 'Sequential(\\n  (0): Conv2d(1, 21, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(21, 26, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=12584, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n"
     ]
    }
   ],
   "source": [
    "worker = PyTorchWorker(run_id='0')\n",
    "cs = worker.get_configspace()\n",
    "\n",
    "config = cs.sample_configuration().get_dictionary()\n",
    "print(config)\n",
    "\n",
    "res = worker.compute(config=config, budget=min_budget, working_directory=working_dir)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLLIuEyzj6Bt"
   },
   "source": [
    "### Run BOHB\n",
    "\n",
    "We now run the hyperparameter search with BOHB and the worker which we defined above and save the result to disk. \n",
    "Try to understand what happens. *HpBandSter* allows to start additional workers on the same or remote devices to parallelize the executions, that's why we need to setup some network stuff (nameserver, nic, host, port, …). If you're interested, you can check this out in the [HpBandSter Examples](https://automl.github.io/HpBandSter/build/html/auto_examples/index.html), but it is beyond the scope of this exercise.\n",
    "\n",
    "**Note:** The code below will try 60 different configurations. Some of them are executed at multiple budgets, which results in about 80 model training. Therefore it might take a while (15-45 minutes on a laptop). If you are interested in how BOHB works, checkout [BOHB (Falkner et al. 2018)](http://proceedings.mlr.press/v80/falkner18a.html).\n",
    "\n",
    "**Note 2:** You can see the progress in the debug output below. The configuration identifier (called `config_id` in the docs) is a three-tuple `(current iteration, resampling in case of error, sample)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swkcHiN_j6Bt"
   },
   "outputs": [],
   "source": [
    "result_file = os.path.join(working_dir, 'bohb_result.pkl')\n",
    "nic_name = 'lo'\n",
    "port = 0\n",
    "run_id = 'bohb_run_1'\n",
    "n_bohb_iterations = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49660
    },
    "colab_type": "code",
    "id": "QLuyPZZyj6Bw",
    "outputId": "6d9eb47b-737a-455c-bbbd-6e0169f20586"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:11:29 wait_for_workers trying to get the condition\n",
      "12:11:29 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x7f9348a9fc88; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:44529>\n",
      "12:11:29 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "12:11:29 WORKER: start listening for jobs\n",
      "12:11:29 DISPATCHER: started the 'discover_worker' thread\n",
      "12:11:29 DISPATCHER: started the 'job_runner' thread\n",
      "12:11:29 DISPATCHER: Pyro daemon running on 127.0.0.1:45549\n",
      "12:11:29 DISPATCHER: Starting worker discovery\n",
      "12:11:29 DISPATCHER: Found 1 potential workers, 0 currently in the pool.\n",
      "12:11:29 DISPATCHER: discovered new worker, hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:11:29 HBMASTER: number of workers changed to 1\n",
      "12:11:29 Enough workers to start this run!\n",
      "12:11:29 HBMASTER: starting run at 1543752689.7397707\n",
      "12:11:29 adjust_queue_size: lock accquired\n",
      "12:11:29 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:11:29 HBMASTER: adjusted queue size to (0, 1)\n",
      "12:11:29 DISPATCHER: Finished worker discovery\n",
      "12:11:29 start sampling a new configuration.\n",
      "12:11:29 DISPATCHER: Trying to submit another job.\n",
      "12:11:29 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:11:29 done sampling a new configuration.\n",
      "12:11:29 HBMASTER: schedule new run for iteration 0\n",
      "12:11:29 HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
      "12:11:29 HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
      "12:11:29 DISPATCHER: trying to submit job (0, 0, 0)\n",
      "12:11:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:11:29 HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
      "12:11:29 DISPATCHER: Trying to submit another job.\n",
      "12:11:29 DISPATCHER: starting job (0, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:11:29 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:11:29 DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:11:29 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:11:29 WORKER: start processing job (0, 0, 0)\n",
      "12:11:29 WORKER: args: ()\n",
      "12:11:29 WORKER: kwargs: {'config': {'lr': 0.012831003903722254, 'num_conv_layers': 3, 'num_filters_1': 24, 'optimizer': 'SGD', 'num_filters_2': 9, 'num_filters_3': 28, 'sgd_momentum': 0.33256631166614137}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:11:52 WORKER: done with job (0, 0, 0), trying to register it.\n",
      "12:11:52 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
      "12:11:52 DISPATCHER: job (0, 0, 0) finished\n",
      "12:11:52 DISPATCHER: register_result: lock acquired\n",
      "12:11:52 DISPATCHER: job (0, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:11:52 job_id: (0, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.012831003903722254, 'num_conv_layers': 3, 'num_filters_1': 24, 'optimizer': 'SGD', 'num_filters_2': 9, 'num_filters_3': 28, 'sgd_momentum': 0.33256631166614137}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.193359375, 'info': {'test_accuracy': 0.8118, 'train_accuracy': 0.824951171875, 'valid_accuracy': 0.806640625, 'model': 'Sequential(\\n  (0): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(24, 9, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(9, 28, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=10108, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:11:52 job_callback for (0, 0, 0) started\n",
      "12:11:52 DISPATCHER: Trying to submit another job.\n",
      "12:11:52 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:11:52 job_callback for (0, 0, 0) got condition\n",
      "12:11:52 Only 1 run(s) for budget 1.000000 available, need more than 9 -> can't build model!\n",
      "12:11:52 HBMASTER: Trying to run another job!\n",
      "12:11:52 job_callback for (0, 0, 0) finished\n",
      "12:11:52 start sampling a new configuration.\n",
      "12:11:52 done sampling a new configuration.\n",
      "12:11:52 HBMASTER: schedule new run for iteration 0\n",
      "12:11:52 HBMASTER: trying submitting job (0, 0, 1) to dispatcher\n",
      "12:11:52 HBMASTER: submitting job (0, 0, 1) to dispatcher\n",
      "12:11:52 DISPATCHER: trying to submit job (0, 0, 1)\n",
      "12:11:52 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:11:52 HBMASTER: job (0, 0, 1) submitted to dispatcher\n",
      "12:11:52 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:11:52 DISPATCHER: Trying to submit another job.\n",
      "12:11:52 DISPATCHER: starting job (0, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:11:52 DISPATCHER: job (0, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:11:52 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:11:52 WORKER: start processing job (0, 0, 1)\n",
      "12:11:52 WORKER: args: ()\n",
      "12:11:52 WORKER: kwargs: {'config': {'lr': 0.00062471585746314, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:12:05 WORKER: done with job (0, 0, 1), trying to register it.\n",
      "12:12:05 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "12:12:05 DISPATCHER: job (0, 0, 1) finished\n",
      "12:12:05 DISPATCHER: register_result: lock acquired\n",
      "12:12:05 DISPATCHER: job (0, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:12:05 job_id: (0, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.00062471585746314, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.08203125, 'info': {'test_accuracy': 0.9131, 'train_accuracy': 0.92919921875, 'valid_accuracy': 0.91796875, 'model': 'Sequential(\\n  (0): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=19375, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:12:05 job_callback for (0, 0, 1) started\n",
      "12:12:05 DISPATCHER: Trying to submit another job.\n",
      "12:12:05 job_callback for (0, 0, 1) got condition\n",
      "12:12:05 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:12:05 Only 2 run(s) for budget 1.000000 available, need more than 9 -> can't build model!\n",
      "12:12:05 HBMASTER: Trying to run another job!\n",
      "12:12:05 job_callback for (0, 0, 1) finished\n",
      "12:12:05 start sampling a new configuration.\n",
      "12:12:05 done sampling a new configuration.\n",
      "12:12:05 HBMASTER: schedule new run for iteration 0\n",
      "12:12:05 HBMASTER: trying submitting job (0, 0, 2) to dispatcher\n",
      "12:12:05 HBMASTER: submitting job (0, 0, 2) to dispatcher\n",
      "12:12:05 DISPATCHER: trying to submit job (0, 0, 2)\n",
      "12:12:05 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:12:05 HBMASTER: job (0, 0, 2) submitted to dispatcher\n",
      "12:12:05 DISPATCHER: Trying to submit another job.\n",
      "12:12:05 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:12:05 DISPATCHER: starting job (0, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:12:05 DISPATCHER: job (0, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:12:05 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:12:05 WORKER: start processing job (0, 0, 2)\n",
      "12:12:05 WORKER: args: ()\n",
      "12:12:05 WORKER: kwargs: {'config': {'lr': 8.987764234087497e-05, 'num_conv_layers': 3, 'num_filters_1': 16, 'optimizer': 'Adam', 'num_filters_2': 17, 'num_filters_3': 19}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:12:27 WORKER: done with job (0, 0, 2), trying to register it.\n",
      "12:12:27 WORKER: registered result for job (0, 0, 2) with dispatcher\n",
      "12:12:27 DISPATCHER: job (0, 0, 2) finished\n",
      "12:12:27 DISPATCHER: register_result: lock acquired\n",
      "12:12:27 DISPATCHER: job (0, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:12:27 job_id: (0, 0, 2)\n",
      "kwargs: {'config': {'lr': 8.987764234087497e-05, 'num_conv_layers': 3, 'num_filters_1': 16, 'optimizer': 'Adam', 'num_filters_2': 17, 'num_filters_3': 19}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.19140625, 'info': {'test_accuracy': 0.8077, 'train_accuracy': 0.80126953125, 'valid_accuracy': 0.80859375, 'model': 'Sequential(\\n  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(17, 19, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=6859, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:12:27 job_callback for (0, 0, 2) started\n",
      "12:12:27 DISPATCHER: Trying to submit another job.\n",
      "12:12:27 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:12:27 job_callback for (0, 0, 2) got condition\n",
      "12:12:27 Only 3 run(s) for budget 1.000000 available, need more than 9 -> can't build model!\n",
      "12:12:27 HBMASTER: Trying to run another job!\n",
      "12:12:27 job_callback for (0, 0, 2) finished\n",
      "12:12:27 start sampling a new configuration.\n",
      "12:12:27 done sampling a new configuration.\n",
      "12:12:27 HBMASTER: schedule new run for iteration 0\n",
      "12:12:27 HBMASTER: trying submitting job (0, 0, 3) to dispatcher\n",
      "12:12:27 HBMASTER: submitting job (0, 0, 3) to dispatcher\n",
      "12:12:27 DISPATCHER: trying to submit job (0, 0, 3)\n",
      "12:12:27 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:12:27 HBMASTER: job (0, 0, 3) submitted to dispatcher\n",
      "12:12:27 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:12:27 DISPATCHER: Trying to submit another job.\n",
      "12:12:27 DISPATCHER: starting job (0, 0, 3) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:12:27 DISPATCHER: job (0, 0, 3) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:12:27 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:12:27 WORKER: start processing job (0, 0, 3)\n",
      "12:12:27 WORKER: args: ()\n",
      "12:12:27 WORKER: kwargs: {'config': {'lr': 6.508880821392936e-05, 'num_conv_layers': 2, 'num_filters_1': 30, 'optimizer': 'SGD', 'num_filters_2': 24, 'sgd_momentum': 0.8955419573112654}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:12:29 DISPATCHER: Starting worker discovery\n",
      "12:12:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:12:29 DISPATCHER: Finished worker discovery\n",
      "12:12:53 WORKER: done with job (0, 0, 3), trying to register it.\n",
      "12:12:53 WORKER: registered result for job (0, 0, 3) with dispatcher\n",
      "12:12:53 DISPATCHER: job (0, 0, 3) finished\n",
      "12:12:53 DISPATCHER: register_result: lock acquired\n",
      "12:12:53 DISPATCHER: job (0, 0, 3) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:12:53 job_id: (0, 0, 3)\n",
      "kwargs: {'config': {'lr': 6.508880821392936e-05, 'num_conv_layers': 2, 'num_filters_1': 30, 'optimizer': 'SGD', 'num_filters_2': 24, 'sgd_momentum': 0.8955419573112654}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.599609375, 'info': {'test_accuracy': 0.4003, 'train_accuracy': 0.402587890625, 'valid_accuracy': 0.400390625, 'model': 'Sequential(\\n  (0): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(30, 24, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=11616, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:12:53 job_callback for (0, 0, 3) started\n",
      "12:12:53 DISPATCHER: Trying to submit another job.\n",
      "12:12:53 job_callback for (0, 0, 3) got condition\n",
      "12:12:53 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:12:53 Only 4 run(s) for budget 1.000000 available, need more than 9 -> can't build model!\n",
      "12:12:53 HBMASTER: Trying to run another job!\n",
      "12:12:53 job_callback for (0, 0, 3) finished\n",
      "12:12:53 start sampling a new configuration.\n",
      "12:12:53 done sampling a new configuration.\n",
      "12:12:53 HBMASTER: schedule new run for iteration 0\n",
      "12:12:53 HBMASTER: trying submitting job (0, 0, 4) to dispatcher\n",
      "12:12:53 HBMASTER: submitting job (0, 0, 4) to dispatcher\n",
      "12:12:53 DISPATCHER: trying to submit job (0, 0, 4)\n",
      "12:12:53 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:12:54 HBMASTER: job (0, 0, 4) submitted to dispatcher\n",
      "12:12:54 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:12:54 DISPATCHER: Trying to submit another job.\n",
      "12:12:54 DISPATCHER: starting job (0, 0, 4) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:12:54 DISPATCHER: job (0, 0, 4) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:12:54 WORKER: start processing job (0, 0, 4)\n",
      "12:12:54 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:12:54 WORKER: args: ()\n",
      "12:12:54 WORKER: kwargs: {'config': {'lr': 2.7710818713923083e-05, 'num_conv_layers': 3, 'num_filters_1': 12, 'optimizer': 'SGD', 'num_filters_2': 23, 'num_filters_3': 12, 'sgd_momentum': 0.23929394203788984}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:13:14 WORKER: done with job (0, 0, 4), trying to register it.\n",
      "12:13:14 WORKER: registered result for job (0, 0, 4) with dispatcher\n",
      "12:13:14 DISPATCHER: job (0, 0, 4) finished\n",
      "12:13:14 DISPATCHER: register_result: lock acquired\n",
      "12:13:15 DISPATCHER: job (0, 0, 4) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:13:15 job_id: (0, 0, 4)\n",
      "kwargs: {'config': {'lr': 2.7710818713923083e-05, 'num_conv_layers': 3, 'num_filters_1': 12, 'optimizer': 'SGD', 'num_filters_2': 23, 'num_filters_3': 12, 'sgd_momentum': 0.23929394203788984}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.8984375, 'info': {'test_accuracy': 0.098, 'train_accuracy': 0.094970703125, 'valid_accuracy': 0.1015625, 'model': 'Sequential(\\n  (0): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(12, 23, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(23, 12, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=4332, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:13:15 job_callback for (0, 0, 4) started\n",
      "12:13:15 DISPATCHER: Trying to submit another job.\n",
      "12:13:15 job_callback for (0, 0, 4) got condition\n",
      "12:13:15 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:13:15 Only 5 run(s) for budget 1.000000 available, need more than 9 -> can't build model!\n",
      "12:13:15 HBMASTER: Trying to run another job!\n",
      "12:13:15 job_callback for (0, 0, 4) finished\n",
      "12:13:15 start sampling a new configuration.\n",
      "12:13:15 done sampling a new configuration.\n",
      "12:13:15 HBMASTER: schedule new run for iteration 0\n",
      "12:13:15 HBMASTER: trying submitting job (0, 0, 5) to dispatcher\n",
      "12:13:15 HBMASTER: submitting job (0, 0, 5) to dispatcher\n",
      "12:13:15 DISPATCHER: trying to submit job (0, 0, 5)\n",
      "12:13:15 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:13:15 HBMASTER: job (0, 0, 5) submitted to dispatcher\n",
      "12:13:15 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:13:15 DISPATCHER: Trying to submit another job.\n",
      "12:13:15 DISPATCHER: starting job (0, 0, 5) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:13:15 DISPATCHER: job (0, 0, 5) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:13:15 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:13:15 WORKER: start processing job (0, 0, 5)\n",
      "12:13:15 WORKER: args: ()\n",
      "12:13:15 WORKER: kwargs: {'config': {'lr': 2.2795179517618962e-05, 'num_conv_layers': 2, 'num_filters_1': 25, 'optimizer': 'SGD', 'num_filters_2': 30, 'sgd_momentum': 0.12834876735825126}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:13:29 DISPATCHER: Starting worker discovery\n",
      "12:13:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:13:29 DISPATCHER: Finished worker discovery\n",
      "12:13:40 WORKER: done with job (0, 0, 5), trying to register it.\n",
      "12:13:40 WORKER: registered result for job (0, 0, 5) with dispatcher\n",
      "12:13:40 DISPATCHER: job (0, 0, 5) finished\n",
      "12:13:40 DISPATCHER: register_result: lock acquired\n",
      "12:13:40 DISPATCHER: job (0, 0, 5) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:13:40 job_id: (0, 0, 5)\n",
      "kwargs: {'config': {'lr': 2.2795179517618962e-05, 'num_conv_layers': 2, 'num_filters_1': 25, 'optimizer': 'SGD', 'num_filters_2': 30, 'sgd_momentum': 0.12834876735825126}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.88671875, 'info': {'test_accuracy': 0.102, 'train_accuracy': 0.10498046875, 'valid_accuracy': 0.11328125, 'model': 'Sequential(\\n  (0): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(25, 30, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=14520, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:13:40 job_callback for (0, 0, 5) started\n",
      "12:13:40 DISPATCHER: Trying to submit another job.\n",
      "12:13:40 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:13:40 job_callback for (0, 0, 5) got condition\n",
      "12:13:40 Only 6 run(s) for budget 1.000000 available, need more than 9 -> can't build model!\n",
      "12:13:40 HBMASTER: Trying to run another job!\n",
      "12:13:40 job_callback for (0, 0, 5) finished\n",
      "12:13:40 start sampling a new configuration.\n",
      "12:13:40 done sampling a new configuration.\n",
      "12:13:40 HBMASTER: schedule new run for iteration 0\n",
      "12:13:40 HBMASTER: trying submitting job (0, 0, 6) to dispatcher\n",
      "12:13:40 HBMASTER: submitting job (0, 0, 6) to dispatcher\n",
      "12:13:40 DISPATCHER: trying to submit job (0, 0, 6)\n",
      "12:13:40 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:13:40 HBMASTER: job (0, 0, 6) submitted to dispatcher\n",
      "12:13:40 DISPATCHER: Trying to submit another job.\n",
      "12:13:40 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:13:40 DISPATCHER: starting job (0, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:13:40 DISPATCHER: job (0, 0, 6) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:13:40 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:13:40 WORKER: start processing job (0, 0, 6)\n",
      "12:13:40 WORKER: args: ()\n",
      "12:13:40 WORKER: kwargs: {'config': {'lr': 2.9779711900846894e-05, 'num_conv_layers': 1, 'num_filters_1': 27, 'optimizer': 'SGD', 'sgd_momentum': 0.2054840534604336}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:13:52 WORKER: done with job (0, 0, 6), trying to register it.\n",
      "12:13:52 WORKER: registered result for job (0, 0, 6) with dispatcher\n",
      "12:13:52 DISPATCHER: job (0, 0, 6) finished\n",
      "12:13:52 DISPATCHER: register_result: lock acquired\n",
      "12:13:52 DISPATCHER: job (0, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:13:52 job_id: (0, 0, 6)\n",
      "kwargs: {'config': {'lr': 2.9779711900846894e-05, 'num_conv_layers': 1, 'num_filters_1': 27, 'optimizer': 'SGD', 'sgd_momentum': 0.2054840534604336}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.8515625, 'info': {'test_accuracy': 0.1361, 'train_accuracy': 0.15625, 'valid_accuracy': 0.1484375, 'model': 'Sequential(\\n  (0): Conv2d(1, 27, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=16875, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:13:52 job_callback for (0, 0, 6) started\n",
      "12:13:52 DISPATCHER: Trying to submit another job.\n",
      "12:13:52 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:13:52 job_callback for (0, 0, 6) got condition\n",
      "12:13:52 Only 7 run(s) for budget 1.000000 available, need more than 9 -> can't build model!\n",
      "12:13:52 HBMASTER: Trying to run another job!\n",
      "12:13:52 job_callback for (0, 0, 6) finished\n",
      "12:13:52 start sampling a new configuration.\n",
      "12:13:52 done sampling a new configuration.\n",
      "12:13:52 HBMASTER: schedule new run for iteration 0\n",
      "12:13:52 HBMASTER: trying submitting job (0, 0, 7) to dispatcher\n",
      "12:13:52 HBMASTER: submitting job (0, 0, 7) to dispatcher\n",
      "12:13:52 DISPATCHER: trying to submit job (0, 0, 7)\n",
      "12:13:52 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:13:52 HBMASTER: job (0, 0, 7) submitted to dispatcher\n",
      "12:13:52 DISPATCHER: Trying to submit another job.\n",
      "12:13:52 DISPATCHER: starting job (0, 0, 7) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:13:52 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:13:52 DISPATCHER: job (0, 0, 7) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:13:52 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:13:52 WORKER: start processing job (0, 0, 7)\n",
      "12:13:52 WORKER: args: ()\n",
      "12:13:52 WORKER: kwargs: {'config': {'lr': 1.122193299279558e-05, 'num_conv_layers': 2, 'num_filters_1': 28, 'optimizer': 'Adam', 'num_filters_2': 7}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:14:10 WORKER: done with job (0, 0, 7), trying to register it.\n",
      "12:14:10 WORKER: registered result for job (0, 0, 7) with dispatcher\n",
      "12:14:10 DISPATCHER: job (0, 0, 7) finished\n",
      "12:14:10 DISPATCHER: register_result: lock acquired\n",
      "12:14:10 DISPATCHER: job (0, 0, 7) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:14:10 job_id: (0, 0, 7)\n",
      "kwargs: {'config': {'lr': 1.122193299279558e-05, 'num_conv_layers': 2, 'num_filters_1': 28, 'optimizer': 'Adam', 'num_filters_2': 7}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.7265625, 'info': {'test_accuracy': 0.2564, 'train_accuracy': 0.273681640625, 'valid_accuracy': 0.2734375, 'model': 'Sequential(\\n  (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(28, 7, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=3388, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:14:10 job_callback for (0, 0, 7) started\n",
      "12:14:10 DISPATCHER: Trying to submit another job.\n",
      "12:14:10 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:14:10 job_callback for (0, 0, 7) got condition\n",
      "12:14:10 HBMASTER: Trying to run another job!\n",
      "12:14:10 job_callback for (0, 0, 7) finished\n",
      "12:14:10 start sampling a new configuration.\n",
      "12:14:10 done sampling a new configuration.\n",
      "12:14:10 HBMASTER: schedule new run for iteration 0\n",
      "12:14:10 HBMASTER: trying submitting job (0, 0, 8) to dispatcher\n",
      "12:14:10 HBMASTER: submitting job (0, 0, 8) to dispatcher\n",
      "12:14:10 DISPATCHER: trying to submit job (0, 0, 8)\n",
      "12:14:10 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:14:10 HBMASTER: job (0, 0, 8) submitted to dispatcher\n",
      "12:14:10 DISPATCHER: Trying to submit another job.\n",
      "12:14:10 DISPATCHER: starting job (0, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:14:10 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:14:10 DISPATCHER: job (0, 0, 8) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:14:10 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:14:10 WORKER: start processing job (0, 0, 8)\n",
      "12:14:10 WORKER: args: ()\n",
      "12:14:10 WORKER: kwargs: {'config': {'lr': 0.00019413398968731022, 'num_conv_layers': 2, 'num_filters_1': 28, 'optimizer': 'SGD', 'num_filters_2': 31, 'sgd_momentum': 0.006433940917154408}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:14:29 DISPATCHER: Starting worker discovery\n",
      "12:14:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:14:29 DISPATCHER: Finished worker discovery\n",
      "12:14:36 WORKER: done with job (0, 0, 8), trying to register it.\n",
      "12:14:36 WORKER: registered result for job (0, 0, 8) with dispatcher\n",
      "12:14:36 DISPATCHER: job (0, 0, 8) finished\n",
      "12:14:36 DISPATCHER: register_result: lock acquired\n",
      "12:14:36 DISPATCHER: job (0, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:14:36 job_id: (0, 0, 8)\n",
      "kwargs: {'config': {'lr': 0.00019413398968731022, 'num_conv_layers': 2, 'num_filters_1': 28, 'optimizer': 'SGD', 'num_filters_2': 31, 'sgd_momentum': 0.006433940917154408}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.8203125, 'info': {'test_accuracy': 0.1892, 'train_accuracy': 0.173583984375, 'valid_accuracy': 0.1796875, 'model': 'Sequential(\\n  (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(28, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=15004, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:14:36 job_callback for (0, 0, 8) started\n",
      "12:14:36 job_callback for (0, 0, 8) got condition\n",
      "12:14:36 DISPATCHER: Trying to submit another job.\n",
      "12:14:36 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:14:36 HBMASTER: Trying to run another job!\n",
      "12:14:36 job_callback for (0, 0, 8) finished\n",
      "12:14:36 ITERATION: Advancing config (0, 0, 0) to next budget 3.000000\n",
      "12:14:36 ITERATION: Advancing config (0, 0, 1) to next budget 3.000000\n",
      "12:14:36 ITERATION: Advancing config (0, 0, 2) to next budget 3.000000\n",
      "12:14:36 HBMASTER: schedule new run for iteration 0\n",
      "12:14:36 HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
      "12:14:36 HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
      "12:14:36 DISPATCHER: trying to submit job (0, 0, 0)\n",
      "12:14:36 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:14:36 HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
      "12:14:36 DISPATCHER: Trying to submit another job.\n",
      "12:14:36 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:14:36 DISPATCHER: starting job (0, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:14:36 DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:14:36 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:14:36 WORKER: start processing job (0, 0, 0)\n",
      "12:14:36 WORKER: args: ()\n",
      "12:14:36 WORKER: kwargs: {'config': {'lr': 0.012831003903722254, 'num_conv_layers': 3, 'num_filters_1': 24, 'optimizer': 'SGD', 'num_filters_2': 9, 'num_filters_3': 28, 'sgd_momentum': 0.33256631166614137}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:15:23 WORKER: done with job (0, 0, 0), trying to register it.\n",
      "12:15:23 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
      "12:15:23 DISPATCHER: job (0, 0, 0) finished\n",
      "12:15:23 DISPATCHER: register_result: lock acquired\n",
      "12:15:23 DISPATCHER: job (0, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:15:23 job_id: (0, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.012831003903722254, 'num_conv_layers': 3, 'num_filters_1': 24, 'optimizer': 'SGD', 'num_filters_2': 9, 'num_filters_3': 28, 'sgd_momentum': 0.33256631166614137}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.091796875, 'info': {'test_accuracy': 0.908, 'train_accuracy': 0.933349609375, 'valid_accuracy': 0.908203125, 'model': 'Sequential(\\n  (0): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(24, 9, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(9, 28, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=10108, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:15:23 job_callback for (0, 0, 0) started\n",
      "12:15:23 DISPATCHER: Trying to submit another job.\n",
      "12:15:23 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:15:23 job_callback for (0, 0, 0) got condition\n",
      "12:15:23 Only 1 run(s) for budget 3.000000 available, need more than 9 -> can't build model!\n",
      "12:15:23 HBMASTER: Trying to run another job!\n",
      "12:15:23 job_callback for (0, 0, 0) finished\n",
      "12:15:23 HBMASTER: schedule new run for iteration 0\n",
      "12:15:23 HBMASTER: trying submitting job (0, 0, 1) to dispatcher\n",
      "12:15:23 HBMASTER: submitting job (0, 0, 1) to dispatcher\n",
      "12:15:23 DISPATCHER: trying to submit job (0, 0, 1)\n",
      "12:15:23 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:15:23 HBMASTER: job (0, 0, 1) submitted to dispatcher\n",
      "12:15:23 DISPATCHER: Trying to submit another job.\n",
      "12:15:23 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:15:23 DISPATCHER: starting job (0, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:15:23 DISPATCHER: job (0, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:15:23 WORKER: start processing job (0, 0, 1)\n",
      "12:15:23 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:15:23 WORKER: args: ()\n",
      "12:15:23 WORKER: kwargs: {'config': {'lr': 0.00062471585746314, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:15:29 DISPATCHER: Starting worker discovery\n",
      "12:15:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:15:29 DISPATCHER: Finished worker discovery\n",
      "12:15:48 WORKER: done with job (0, 0, 1), trying to register it.\n",
      "12:15:48 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "12:15:48 DISPATCHER: job (0, 0, 1) finished\n",
      "12:15:48 DISPATCHER: register_result: lock acquired\n",
      "12:15:48 DISPATCHER: job (0, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:15:48 job_id: (0, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.00062471585746314, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.056640625, 'info': {'test_accuracy': 0.9356, 'train_accuracy': 0.9658203125, 'valid_accuracy': 0.943359375, 'model': 'Sequential(\\n  (0): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=19375, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:15:48 job_callback for (0, 0, 1) started\n",
      "12:15:48 DISPATCHER: Trying to submit another job.\n",
      "12:15:48 job_callback for (0, 0, 1) got condition\n",
      "12:15:48 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:15:48 Only 2 run(s) for budget 3.000000 available, need more than 9 -> can't build model!\n",
      "12:15:48 HBMASTER: Trying to run another job!\n",
      "12:15:48 job_callback for (0, 0, 1) finished\n",
      "12:15:48 HBMASTER: schedule new run for iteration 0\n",
      "12:15:48 HBMASTER: trying submitting job (0, 0, 2) to dispatcher\n",
      "12:15:48 HBMASTER: submitting job (0, 0, 2) to dispatcher\n",
      "12:15:48 DISPATCHER: trying to submit job (0, 0, 2)\n",
      "12:15:48 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:15:48 HBMASTER: job (0, 0, 2) submitted to dispatcher\n",
      "12:15:48 DISPATCHER: Trying to submit another job.\n",
      "12:15:48 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:15:48 DISPATCHER: starting job (0, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:15:48 DISPATCHER: job (0, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:15:48 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:15:48 WORKER: start processing job (0, 0, 2)\n",
      "12:15:48 WORKER: args: ()\n",
      "12:15:48 WORKER: kwargs: {'config': {'lr': 8.987764234087497e-05, 'num_conv_layers': 3, 'num_filters_1': 16, 'optimizer': 'Adam', 'num_filters_2': 17, 'num_filters_3': 19}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:16:29 DISPATCHER: Starting worker discovery\n",
      "12:16:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:16:29 DISPATCHER: Finished worker discovery\n",
      "12:16:31 WORKER: done with job (0, 0, 2), trying to register it.\n",
      "12:16:31 WORKER: registered result for job (0, 0, 2) with dispatcher\n",
      "12:16:31 DISPATCHER: job (0, 0, 2) finished\n",
      "12:16:31 DISPATCHER: register_result: lock acquired\n",
      "12:16:31 DISPATCHER: job (0, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:16:31 job_id: (0, 0, 2)\n",
      "kwargs: {'config': {'lr': 8.987764234087497e-05, 'num_conv_layers': 3, 'num_filters_1': 16, 'optimizer': 'Adam', 'num_filters_2': 17, 'num_filters_3': 19}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.130859375, 'info': {'test_accuracy': 0.8717, 'train_accuracy': 0.884521484375, 'valid_accuracy': 0.869140625, 'model': 'Sequential(\\n  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(17, 19, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=6859, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:16:31 job_callback for (0, 0, 2) started\n",
      "12:16:31 DISPATCHER: Trying to submit another job.\n",
      "12:16:31 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:16:31 job_callback for (0, 0, 2) got condition\n",
      "12:16:31 Only 3 run(s) for budget 3.000000 available, need more than 9 -> can't build model!\n",
      "12:16:31 HBMASTER: Trying to run another job!\n",
      "12:16:31 job_callback for (0, 0, 2) finished\n",
      "12:16:31 ITERATION: Advancing config (0, 0, 1) to next budget 9.000000\n",
      "12:16:31 HBMASTER: schedule new run for iteration 0\n",
      "12:16:31 HBMASTER: trying submitting job (0, 0, 1) to dispatcher\n",
      "12:16:31 HBMASTER: submitting job (0, 0, 1) to dispatcher\n",
      "12:16:31 DISPATCHER: trying to submit job (0, 0, 1)\n",
      "12:16:31 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:16:31 HBMASTER: job (0, 0, 1) submitted to dispatcher\n",
      "12:16:31 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:16:31 DISPATCHER: Trying to submit another job.\n",
      "12:16:31 DISPATCHER: starting job (0, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:16:31 DISPATCHER: job (0, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:16:31 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:16:31 WORKER: start processing job (0, 0, 1)\n",
      "12:16:31 WORKER: args: ()\n",
      "12:16:31 WORKER: kwargs: {'config': {'lr': 0.00062471585746314, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'Adam'}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:17:25 WORKER: done with job (0, 0, 1), trying to register it.\n",
      "12:17:25 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "12:17:25 DISPATCHER: job (0, 0, 1) finished\n",
      "12:17:25 DISPATCHER: register_result: lock acquired\n",
      "12:17:25 DISPATCHER: job (0, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:17:25 job_id: (0, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.00062471585746314, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'Adam'}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.037109375, 'info': {'test_accuracy': 0.9586, 'train_accuracy': 1.0, 'valid_accuracy': 0.962890625, 'model': 'Sequential(\\n  (0): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=19375, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:17:25 job_callback for (0, 0, 1) started\n",
      "12:17:25 DISPATCHER: Trying to submit another job.\n",
      "12:17:25 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:17:25 job_callback for (0, 0, 1) got condition\n",
      "12:17:25 Only 1 run(s) for budget 9.000000 available, need more than 9 -> can't build model!\n",
      "12:17:25 HBMASTER: Trying to run another job!\n",
      "12:17:25 job_callback for (0, 0, 1) finished\n",
      "12:17:25 start sampling a new configuration.\n",
      "12:17:25 done sampling a new configuration.\n",
      "12:17:25 HBMASTER: schedule new run for iteration 1\n",
      "12:17:25 HBMASTER: trying submitting job (1, 0, 0) to dispatcher\n",
      "12:17:25 HBMASTER: submitting job (1, 0, 0) to dispatcher\n",
      "12:17:25 DISPATCHER: trying to submit job (1, 0, 0)\n",
      "12:17:25 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:17:25 HBMASTER: job (1, 0, 0) submitted to dispatcher\n",
      "12:17:25 DISPATCHER: Trying to submit another job.\n",
      "12:17:25 DISPATCHER: starting job (1, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:17:25 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:17:25 DISPATCHER: job (1, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:17:25 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:17:25 WORKER: start processing job (1, 0, 0)\n",
      "12:17:25 WORKER: args: ()\n",
      "12:17:25 WORKER: kwargs: {'config': {'lr': 5.480866712911336e-06, 'num_conv_layers': 1, 'num_filters_1': 11, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:17:29 DISPATCHER: Starting worker discovery\n",
      "12:17:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:17:29 DISPATCHER: Finished worker discovery\n",
      "12:17:34 WORKER: done with job (1, 0, 0), trying to register it.\n",
      "12:17:34 WORKER: registered result for job (1, 0, 0) with dispatcher\n",
      "12:17:34 DISPATCHER: job (1, 0, 0) finished\n",
      "12:17:34 DISPATCHER: register_result: lock acquired\n",
      "12:17:34 DISPATCHER: job (1, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:17:34 job_id: (1, 0, 0)\n",
      "kwargs: {'config': {'lr': 5.480866712911336e-06, 'num_conv_layers': 1, 'num_filters_1': 11, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.35546875, 'info': {'test_accuracy': 0.5915, 'train_accuracy': 0.62451171875, 'valid_accuracy': 0.64453125, 'model': 'Sequential(\\n  (0): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=6875, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:17:34 job_callback for (1, 0, 0) started\n",
      "12:17:34 job_callback for (1, 0, 0) got condition\n",
      "12:17:34 DISPATCHER: Trying to submit another job.\n",
      "12:17:34 Only 4 run(s) for budget 3.000000 available, need more than 9 -> can't build model!\n",
      "12:17:34 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:17:34 HBMASTER: Trying to run another job!\n",
      "12:17:34 job_callback for (1, 0, 0) finished\n",
      "12:17:34 start sampling a new configuration.\n",
      "12:17:34 done sampling a new configuration.\n",
      "12:17:34 HBMASTER: schedule new run for iteration 1\n",
      "12:17:34 HBMASTER: trying submitting job (1, 0, 1) to dispatcher\n",
      "12:17:34 HBMASTER: submitting job (1, 0, 1) to dispatcher\n",
      "12:17:34 DISPATCHER: trying to submit job (1, 0, 1)\n",
      "12:17:34 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:17:34 HBMASTER: job (1, 0, 1) submitted to dispatcher\n",
      "12:17:34 DISPATCHER: Trying to submit another job.\n",
      "12:17:34 DISPATCHER: starting job (1, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:17:34 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:17:34 DISPATCHER: job (1, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:17:34 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:17:34 WORKER: start processing job (1, 0, 1)\n",
      "12:17:34 WORKER: args: ()\n",
      "12:17:34 WORKER: kwargs: {'config': {'lr': 8.708177091869013e-06, 'num_conv_layers': 3, 'num_filters_1': 25, 'optimizer': 'SGD', 'num_filters_2': 12, 'num_filters_3': 9, 'sgd_momentum': 0.5550690900753505}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:18:22 WORKER: done with job (1, 0, 1), trying to register it.\n",
      "12:18:22 WORKER: registered result for job (1, 0, 1) with dispatcher\n",
      "12:18:22 DISPATCHER: job (1, 0, 1) finished\n",
      "12:18:22 DISPATCHER: register_result: lock acquired\n",
      "12:18:22 DISPATCHER: job (1, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:18:22 job_id: (1, 0, 1)\n",
      "kwargs: {'config': {'lr': 8.708177091869013e-06, 'num_conv_layers': 3, 'num_filters_1': 25, 'optimizer': 'SGD', 'num_filters_2': 12, 'num_filters_3': 9, 'sgd_momentum': 0.5550690900753505}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.908203125, 'info': {'test_accuracy': 0.0796, 'train_accuracy': 0.0771484375, 'valid_accuracy': 0.091796875, 'model': 'Sequential(\\n  (0): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(25, 12, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(12, 9, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=3249, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:18:22 job_callback for (1, 0, 1) started\n",
      "12:18:22 DISPATCHER: Trying to submit another job.\n",
      "12:18:22 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:18:22 job_callback for (1, 0, 1) got condition\n",
      "12:18:22 Only 5 run(s) for budget 3.000000 available, need more than 9 -> can't build model!\n",
      "12:18:22 HBMASTER: Trying to run another job!\n",
      "12:18:22 job_callback for (1, 0, 1) finished\n",
      "12:18:22 start sampling a new configuration.\n",
      "12:18:22 done sampling a new configuration.\n",
      "12:18:22 HBMASTER: schedule new run for iteration 1\n",
      "12:18:22 HBMASTER: trying submitting job (1, 0, 2) to dispatcher\n",
      "12:18:22 HBMASTER: submitting job (1, 0, 2) to dispatcher\n",
      "12:18:22 DISPATCHER: trying to submit job (1, 0, 2)\n",
      "12:18:22 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:18:22 HBMASTER: job (1, 0, 2) submitted to dispatcher\n",
      "12:18:22 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:18:22 DISPATCHER: Trying to submit another job.\n",
      "12:18:22 DISPATCHER: starting job (1, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:18:22 DISPATCHER: job (1, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:18:22 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:18:22 WORKER: start processing job (1, 0, 2)\n",
      "12:18:22 WORKER: args: ()\n",
      "12:18:22 WORKER: kwargs: {'config': {'lr': 0.025512011436978364, 'num_conv_layers': 1, 'num_filters_1': 14, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:18:29 DISPATCHER: Starting worker discovery\n",
      "12:18:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:18:29 DISPATCHER: Finished worker discovery\n",
      "12:18:35 WORKER: done with job (1, 0, 2), trying to register it.\n",
      "12:18:35 WORKER: registered result for job (1, 0, 2) with dispatcher\n",
      "12:18:35 DISPATCHER: job (1, 0, 2) finished\n",
      "12:18:35 DISPATCHER: register_result: lock acquired\n",
      "12:18:35 DISPATCHER: job (1, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:18:35 job_id: (1, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.025512011436978364, 'num_conv_layers': 1, 'num_filters_1': 14, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.064453125, 'info': {'test_accuracy': 0.9087, 'train_accuracy': 0.975341796875, 'valid_accuracy': 0.935546875, 'model': 'Sequential(\\n  (0): Conv2d(1, 14, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=8750, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:18:35 job_callback for (1, 0, 2) started\n",
      "12:18:35 DISPATCHER: Trying to submit another job.\n",
      "12:18:35 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:18:35 job_callback for (1, 0, 2) got condition\n",
      "12:18:35 Only 6 run(s) for budget 3.000000 available, need more than 9 -> can't build model!\n",
      "12:18:35 HBMASTER: Trying to run another job!\n",
      "12:18:35 job_callback for (1, 0, 2) finished\n",
      "12:18:35 ITERATION: Advancing config (1, 0, 2) to next budget 9.000000\n",
      "12:18:35 HBMASTER: schedule new run for iteration 1\n",
      "12:18:35 HBMASTER: trying submitting job (1, 0, 2) to dispatcher\n",
      "12:18:35 HBMASTER: submitting job (1, 0, 2) to dispatcher\n",
      "12:18:35 DISPATCHER: trying to submit job (1, 0, 2)\n",
      "12:18:35 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:18:35 HBMASTER: job (1, 0, 2) submitted to dispatcher\n",
      "12:18:35 DISPATCHER: Trying to submit another job.\n",
      "12:18:35 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:18:35 DISPATCHER: starting job (1, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:18:35 DISPATCHER: job (1, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:18:35 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:18:35 WORKER: start processing job (1, 0, 2)\n",
      "12:18:35 WORKER: args: ()\n",
      "12:18:35 WORKER: kwargs: {'config': {'lr': 0.025512011436978364, 'num_conv_layers': 1, 'num_filters_1': 14, 'optimizer': 'Adam'}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:19:09 WORKER: done with job (1, 0, 2), trying to register it.\n",
      "12:19:09 WORKER: registered result for job (1, 0, 2) with dispatcher\n",
      "12:19:09 DISPATCHER: job (1, 0, 2) finished\n",
      "12:19:09 DISPATCHER: register_result: lock acquired\n",
      "12:19:09 DISPATCHER: job (1, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:19:09 job_id: (1, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.025512011436978364, 'num_conv_layers': 1, 'num_filters_1': 14, 'optimizer': 'Adam'}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.046875, 'info': {'test_accuracy': 0.9417, 'train_accuracy': 0.99462890625, 'valid_accuracy': 0.953125, 'model': 'Sequential(\\n  (0): Conv2d(1, 14, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=8750, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:19:09 job_callback for (1, 0, 2) started\n",
      "12:19:09 DISPATCHER: Trying to submit another job.\n",
      "12:19:09 job_callback for (1, 0, 2) got condition\n",
      "12:19:09 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:19:09 Only 2 run(s) for budget 9.000000 available, need more than 9 -> can't build model!\n",
      "12:19:09 HBMASTER: Trying to run another job!\n",
      "12:19:09 job_callback for (1, 0, 2) finished\n",
      "12:19:09 start sampling a new configuration.\n",
      "12:19:09 done sampling a new configuration.\n",
      "12:19:09 HBMASTER: schedule new run for iteration 2\n",
      "12:19:09 HBMASTER: trying submitting job (2, 0, 0) to dispatcher\n",
      "12:19:09 HBMASTER: submitting job (2, 0, 0) to dispatcher\n",
      "12:19:09 DISPATCHER: trying to submit job (2, 0, 0)\n",
      "12:19:09 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:19:09 HBMASTER: job (2, 0, 0) submitted to dispatcher\n",
      "12:19:09 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:19:09 DISPATCHER: Trying to submit another job.\n",
      "12:19:09 DISPATCHER: starting job (2, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:19:09 DISPATCHER: job (2, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:19:09 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:19:09 WORKER: start processing job (2, 0, 0)\n",
      "12:19:09 WORKER: args: ()\n",
      "12:19:09 WORKER: kwargs: {'config': {'lr': 0.7204016826998266, 'num_conv_layers': 3, 'num_filters_1': 12, 'optimizer': 'SGD', 'num_filters_2': 18, 'num_filters_3': 21, 'sgd_momentum': 0.0353811165004797}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:19:29 DISPATCHER: Starting worker discovery\n",
      "12:19:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:19:29 DISPATCHER: Finished worker discovery\n",
      "12:20:29 DISPATCHER: Starting worker discovery\n",
      "12:20:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:20:29 DISPATCHER: Finished worker discovery\n",
      "12:20:49 WORKER: done with job (2, 0, 0), trying to register it.\n",
      "12:20:49 WORKER: registered result for job (2, 0, 0) with dispatcher\n",
      "12:20:49 DISPATCHER: job (2, 0, 0) finished\n",
      "12:20:49 DISPATCHER: register_result: lock acquired\n",
      "12:20:49 DISPATCHER: job (2, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:20:49 job_id: (2, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.7204016826998266, 'num_conv_layers': 3, 'num_filters_1': 12, 'optimizer': 'SGD', 'num_filters_2': 18, 'num_filters_3': 21, 'sgd_momentum': 0.0353811165004797}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.875, 'info': {'test_accuracy': 0.1028, 'train_accuracy': 0.108154296875, 'valid_accuracy': 0.125, 'model': 'Sequential(\\n  (0): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(12, 18, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(18, 21, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=7581, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:20:49 job_callback for (2, 0, 0) started\n",
      "12:20:49 DISPATCHER: Trying to submit another job.\n",
      "12:20:49 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:20:49 job_callback for (2, 0, 0) got condition\n",
      "12:20:49 Only 3 run(s) for budget 9.000000 available, need more than 9 -> can't build model!\n",
      "12:20:49 HBMASTER: Trying to run another job!\n",
      "12:20:49 job_callback for (2, 0, 0) finished\n",
      "12:20:49 start sampling a new configuration.\n",
      "12:20:49 done sampling a new configuration.\n",
      "12:20:49 HBMASTER: schedule new run for iteration 2\n",
      "12:20:49 HBMASTER: trying submitting job (2, 0, 1) to dispatcher\n",
      "12:20:49 HBMASTER: submitting job (2, 0, 1) to dispatcher\n",
      "12:20:49 DISPATCHER: trying to submit job (2, 0, 1)\n",
      "12:20:49 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:20:49 HBMASTER: job (2, 0, 1) submitted to dispatcher\n",
      "12:20:49 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:20:49 DISPATCHER: Trying to submit another job.\n",
      "12:20:49 DISPATCHER: starting job (2, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:20:49 DISPATCHER: job (2, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:20:49 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:20:49 WORKER: start processing job (2, 0, 1)\n",
      "12:20:49 WORKER: args: ()\n",
      "12:20:49 WORKER: kwargs: {'config': {'lr': 0.0014994941495752262, 'num_conv_layers': 2, 'num_filters_1': 5, 'optimizer': 'Adam', 'num_filters_2': 30}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:21:29 DISPATCHER: Starting worker discovery\n",
      "12:21:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:21:29 DISPATCHER: Finished worker discovery\n",
      "12:21:47 WORKER: done with job (2, 0, 1), trying to register it.\n",
      "12:21:47 WORKER: registered result for job (2, 0, 1) with dispatcher\n",
      "12:21:47 DISPATCHER: job (2, 0, 1) finished\n",
      "12:21:47 DISPATCHER: register_result: lock acquired\n",
      "12:21:47 DISPATCHER: job (2, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:21:47 job_id: (2, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.0014994941495752262, 'num_conv_layers': 2, 'num_filters_1': 5, 'optimizer': 'Adam', 'num_filters_2': 30}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.033203125, 'info': {'test_accuracy': 0.9621, 'train_accuracy': 0.999755859375, 'valid_accuracy': 0.966796875, 'model': 'Sequential(\\n  (0): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(5, 30, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=14520, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:21:47 job_callback for (2, 0, 1) started\n",
      "12:21:47 job_callback for (2, 0, 1) got condition\n",
      "12:21:47 DISPATCHER: Trying to submit another job.\n",
      "12:21:47 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:21:47 Only 4 run(s) for budget 9.000000 available, need more than 9 -> can't build model!\n",
      "12:21:47 HBMASTER: Trying to run another job!\n",
      "12:21:47 job_callback for (2, 0, 1) finished\n",
      "12:21:47 start sampling a new configuration.\n",
      "12:21:47 done sampling a new configuration.\n",
      "12:21:47 HBMASTER: schedule new run for iteration 2\n",
      "12:21:47 HBMASTER: trying submitting job (2, 0, 2) to dispatcher\n",
      "12:21:47 HBMASTER: submitting job (2, 0, 2) to dispatcher\n",
      "12:21:47 DISPATCHER: trying to submit job (2, 0, 2)\n",
      "12:21:47 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:21:47 HBMASTER: job (2, 0, 2) submitted to dispatcher\n",
      "12:21:47 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:21:47 DISPATCHER: Trying to submit another job.\n",
      "12:21:47 DISPATCHER: starting job (2, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:21:47 DISPATCHER: job (2, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:21:47 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:21:47 WORKER: start processing job (2, 0, 2)\n",
      "12:21:47 WORKER: args: ()\n",
      "12:21:47 WORKER: kwargs: {'config': {'lr': 0.09499784990665375, 'num_conv_layers': 1, 'num_filters_1': 21, 'optimizer': 'SGD', 'sgd_momentum': 0.5834115589518558}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:22:29 DISPATCHER: Starting worker discovery\n",
      "12:22:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:22:29 DISPATCHER: Finished worker discovery\n",
      "12:22:31 WORKER: done with job (2, 0, 2), trying to register it.\n",
      "12:22:31 WORKER: registered result for job (2, 0, 2) with dispatcher\n",
      "12:22:31 DISPATCHER: job (2, 0, 2) finished\n",
      "12:22:31 DISPATCHER: register_result: lock acquired\n",
      "12:22:31 DISPATCHER: job (2, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:22:31 job_id: (2, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.09499784990665375, 'num_conv_layers': 1, 'num_filters_1': 21, 'optimizer': 'SGD', 'sgd_momentum': 0.5834115589518558}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.033203125, 'info': {'test_accuracy': 0.9588, 'train_accuracy': 1.0, 'valid_accuracy': 0.966796875, 'model': 'Sequential(\\n  (0): Conv2d(1, 21, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=13125, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:22:31 job_callback for (2, 0, 2) started\n",
      "12:22:31 DISPATCHER: Trying to submit another job.\n",
      "12:22:31 job_callback for (2, 0, 2) got condition\n",
      "12:22:31 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:22:31 Only 5 run(s) for budget 9.000000 available, need more than 9 -> can't build model!\n",
      "12:22:31 HBMASTER: Trying to run another job!\n",
      "12:22:31 job_callback for (2, 0, 2) finished\n",
      "12:22:31 start sampling a new configuration.\n",
      "12:22:31 done sampling a new configuration.\n",
      "12:22:31 HBMASTER: schedule new run for iteration 3\n",
      "12:22:31 HBMASTER: trying submitting job (3, 0, 0) to dispatcher\n",
      "12:22:31 HBMASTER: submitting job (3, 0, 0) to dispatcher\n",
      "12:22:31 DISPATCHER: trying to submit job (3, 0, 0)\n",
      "12:22:31 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:22:31 HBMASTER: job (3, 0, 0) submitted to dispatcher\n",
      "12:22:31 DISPATCHER: Trying to submit another job.\n",
      "12:22:31 DISPATCHER: starting job (3, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:22:31 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:22:31 DISPATCHER: job (3, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:22:31 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:22:31 WORKER: start processing job (3, 0, 0)\n",
      "12:22:31 WORKER: args: ()\n",
      "12:22:31 WORKER: kwargs: {'config': {'lr': 0.021419342292898852, 'num_conv_layers': 3, 'num_filters_1': 32, 'optimizer': 'Adam', 'num_filters_2': 21, 'num_filters_3': 13}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:23:06 WORKER: done with job (3, 0, 0), trying to register it.\n",
      "12:23:06 WORKER: registered result for job (3, 0, 0) with dispatcher\n",
      "12:23:06 DISPATCHER: job (3, 0, 0) finished\n",
      "12:23:06 DISPATCHER: register_result: lock acquired\n",
      "12:23:06 DISPATCHER: job (3, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:23:06 job_id: (3, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.021419342292898852, 'num_conv_layers': 3, 'num_filters_1': 32, 'optimizer': 'Adam', 'num_filters_2': 21, 'num_filters_3': 13}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.875, 'info': {'test_accuracy': 0.1028, 'train_accuracy': 0.108154296875, 'valid_accuracy': 0.125, 'model': 'Sequential(\\n  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(32, 21, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(21, 13, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=4693, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:23:06 job_callback for (3, 0, 0) started\n",
      "12:23:06 DISPATCHER: Trying to submit another job.\n",
      "12:23:06 job_callback for (3, 0, 0) got condition\n",
      "12:23:06 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:23:06 HBMASTER: Trying to run another job!\n",
      "12:23:06 job_callback for (3, 0, 0) finished\n",
      "12:23:06 start sampling a new configuration.\n",
      "12:23:06 done sampling a new configuration.\n",
      "12:23:06 HBMASTER: schedule new run for iteration 3\n",
      "12:23:06 HBMASTER: trying submitting job (3, 0, 1) to dispatcher\n",
      "12:23:06 HBMASTER: submitting job (3, 0, 1) to dispatcher\n",
      "12:23:06 DISPATCHER: trying to submit job (3, 0, 1)\n",
      "12:23:06 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:23:06 HBMASTER: job (3, 0, 1) submitted to dispatcher\n",
      "12:23:06 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:23:06 DISPATCHER: Trying to submit another job.\n",
      "12:23:06 DISPATCHER: starting job (3, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:23:06 DISPATCHER: job (3, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:23:06 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:23:06 WORKER: start processing job (3, 0, 1)\n",
      "12:23:06 WORKER: args: ()\n",
      "12:23:06 WORKER: kwargs: {'config': {'lr': 0.020566108397618216, 'num_conv_layers': 1, 'num_filters_1': 24, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:23:18 WORKER: done with job (3, 0, 1), trying to register it.\n",
      "12:23:18 WORKER: registered result for job (3, 0, 1) with dispatcher\n",
      "12:23:18 DISPATCHER: job (3, 0, 1) finished\n",
      "12:23:18 DISPATCHER: register_result: lock acquired\n",
      "12:23:18 DISPATCHER: job (3, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:23:18 job_id: (3, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.020566108397618216, 'num_conv_layers': 1, 'num_filters_1': 24, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.064453125, 'info': {'test_accuracy': 0.9358, 'train_accuracy': 0.96484375, 'valid_accuracy': 0.935546875, 'model': 'Sequential(\\n  (0): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=15000, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:23:18 job_callback for (3, 0, 1) started\n",
      "12:23:18 DISPATCHER: Trying to submit another job.\n",
      "12:23:18 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:23:18 job_callback for (3, 0, 1) got condition\n",
      "12:23:18 HBMASTER: Trying to run another job!\n",
      "12:23:18 job_callback for (3, 0, 1) finished\n",
      "12:23:18 start sampling a new configuration.\n",
      "12:23:18 done sampling a new configuration.\n",
      "12:23:18 HBMASTER: schedule new run for iteration 3\n",
      "12:23:18 HBMASTER: trying submitting job (3, 0, 2) to dispatcher\n",
      "12:23:18 HBMASTER: submitting job (3, 0, 2) to dispatcher\n",
      "12:23:18 DISPATCHER: trying to submit job (3, 0, 2)\n",
      "12:23:18 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:23:18 HBMASTER: job (3, 0, 2) submitted to dispatcher\n",
      "12:23:18 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:23:18 DISPATCHER: Trying to submit another job.\n",
      "12:23:18 DISPATCHER: starting job (3, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:23:18 DISPATCHER: job (3, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:23:18 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:23:18 WORKER: start processing job (3, 0, 2)\n",
      "12:23:18 WORKER: args: ()\n",
      "12:23:18 WORKER: kwargs: {'config': {'lr': 0.12594621853426424, 'num_conv_layers': 1, 'num_filters_1': 32, 'optimizer': 'SGD', 'sgd_momentum': 0.7339014681394149}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:23:29 DISPATCHER: Starting worker discovery\n",
      "12:23:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:23:29 DISPATCHER: Finished worker discovery\n",
      "12:23:31 WORKER: done with job (3, 0, 2), trying to register it.\n",
      "12:23:31 WORKER: registered result for job (3, 0, 2) with dispatcher\n",
      "12:23:31 DISPATCHER: job (3, 0, 2) finished\n",
      "12:23:31 DISPATCHER: register_result: lock acquired\n",
      "12:23:31 DISPATCHER: job (3, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:23:31 job_id: (3, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.12594621853426424, 'num_conv_layers': 1, 'num_filters_1': 32, 'optimizer': 'SGD', 'sgd_momentum': 0.7339014681394149}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.0546875, 'info': {'test_accuracy': 0.933, 'train_accuracy': 0.955078125, 'valid_accuracy': 0.9453125, 'model': 'Sequential(\\n  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=20000, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:23:31 job_callback for (3, 0, 2) started\n",
      "12:23:31 DISPATCHER: Trying to submit another job.\n",
      "12:23:31 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:23:31 job_callback for (3, 0, 2) got condition\n",
      "12:23:31 HBMASTER: Trying to run another job!\n",
      "12:23:31 job_callback for (3, 0, 2) finished\n",
      "12:23:31 start sampling a new configuration.\n",
      "12:23:31 done sampling a new configuration.\n",
      "12:23:31 HBMASTER: schedule new run for iteration 3\n",
      "12:23:31 HBMASTER: trying submitting job (3, 0, 3) to dispatcher\n",
      "12:23:31 HBMASTER: submitting job (3, 0, 3) to dispatcher\n",
      "12:23:31 DISPATCHER: trying to submit job (3, 0, 3)\n",
      "12:23:31 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:23:31 HBMASTER: job (3, 0, 3) submitted to dispatcher\n",
      "12:23:31 DISPATCHER: Trying to submit another job.\n",
      "12:23:31 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:23:31 DISPATCHER: starting job (3, 0, 3) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:23:31 DISPATCHER: job (3, 0, 3) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:23:31 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:23:31 WORKER: start processing job (3, 0, 3)\n",
      "12:23:31 WORKER: args: ()\n",
      "12:23:31 WORKER: kwargs: {'config': {'lr': 0.06907930253663387, 'num_conv_layers': 3, 'num_filters_1': 14, 'optimizer': 'SGD', 'num_filters_2': 30, 'num_filters_3': 14, 'sgd_momentum': 0.14268048081780527}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:23:59 WORKER: done with job (3, 0, 3), trying to register it.\n",
      "12:23:59 WORKER: registered result for job (3, 0, 3) with dispatcher\n",
      "12:23:59 DISPATCHER: job (3, 0, 3) finished\n",
      "12:23:59 DISPATCHER: register_result: lock acquired\n",
      "12:23:59 DISPATCHER: job (3, 0, 3) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:23:59 job_id: (3, 0, 3)\n",
      "kwargs: {'config': {'lr': 0.06907930253663387, 'num_conv_layers': 3, 'num_filters_1': 14, 'optimizer': 'SGD', 'num_filters_2': 30, 'num_filters_3': 14, 'sgd_momentum': 0.14268048081780527}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.12109375, 'info': {'test_accuracy': 0.8583, 'train_accuracy': 0.882080078125, 'valid_accuracy': 0.87890625, 'model': 'Sequential(\\n  (0): Conv2d(1, 14, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(14, 30, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(30, 14, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=5054, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:23:59 job_callback for (3, 0, 3) started\n",
      "12:23:59 DISPATCHER: Trying to submit another job.\n",
      "12:23:59 job_callback for (3, 0, 3) got condition\n",
      "12:23:59 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:23:59 HBMASTER: Trying to run another job!\n",
      "12:23:59 job_callback for (3, 0, 3) finished\n",
      "12:23:59 start sampling a new configuration.\n",
      "12:23:59 done sampling a new configuration.\n",
      "12:23:59 HBMASTER: schedule new run for iteration 3\n",
      "12:23:59 HBMASTER: trying submitting job (3, 0, 4) to dispatcher\n",
      "12:23:59 HBMASTER: submitting job (3, 0, 4) to dispatcher\n",
      "12:23:59 DISPATCHER: trying to submit job (3, 0, 4)\n",
      "12:23:59 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:23:59 HBMASTER: job (3, 0, 4) submitted to dispatcher\n",
      "12:23:59 DISPATCHER: Trying to submit another job.\n",
      "12:23:59 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:23:59 DISPATCHER: starting job (3, 0, 4) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:23:59 DISPATCHER: job (3, 0, 4) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:23:59 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:23:59 WORKER: start processing job (3, 0, 4)\n",
      "12:23:59 WORKER: args: ()\n",
      "12:23:59 WORKER: kwargs: {'config': {'lr': 0.35196741546796123, 'num_conv_layers': 1, 'num_filters_1': 17, 'optimizer': 'SGD', 'sgd_momentum': 0.6701205547297927}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:24:08 WORKER: done with job (3, 0, 4), trying to register it.\n",
      "12:24:08 WORKER: registered result for job (3, 0, 4) with dispatcher\n",
      "12:24:08 DISPATCHER: job (3, 0, 4) finished\n",
      "12:24:08 DISPATCHER: register_result: lock acquired\n",
      "12:24:08 DISPATCHER: job (3, 0, 4) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:24:08 job_id: (3, 0, 4)\n",
      "kwargs: {'config': {'lr': 0.35196741546796123, 'num_conv_layers': 1, 'num_filters_1': 17, 'optimizer': 'SGD', 'sgd_momentum': 0.6701205547297927}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.1953125, 'info': {'test_accuracy': 0.81, 'train_accuracy': 0.85107421875, 'valid_accuracy': 0.8046875, 'model': 'Sequential(\\n  (0): Conv2d(1, 17, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=10625, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:24:08 job_callback for (3, 0, 4) started\n",
      "12:24:08 DISPATCHER: Trying to submit another job.\n",
      "12:24:08 job_callback for (3, 0, 4) got condition\n",
      "12:24:08 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:24:08 HBMASTER: Trying to run another job!\n",
      "12:24:08 job_callback for (3, 0, 4) finished\n",
      "12:24:08 start sampling a new configuration.\n",
      "12:24:08 done sampling a new configuration.\n",
      "12:24:08 HBMASTER: schedule new run for iteration 3\n",
      "12:24:08 HBMASTER: trying submitting job (3, 0, 5) to dispatcher\n",
      "12:24:08 HBMASTER: submitting job (3, 0, 5) to dispatcher\n",
      "12:24:08 DISPATCHER: trying to submit job (3, 0, 5)\n",
      "12:24:08 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:24:08 HBMASTER: job (3, 0, 5) submitted to dispatcher\n",
      "12:24:08 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:24:08 DISPATCHER: Trying to submit another job.\n",
      "12:24:08 DISPATCHER: starting job (3, 0, 5) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:24:08 DISPATCHER: job (3, 0, 5) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:24:08 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:24:08 WORKER: start processing job (3, 0, 5)\n",
      "12:24:08 WORKER: args: ()\n",
      "12:24:08 WORKER: kwargs: {'config': {'lr': 0.00027965365315665337, 'num_conv_layers': 1, 'num_filters_1': 10, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:24:15 WORKER: done with job (3, 0, 5), trying to register it.\n",
      "12:24:15 WORKER: registered result for job (3, 0, 5) with dispatcher\n",
      "12:24:15 DISPATCHER: job (3, 0, 5) finished\n",
      "12:24:15 DISPATCHER: register_result: lock acquired\n",
      "12:24:15 DISPATCHER: job (3, 0, 5) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:24:15 job_id: (3, 0, 5)\n",
      "kwargs: {'config': {'lr': 0.00027965365315665337, 'num_conv_layers': 1, 'num_filters_1': 10, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.103515625, 'info': {'test_accuracy': 0.8748, 'train_accuracy': 0.882568359375, 'valid_accuracy': 0.896484375, 'model': 'Sequential(\\n  (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=6250, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:24:15 job_callback for (3, 0, 5) started\n",
      "12:24:15 DISPATCHER: Trying to submit another job.\n",
      "12:24:15 job_callback for (3, 0, 5) got condition\n",
      "12:24:15 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:24:15 HBMASTER: Trying to run another job!\n",
      "12:24:15 job_callback for (3, 0, 5) finished\n",
      "12:24:15 start sampling a new configuration.\n",
      "12:24:15 done sampling a new configuration.\n",
      "12:24:15 HBMASTER: schedule new run for iteration 3\n",
      "12:24:15 HBMASTER: trying submitting job (3, 0, 6) to dispatcher\n",
      "12:24:15 HBMASTER: submitting job (3, 0, 6) to dispatcher\n",
      "12:24:15 DISPATCHER: trying to submit job (3, 0, 6)\n",
      "12:24:15 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:24:15 HBMASTER: job (3, 0, 6) submitted to dispatcher\n",
      "12:24:15 DISPATCHER: Trying to submit another job.\n",
      "12:24:15 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:24:15 DISPATCHER: starting job (3, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:24:15 DISPATCHER: job (3, 0, 6) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:24:15 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:24:15 WORKER: start processing job (3, 0, 6)\n",
      "12:24:15 WORKER: args: ()\n",
      "12:24:15 WORKER: kwargs: {'config': {'lr': 1.6490727951535354e-05, 'num_conv_layers': 2, 'num_filters_1': 30, 'optimizer': 'SGD', 'num_filters_2': 29, 'sgd_momentum': 0.3759475407375939}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:24:29 DISPATCHER: Starting worker discovery\n",
      "12:24:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:24:29 DISPATCHER: Finished worker discovery\n",
      "12:24:43 WORKER: done with job (3, 0, 6), trying to register it.\n",
      "12:24:43 WORKER: registered result for job (3, 0, 6) with dispatcher\n",
      "12:24:43 DISPATCHER: job (3, 0, 6) finished\n",
      "12:24:43 DISPATCHER: register_result: lock acquired\n",
      "12:24:43 DISPATCHER: job (3, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:24:43 job_id: (3, 0, 6)\n",
      "kwargs: {'config': {'lr': 1.6490727951535354e-05, 'num_conv_layers': 2, 'num_filters_1': 30, 'optimizer': 'SGD', 'num_filters_2': 29, 'sgd_momentum': 0.3759475407375939}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.88671875, 'info': {'test_accuracy': 0.0982, 'train_accuracy': 0.106201171875, 'valid_accuracy': 0.11328125, 'model': 'Sequential(\\n  (0): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(30, 29, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=14036, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:24:43 job_callback for (3, 0, 6) started\n",
      "12:24:43 DISPATCHER: Trying to submit another job.\n",
      "12:24:43 job_callback for (3, 0, 6) got condition\n",
      "12:24:43 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:24:43 done building a new model for budget 1.000000 based on 8/13 split\n",
      "Best loss for this budget:0.054688\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:24:43 HBMASTER: Trying to run another job!\n",
      "12:24:43 job_callback for (3, 0, 6) finished\n",
      "12:24:43 start sampling a new configuration.\n",
      "12:24:43 best_vector: [0.9867643026781975, 0.10381164483588512, 0.5637547369368202, 1, 0.2869678549919864, 0.7646417241012926, 0.5234124543581313], 6.243645004739876e-33, 1.6016285346794188, 5.512094563146167e-199\n",
      "12:24:43 done sampling a new configuration.\n",
      "12:24:43 HBMASTER: schedule new run for iteration 3\n",
      "12:24:43 HBMASTER: trying submitting job (3, 0, 7) to dispatcher\n",
      "12:24:43 HBMASTER: submitting job (3, 0, 7) to dispatcher\n",
      "12:24:43 DISPATCHER: trying to submit job (3, 0, 7)\n",
      "12:24:43 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:24:43 HBMASTER: job (3, 0, 7) submitted to dispatcher\n",
      "12:24:43 DISPATCHER: Trying to submit another job.\n",
      "12:24:43 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:24:43 DISPATCHER: starting job (3, 0, 7) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:24:43 DISPATCHER: job (3, 0, 7) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:24:43 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:24:43 WORKER: start processing job (3, 0, 7)\n",
      "12:24:43 WORKER: args: ()\n",
      "12:24:43 WORKER: kwargs: {'config': {'lr': 0.8328864870979222, 'num_conv_layers': 1, 'num_filters_1': 20, 'optimizer': 'SGD', 'sgd_momentum': 0.51817832981455}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:24:53 WORKER: done with job (3, 0, 7), trying to register it.\n",
      "12:24:53 WORKER: registered result for job (3, 0, 7) with dispatcher\n",
      "12:24:53 DISPATCHER: job (3, 0, 7) finished\n",
      "12:24:53 DISPATCHER: register_result: lock acquired\n",
      "12:24:53 DISPATCHER: job (3, 0, 7) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:24:53 job_id: (3, 0, 7)\n",
      "kwargs: {'config': {'lr': 0.8328864870979222, 'num_conv_layers': 1, 'num_filters_1': 20, 'optimizer': 'SGD', 'sgd_momentum': 0.51817832981455}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.88671875, 'info': {'test_accuracy': 0.0982, 'train_accuracy': 0.106201171875, 'valid_accuracy': 0.11328125, 'model': 'Sequential(\\n  (0): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=12500, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:24:53 job_callback for (3, 0, 7) started\n",
      "12:24:53 DISPATCHER: Trying to submit another job.\n",
      "12:24:53 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:24:53 job_callback for (3, 0, 7) got condition\n",
      "12:24:53 done building a new model for budget 1.000000 based on 8/14 split\n",
      "Best loss for this budget:0.054688\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:24:53 HBMASTER: Trying to run another job!\n",
      "12:24:53 job_callback for (3, 0, 7) finished\n",
      "12:24:53 start sampling a new configuration.\n",
      "12:24:53 done sampling a new configuration.\n",
      "12:24:53 HBMASTER: schedule new run for iteration 3\n",
      "12:24:53 HBMASTER: trying submitting job (3, 0, 8) to dispatcher\n",
      "12:24:53 HBMASTER: submitting job (3, 0, 8) to dispatcher\n",
      "12:24:53 DISPATCHER: trying to submit job (3, 0, 8)\n",
      "12:24:53 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:24:53 HBMASTER: job (3, 0, 8) submitted to dispatcher\n",
      "12:24:53 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:24:53 DISPATCHER: Trying to submit another job.\n",
      "12:24:53 DISPATCHER: starting job (3, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:24:53 DISPATCHER: job (3, 0, 8) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:24:53 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:24:53 WORKER: start processing job (3, 0, 8)\n",
      "12:24:53 WORKER: args: ()\n",
      "12:24:53 WORKER: kwargs: {'config': {'lr': 8.071454962350994e-05, 'num_conv_layers': 2, 'num_filters_1': 26, 'optimizer': 'Adam', 'num_filters_2': 10}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:25:14 WORKER: done with job (3, 0, 8), trying to register it.\n",
      "12:25:14 WORKER: registered result for job (3, 0, 8) with dispatcher\n",
      "12:25:14 DISPATCHER: job (3, 0, 8) finished\n",
      "12:25:14 DISPATCHER: register_result: lock acquired\n",
      "12:25:14 DISPATCHER: job (3, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:25:14 job_id: (3, 0, 8)\n",
      "kwargs: {'config': {'lr': 8.071454962350994e-05, 'num_conv_layers': 2, 'num_filters_1': 26, 'optimizer': 'Adam', 'num_filters_2': 10}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.201171875, 'info': {'test_accuracy': 0.7859, 'train_accuracy': 0.7880859375, 'valid_accuracy': 0.798828125, 'model': 'Sequential(\\n  (0): Conv2d(1, 26, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(26, 10, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=4840, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:25:14 job_callback for (3, 0, 8) started\n",
      "12:25:14 DISPATCHER: Trying to submit another job.\n",
      "12:25:14 job_callback for (3, 0, 8) got condition\n",
      "12:25:14 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:25:14 done building a new model for budget 1.000000 based on 8/15 split\n",
      "Best loss for this budget:0.054688\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:25:14 HBMASTER: Trying to run another job!\n",
      "12:25:14 job_callback for (3, 0, 8) finished\n",
      "12:25:14 ITERATION: Advancing config (3, 0, 1) to next budget 3.000000\n",
      "12:25:14 ITERATION: Advancing config (3, 0, 2) to next budget 3.000000\n",
      "12:25:14 ITERATION: Advancing config (3, 0, 5) to next budget 3.000000\n",
      "12:25:14 HBMASTER: schedule new run for iteration 3\n",
      "12:25:14 HBMASTER: trying submitting job (3, 0, 1) to dispatcher\n",
      "12:25:14 HBMASTER: submitting job (3, 0, 1) to dispatcher\n",
      "12:25:14 DISPATCHER: trying to submit job (3, 0, 1)\n",
      "12:25:14 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:25:14 HBMASTER: job (3, 0, 1) submitted to dispatcher\n",
      "12:25:14 DISPATCHER: Trying to submit another job.\n",
      "12:25:14 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:25:14 DISPATCHER: starting job (3, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:25:14 DISPATCHER: job (3, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:25:14 WORKER: start processing job (3, 0, 1)\n",
      "12:25:14 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:25:14 WORKER: args: ()\n",
      "12:25:14 WORKER: kwargs: {'config': {'lr': 0.020566108397618216, 'num_conv_layers': 1, 'num_filters_1': 24, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:25:29 DISPATCHER: Starting worker discovery\n",
      "12:25:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:25:29 DISPATCHER: Finished worker discovery\n",
      "12:25:34 WORKER: done with job (3, 0, 1), trying to register it.\n",
      "12:25:34 WORKER: registered result for job (3, 0, 1) with dispatcher\n",
      "12:25:34 DISPATCHER: job (3, 0, 1) finished\n",
      "12:25:34 DISPATCHER: register_result: lock acquired\n",
      "12:25:34 DISPATCHER: job (3, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:25:34 job_id: (3, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.020566108397618216, 'num_conv_layers': 1, 'num_filters_1': 24, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.048828125, 'info': {'test_accuracy': 0.9395, 'train_accuracy': 0.98583984375, 'valid_accuracy': 0.951171875, 'model': 'Sequential(\\n  (0): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=15000, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:25:34 job_callback for (3, 0, 1) started\n",
      "12:25:34 DISPATCHER: Trying to submit another job.\n",
      "12:25:34 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:25:34 job_callback for (3, 0, 1) got condition\n",
      "12:25:34 Only 7 run(s) for budget 3.000000 available, need more than 9 -> can't build model!\n",
      "12:25:34 HBMASTER: Trying to run another job!\n",
      "12:25:34 job_callback for (3, 0, 1) finished\n",
      "12:25:34 HBMASTER: schedule new run for iteration 3\n",
      "12:25:34 HBMASTER: trying submitting job (3, 0, 2) to dispatcher\n",
      "12:25:34 HBMASTER: submitting job (3, 0, 2) to dispatcher\n",
      "12:25:34 DISPATCHER: trying to submit job (3, 0, 2)\n",
      "12:25:34 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:25:34 HBMASTER: job (3, 0, 2) submitted to dispatcher\n",
      "12:25:34 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:25:34 DISPATCHER: Trying to submit another job.\n",
      "12:25:34 DISPATCHER: starting job (3, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:25:34 DISPATCHER: job (3, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:25:34 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:25:34 WORKER: start processing job (3, 0, 2)\n",
      "12:25:34 WORKER: args: ()\n",
      "12:25:34 WORKER: kwargs: {'config': {'lr': 0.12594621853426424, 'num_conv_layers': 1, 'num_filters_1': 32, 'optimizer': 'SGD', 'sgd_momentum': 0.7339014681394149}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:25:59 WORKER: done with job (3, 0, 2), trying to register it.\n",
      "12:25:59 WORKER: registered result for job (3, 0, 2) with dispatcher\n",
      "12:25:59 DISPATCHER: job (3, 0, 2) finished\n",
      "12:25:59 DISPATCHER: register_result: lock acquired\n",
      "12:25:59 DISPATCHER: job (3, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:25:59 job_id: (3, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.12594621853426424, 'num_conv_layers': 1, 'num_filters_1': 32, 'optimizer': 'SGD', 'sgd_momentum': 0.7339014681394149}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.046875, 'info': {'test_accuracy': 0.9497, 'train_accuracy': 0.98681640625, 'valid_accuracy': 0.953125, 'model': 'Sequential(\\n  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=20000, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:25:59 job_callback for (3, 0, 2) started\n",
      "12:25:59 DISPATCHER: Trying to submit another job.\n",
      "12:25:59 job_callback for (3, 0, 2) got condition\n",
      "12:25:59 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:25:59 HBMASTER: Trying to run another job!\n",
      "12:25:59 job_callback for (3, 0, 2) finished\n",
      "12:25:59 HBMASTER: schedule new run for iteration 3\n",
      "12:25:59 HBMASTER: trying submitting job (3, 0, 5) to dispatcher\n",
      "12:25:59 HBMASTER: submitting job (3, 0, 5) to dispatcher\n",
      "12:25:59 DISPATCHER: trying to submit job (3, 0, 5)\n",
      "12:25:59 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:25:59 HBMASTER: job (3, 0, 5) submitted to dispatcher\n",
      "12:25:59 DISPATCHER: Trying to submit another job.\n",
      "12:25:59 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:25:59 DISPATCHER: starting job (3, 0, 5) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:25:59 DISPATCHER: job (3, 0, 5) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:25:59 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:25:59 WORKER: start processing job (3, 0, 5)\n",
      "12:25:59 WORKER: args: ()\n",
      "12:25:59 WORKER: kwargs: {'config': {'lr': 0.00027965365315665337, 'num_conv_layers': 1, 'num_filters_1': 10, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:26:11 WORKER: done with job (3, 0, 5), trying to register it.\n",
      "12:26:11 WORKER: registered result for job (3, 0, 5) with dispatcher\n",
      "12:26:11 DISPATCHER: job (3, 0, 5) finished\n",
      "12:26:11 DISPATCHER: register_result: lock acquired\n",
      "12:26:11 DISPATCHER: job (3, 0, 5) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:26:11 job_id: (3, 0, 5)\n",
      "kwargs: {'config': {'lr': 0.00027965365315665337, 'num_conv_layers': 1, 'num_filters_1': 10, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.099609375, 'info': {'test_accuracy': 0.8906, 'train_accuracy': 0.9052734375, 'valid_accuracy': 0.900390625, 'model': 'Sequential(\\n  (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=6250, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:26:11 job_callback for (3, 0, 5) started\n",
      "12:26:11 DISPATCHER: Trying to submit another job.\n",
      "12:26:11 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:26:11 job_callback for (3, 0, 5) got condition\n",
      "12:26:11 HBMASTER: Trying to run another job!\n",
      "12:26:11 job_callback for (3, 0, 5) finished\n",
      "12:26:11 ITERATION: Advancing config (3, 0, 2) to next budget 9.000000\n",
      "12:26:11 HBMASTER: schedule new run for iteration 3\n",
      "12:26:11 HBMASTER: trying submitting job (3, 0, 2) to dispatcher\n",
      "12:26:11 HBMASTER: submitting job (3, 0, 2) to dispatcher\n",
      "12:26:11 DISPATCHER: trying to submit job (3, 0, 2)\n",
      "12:26:11 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:26:11 HBMASTER: job (3, 0, 2) submitted to dispatcher\n",
      "12:26:11 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:26:11 DISPATCHER: Trying to submit another job.\n",
      "12:26:11 DISPATCHER: starting job (3, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:26:11 DISPATCHER: job (3, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:26:11 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:26:11 WORKER: start processing job (3, 0, 2)\n",
      "12:26:11 WORKER: args: ()\n",
      "12:26:11 WORKER: kwargs: {'config': {'lr': 0.12594621853426424, 'num_conv_layers': 1, 'num_filters_1': 32, 'optimizer': 'SGD', 'sgd_momentum': 0.7339014681394149}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:26:29 DISPATCHER: Starting worker discovery\n",
      "12:26:29 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:26:29 DISPATCHER: Finished worker discovery\n",
      "12:27:07 WORKER: done with job (3, 0, 2), trying to register it.\n",
      "12:27:07 WORKER: registered result for job (3, 0, 2) with dispatcher\n",
      "12:27:07 DISPATCHER: job (3, 0, 2) finished\n",
      "12:27:07 DISPATCHER: register_result: lock acquired\n",
      "12:27:07 DISPATCHER: job (3, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:27:07 job_id: (3, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.12594621853426424, 'num_conv_layers': 1, 'num_filters_1': 32, 'optimizer': 'SGD', 'sgd_momentum': 0.7339014681394149}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.037109375, 'info': {'test_accuracy': 0.96, 'train_accuracy': 1.0, 'valid_accuracy': 0.962890625, 'model': 'Sequential(\\n  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=20000, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:27:07 job_callback for (3, 0, 2) started\n",
      "12:27:07 DISPATCHER: Trying to submit another job.\n",
      "12:27:07 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:27:07 job_callback for (3, 0, 2) got condition\n",
      "12:27:07 Only 6 run(s) for budget 9.000000 available, need more than 9 -> can't build model!\n",
      "12:27:07 HBMASTER: Trying to run another job!\n",
      "12:27:07 job_callback for (3, 0, 2) finished\n",
      "12:27:07 start sampling a new configuration.\n",
      "12:27:07 done sampling a new configuration.\n",
      "12:27:07 HBMASTER: schedule new run for iteration 4\n",
      "12:27:07 HBMASTER: trying submitting job (4, 0, 0) to dispatcher\n",
      "12:27:07 HBMASTER: submitting job (4, 0, 0) to dispatcher\n",
      "12:27:07 DISPATCHER: trying to submit job (4, 0, 0)\n",
      "12:27:07 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:27:07 HBMASTER: job (4, 0, 0) submitted to dispatcher\n",
      "12:27:07 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:27:07 DISPATCHER: Trying to submit another job.\n",
      "12:27:07 DISPATCHER: starting job (4, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:27:07 DISPATCHER: job (4, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:27:08 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:27:08 WORKER: start processing job (4, 0, 0)\n",
      "12:27:08 WORKER: args: ()\n",
      "12:27:08 WORKER: kwargs: {'config': {'lr': 0.2916402828919966, 'num_conv_layers': 2, 'num_filters_1': 32, 'optimizer': 'Adam', 'num_filters_2': 4}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:27:29 DISPATCHER: Starting worker discovery\n",
      "12:27:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:27:30 DISPATCHER: Finished worker discovery\n",
      "12:27:45 WORKER: done with job (4, 0, 0), trying to register it.\n",
      "12:27:45 WORKER: registered result for job (4, 0, 0) with dispatcher\n",
      "12:27:45 DISPATCHER: job (4, 0, 0) finished\n",
      "12:27:45 DISPATCHER: register_result: lock acquired\n",
      "12:27:45 DISPATCHER: job (4, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:27:45 job_id: (4, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.2916402828919966, 'num_conv_layers': 2, 'num_filters_1': 32, 'optimizer': 'Adam', 'num_filters_2': 4}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.916015625, 'info': {'test_accuracy': 0.0974, 'train_accuracy': 0.093505859375, 'valid_accuracy': 0.083984375, 'model': 'Sequential(\\n  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=1936, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:27:45 job_callback for (4, 0, 0) started\n",
      "12:27:45 DISPATCHER: Trying to submit another job.\n",
      "12:27:45 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:27:45 job_callback for (4, 0, 0) got condition\n",
      "12:27:45 HBMASTER: Trying to run another job!\n",
      "12:27:45 job_callback for (4, 0, 0) finished\n",
      "12:27:45 start sampling a new configuration.\n",
      "12:27:45 best_vector: [0.8981860727773123, 0.15173959792736536, 0.55775686241713, 1, 0.5470910044990062, 0.7631662512104787, 0.4663784154033604], 1.1425560555427445e-32, 0.8752305807219003, 3.1077424022759427e-190\n",
      "12:27:45 done sampling a new configuration.\n",
      "12:27:45 HBMASTER: schedule new run for iteration 4\n",
      "12:27:45 HBMASTER: trying submitting job (4, 0, 1) to dispatcher\n",
      "12:27:45 HBMASTER: submitting job (4, 0, 1) to dispatcher\n",
      "12:27:45 DISPATCHER: trying to submit job (4, 0, 1)\n",
      "12:27:45 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:27:45 HBMASTER: job (4, 0, 1) submitted to dispatcher\n",
      "12:27:45 DISPATCHER: Trying to submit another job.\n",
      "12:27:45 DISPATCHER: starting job (4, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:27:45 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:27:45 DISPATCHER: job (4, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:27:45 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:27:45 WORKER: start processing job (4, 0, 1)\n",
      "12:27:45 WORKER: args: ()\n",
      "12:27:45 WORKER: kwargs: {'config': {'lr': 0.24497199367726233, 'num_conv_layers': 1, 'num_filters_1': 20, 'optimizer': 'SGD', 'sgd_momentum': 0.46171463124932677}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:28:03 WORKER: done with job (4, 0, 1), trying to register it.\n",
      "12:28:03 WORKER: registered result for job (4, 0, 1) with dispatcher\n",
      "12:28:03 DISPATCHER: job (4, 0, 1) finished\n",
      "12:28:03 DISPATCHER: register_result: lock acquired\n",
      "12:28:03 DISPATCHER: job (4, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:28:03 job_id: (4, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.24497199367726233, 'num_conv_layers': 1, 'num_filters_1': 20, 'optimizer': 'SGD', 'sgd_momentum': 0.46171463124932677}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.0625, 'info': {'test_accuracy': 0.9211, 'train_accuracy': 0.957275390625, 'valid_accuracy': 0.9375, 'model': 'Sequential(\\n  (0): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=12500, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:28:03 job_callback for (4, 0, 1) started\n",
      "12:28:03 DISPATCHER: Trying to submit another job.\n",
      "12:28:03 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:28:03 job_callback for (4, 0, 1) got condition\n",
      "12:28:03 HBMASTER: Trying to run another job!\n",
      "12:28:03 job_callback for (4, 0, 1) finished\n",
      "12:28:03 start sampling a new configuration.\n",
      "12:28:03 best_vector: [0.3990023793692521, 0.8089901440214833, 0.6744465934442854, 0, 0.4770846924973684, 0.7488605250916379, 0.7748033071173644], 1.7065829309035115e-32, 0.5859662498033851, 1.0351893219543802e-175\n",
      "12:28:03 done sampling a new configuration.\n",
      "12:28:03 HBMASTER: schedule new run for iteration 4\n",
      "12:28:03 HBMASTER: trying submitting job (4, 0, 2) to dispatcher\n",
      "12:28:03 HBMASTER: submitting job (4, 0, 2) to dispatcher\n",
      "12:28:03 DISPATCHER: trying to submit job (4, 0, 2)\n",
      "12:28:03 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:28:03 HBMASTER: job (4, 0, 2) submitted to dispatcher\n",
      "12:28:03 DISPATCHER: Trying to submit another job.\n",
      "12:28:03 DISPATCHER: starting job (4, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:28:03 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:28:03 DISPATCHER: job (4, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:28:03 WORKER: start processing job (4, 0, 2)\n",
      "12:28:03 WORKER: args: ()\n",
      "12:28:03 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:28:03 WORKER: kwargs: {'config': {'lr': 0.00024775034972877267, 'num_conv_layers': 3, 'num_filters_1': 23, 'optimizer': 'Adam', 'num_filters_2': 17, 'num_filters_3': 25}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:28:30 DISPATCHER: Starting worker discovery\n",
      "12:28:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:28:30 DISPATCHER: Finished worker discovery\n",
      "12:29:00 WORKER: done with job (4, 0, 2), trying to register it.\n",
      "12:29:00 WORKER: registered result for job (4, 0, 2) with dispatcher\n",
      "12:29:00 DISPATCHER: job (4, 0, 2) finished\n",
      "12:29:00 DISPATCHER: register_result: lock acquired\n",
      "12:29:00 DISPATCHER: job (4, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:29:00 job_id: (4, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.00024775034972877267, 'num_conv_layers': 3, 'num_filters_1': 23, 'optimizer': 'Adam', 'num_filters_2': 17, 'num_filters_3': 25}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.056640625, 'info': {'test_accuracy': 0.9361, 'train_accuracy': 0.95068359375, 'valid_accuracy': 0.943359375, 'model': 'Sequential(\\n  (0): Conv2d(1, 23, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(23, 17, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(17, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=9025, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:29:00 job_callback for (4, 0, 2) started\n",
      "12:29:00 DISPATCHER: Trying to submit another job.\n",
      "12:29:00 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:29:00 job_callback for (4, 0, 2) got condition\n",
      "12:29:00 HBMASTER: Trying to run another job!\n",
      "12:29:00 job_callback for (4, 0, 2) finished\n",
      "12:29:00 ITERATION: Advancing config (4, 0, 2) to next budget 9.000000\n",
      "12:29:00 HBMASTER: schedule new run for iteration 4\n",
      "12:29:00 HBMASTER: trying submitting job (4, 0, 2) to dispatcher\n",
      "12:29:00 HBMASTER: submitting job (4, 0, 2) to dispatcher\n",
      "12:29:00 DISPATCHER: trying to submit job (4, 0, 2)\n",
      "12:29:00 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:29:00 HBMASTER: job (4, 0, 2) submitted to dispatcher\n",
      "12:29:00 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:29:00 DISPATCHER: Trying to submit another job.\n",
      "12:29:00 DISPATCHER: starting job (4, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:29:00 DISPATCHER: job (4, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:29:00 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:29:00 WORKER: start processing job (4, 0, 2)\n",
      "12:29:00 WORKER: args: ()\n",
      "12:29:00 WORKER: kwargs: {'config': {'lr': 0.00024775034972877267, 'num_conv_layers': 3, 'num_filters_1': 23, 'optimizer': 'Adam', 'num_filters_2': 17, 'num_filters_3': 25}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:29:30 DISPATCHER: Starting worker discovery\n",
      "12:29:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:29:30 DISPATCHER: Finished worker discovery\n",
      "12:30:30 DISPATCHER: Starting worker discovery\n",
      "12:30:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:30:30 DISPATCHER: Finished worker discovery\n",
      "12:31:17 WORKER: done with job (4, 0, 2), trying to register it.\n",
      "12:31:17 WORKER: registered result for job (4, 0, 2) with dispatcher\n",
      "12:31:17 DISPATCHER: job (4, 0, 2) finished\n",
      "12:31:17 DISPATCHER: register_result: lock acquired\n",
      "12:31:17 DISPATCHER: job (4, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:31:17 job_id: (4, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.00024775034972877267, 'num_conv_layers': 3, 'num_filters_1': 23, 'optimizer': 'Adam', 'num_filters_2': 17, 'num_filters_3': 25}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.03125, 'info': {'test_accuracy': 0.9697, 'train_accuracy': 0.986572265625, 'valid_accuracy': 0.96875, 'model': 'Sequential(\\n  (0): Conv2d(1, 23, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(23, 17, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(17, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=9025, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:31:17 job_callback for (4, 0, 2) started\n",
      "12:31:17 DISPATCHER: Trying to submit another job.\n",
      "12:31:17 job_callback for (4, 0, 2) got condition\n",
      "12:31:17 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:31:17 Only 7 run(s) for budget 9.000000 available, need more than 9 -> can't build model!\n",
      "12:31:17 HBMASTER: Trying to run another job!\n",
      "12:31:17 job_callback for (4, 0, 2) finished\n",
      "12:31:17 start sampling a new configuration.\n",
      "12:31:17 best_vector: [0.3633489606887446, 0.9101981308381941, 0.5184688759168803, 0, 0.6510493168607021, 0.5495668757570799, 0.5320709386335467], 7.756744017860083e-33, 1.289200723521979, 5.148356838703619e-50\n",
      "12:31:17 done sampling a new configuration.\n",
      "12:31:17 HBMASTER: schedule new run for iteration 5\n",
      "12:31:17 HBMASTER: trying submitting job (5, 0, 0) to dispatcher\n",
      "12:31:17 HBMASTER: submitting job (5, 0, 0) to dispatcher\n",
      "12:31:17 DISPATCHER: trying to submit job (5, 0, 0)\n",
      "12:31:17 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:31:17 HBMASTER: job (5, 0, 0) submitted to dispatcher\n",
      "12:31:17 DISPATCHER: Trying to submit another job.\n",
      "12:31:17 DISPATCHER: starting job (5, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:31:17 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:31:17 DISPATCHER: job (5, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:31:17 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:31:17 WORKER: start processing job (5, 0, 0)\n",
      "12:31:17 WORKER: args: ()\n",
      "12:31:17 WORKER: kwargs: {'config': {'lr': 0.00015138880614269957, 'num_conv_layers': 3, 'num_filters_1': 19, 'optimizer': 'Adam', 'num_filters_2': 22, 'num_filters_3': 19}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:31:30 DISPATCHER: Starting worker discovery\n",
      "12:31:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:31:30 DISPATCHER: Finished worker discovery\n",
      "12:32:30 DISPATCHER: Starting worker discovery\n",
      "12:32:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:32:30 DISPATCHER: Finished worker discovery\n",
      "12:33:30 DISPATCHER: Starting worker discovery\n",
      "12:33:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:33:30 DISPATCHER: Finished worker discovery\n",
      "12:33:34 WORKER: done with job (5, 0, 0), trying to register it.\n",
      "12:33:34 WORKER: registered result for job (5, 0, 0) with dispatcher\n",
      "12:33:34 DISPATCHER: job (5, 0, 0) finished\n",
      "12:33:34 DISPATCHER: register_result: lock acquired\n",
      "12:33:34 DISPATCHER: job (5, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:33:34 job_id: (5, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.00015138880614269957, 'num_conv_layers': 3, 'num_filters_1': 19, 'optimizer': 'Adam', 'num_filters_2': 22, 'num_filters_3': 19}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.03515625, 'info': {'test_accuracy': 0.9683, 'train_accuracy': 0.98291015625, 'valid_accuracy': 0.96484375, 'model': 'Sequential(\\n  (0): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(19, 22, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(22, 19, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=6859, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:33:34 job_callback for (5, 0, 0) started\n",
      "12:33:34 DISPATCHER: Trying to submit another job.\n",
      "12:33:34 job_callback for (5, 0, 0) got condition\n",
      "12:33:34 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:33:34 HBMASTER: Trying to run another job!\n",
      "12:33:34 job_callback for (5, 0, 0) finished\n",
      "12:33:34 start sampling a new configuration.\n",
      "12:33:34 best_vector: [0.2468203722528497, 0.19627209445236665, 0.9487565058823814, 0, 0.1234511093749736, 0.9664728004748997, 0.22425173764690462], 1.2517680344950606e-32, 0.7988700561469291, 0.0\n",
      "12:33:34 done sampling a new configuration.\n",
      "12:33:34 HBMASTER: schedule new run for iteration 5\n",
      "12:33:34 HBMASTER: trying submitting job (5, 0, 1) to dispatcher\n",
      "12:33:34 HBMASTER: submitting job (5, 0, 1) to dispatcher\n",
      "12:33:34 DISPATCHER: trying to submit job (5, 0, 1)\n",
      "12:33:34 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:33:34 HBMASTER: job (5, 0, 1) submitted to dispatcher\n",
      "12:33:34 DISPATCHER: Trying to submit another job.\n",
      "12:33:34 DISPATCHER: starting job (5, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:33:34 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:33:34 DISPATCHER: job (5, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:33:34 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:33:34 WORKER: start processing job (5, 0, 1)\n",
      "12:33:34 WORKER: args: ()\n",
      "12:33:34 WORKER: kwargs: {'config': {'lr': 3.0263714655683442e-05, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'Adam'}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:34:30 DISPATCHER: Starting worker discovery\n",
      "12:34:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:34:30 DISPATCHER: Finished worker discovery\n",
      "12:34:41 WORKER: done with job (5, 0, 1), trying to register it.\n",
      "12:34:41 WORKER: registered result for job (5, 0, 1) with dispatcher\n",
      "12:34:41 DISPATCHER: job (5, 0, 1) finished\n",
      "12:34:41 DISPATCHER: register_result: lock acquired\n",
      "12:34:41 DISPATCHER: job (5, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:34:41 job_id: (5, 0, 1)\n",
      "kwargs: {'config': {'lr': 3.0263714655683442e-05, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'Adam'}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.08984375, 'info': {'test_accuracy': 0.9031, 'train_accuracy': 0.913330078125, 'valid_accuracy': 0.91015625, 'model': 'Sequential(\\n  (0): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=19375, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:34:41 job_callback for (5, 0, 1) started\n",
      "12:34:41 DISPATCHER: Trying to submit another job.\n",
      "12:34:41 job_callback for (5, 0, 1) got condition\n",
      "12:34:41 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:34:41 HBMASTER: Trying to run another job!\n",
      "12:34:41 job_callback for (5, 0, 1) finished\n",
      "12:34:41 start sampling a new configuration.\n",
      "12:34:41 done sampling a new configuration.\n",
      "12:34:41 HBMASTER: schedule new run for iteration 5\n",
      "12:34:41 HBMASTER: trying submitting job (5, 0, 2) to dispatcher\n",
      "12:34:41 HBMASTER: submitting job (5, 0, 2) to dispatcher\n",
      "12:34:41 DISPATCHER: trying to submit job (5, 0, 2)\n",
      "12:34:41 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:34:41 HBMASTER: job (5, 0, 2) submitted to dispatcher\n",
      "12:34:41 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:34:41 DISPATCHER: Trying to submit another job.\n",
      "12:34:41 DISPATCHER: starting job (5, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:34:41 DISPATCHER: job (5, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:34:41 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:34:41 WORKER: start processing job (5, 0, 2)\n",
      "12:34:41 WORKER: args: ()\n",
      "12:34:41 WORKER: kwargs: {'config': {'lr': 0.07560729625766781, 'num_conv_layers': 2, 'num_filters_1': 21, 'optimizer': 'Adam', 'num_filters_2': 23}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:35:30 DISPATCHER: Starting worker discovery\n",
      "12:35:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:35:30 DISPATCHER: Finished worker discovery\n",
      "12:36:19 WORKER: done with job (5, 0, 2), trying to register it.\n",
      "12:36:19 WORKER: registered result for job (5, 0, 2) with dispatcher\n",
      "12:36:19 DISPATCHER: job (5, 0, 2) finished\n",
      "12:36:19 DISPATCHER: register_result: lock acquired\n",
      "12:36:19 DISPATCHER: job (5, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:36:19 job_id: (5, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.07560729625766781, 'num_conv_layers': 2, 'num_filters_1': 21, 'optimizer': 'Adam', 'num_filters_2': 23}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.88671875, 'info': {'test_accuracy': 0.1135, 'train_accuracy': 0.111328125, 'valid_accuracy': 0.11328125, 'model': 'Sequential(\\n  (0): Conv2d(1, 21, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(21, 23, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=11132, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:36:19 job_callback for (5, 0, 2) started\n",
      "12:36:19 DISPATCHER: Trying to submit another job.\n",
      "12:36:19 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:36:19 job_callback for (5, 0, 2) got condition\n",
      "12:36:19 HBMASTER: Trying to run another job!\n",
      "12:36:19 job_callback for (5, 0, 2) finished\n",
      "12:36:19 start sampling a new configuration.\n",
      "12:36:19 best_vector: [0.8143486370134485, 0.24526495956785943, 0.9339930368390259, 1, 0.9714082080080864, 0.5262035059725076, 0.6868168913420761], 6.942705431022741e-33, 1.4403606921469063, 4.3698645328635674e-40\n",
      "12:36:19 done sampling a new configuration.\n",
      "12:36:19 HBMASTER: schedule new run for iteration 6\n",
      "12:36:19 HBMASTER: trying submitting job (6, 0, 0) to dispatcher\n",
      "12:36:19 HBMASTER: submitting job (6, 0, 0) to dispatcher\n",
      "12:36:19 DISPATCHER: trying to submit job (6, 0, 0)\n",
      "12:36:19 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:36:19 HBMASTER: job (6, 0, 0) submitted to dispatcher\n",
      "12:36:19 DISPATCHER: Trying to submit another job.\n",
      "12:36:19 DISPATCHER: starting job (6, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:36:19 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:36:19 DISPATCHER: job (6, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:36:19 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:36:19 WORKER: start processing job (6, 0, 0)\n",
      "12:36:19 WORKER: args: ()\n",
      "12:36:19 WORKER: kwargs: {'config': {'lr': 0.07692930733079373, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'SGD', 'sgd_momentum': 0.6799487224286553}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:36:30 DISPATCHER: Starting worker discovery\n",
      "12:36:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:36:30 DISPATCHER: Finished worker discovery\n",
      "12:36:32 WORKER: done with job (6, 0, 0), trying to register it.\n",
      "12:36:32 WORKER: registered result for job (6, 0, 0) with dispatcher\n",
      "12:36:32 DISPATCHER: job (6, 0, 0) finished\n",
      "12:36:32 DISPATCHER: register_result: lock acquired\n",
      "12:36:32 DISPATCHER: job (6, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:36:32 job_id: (6, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.07692930733079373, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'SGD', 'sgd_momentum': 0.6799487224286553}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.0625, 'info': {'test_accuracy': 0.936, 'train_accuracy': 0.956787109375, 'valid_accuracy': 0.9375, 'model': 'Sequential(\\n  (0): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=19375, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:36:32 job_callback for (6, 0, 0) started\n",
      "12:36:32 DISPATCHER: Trying to submit another job.\n",
      "12:36:32 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:36:32 job_callback for (6, 0, 0) got condition\n",
      "12:36:32 done building a new model for budget 1.000000 based on 8/16 split\n",
      "Best loss for this budget:0.054688\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:36:32 HBMASTER: Trying to run another job!\n",
      "12:36:32 job_callback for (6, 0, 0) finished\n",
      "12:36:32 start sampling a new configuration.\n",
      "12:36:32 done sampling a new configuration.\n",
      "12:36:32 HBMASTER: schedule new run for iteration 6\n",
      "12:36:32 HBMASTER: trying submitting job (6, 0, 1) to dispatcher\n",
      "12:36:32 HBMASTER: submitting job (6, 0, 1) to dispatcher\n",
      "12:36:32 DISPATCHER: trying to submit job (6, 0, 1)\n",
      "12:36:32 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:36:32 HBMASTER: job (6, 0, 1) submitted to dispatcher\n",
      "12:36:32 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:36:32 DISPATCHER: Trying to submit another job.\n",
      "12:36:32 DISPATCHER: starting job (6, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:36:32 DISPATCHER: job (6, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:36:32 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:36:32 WORKER: start processing job (6, 0, 1)\n",
      "12:36:32 WORKER: args: ()\n",
      "12:36:32 WORKER: kwargs: {'config': {'lr': 0.0006078810364659288, 'num_conv_layers': 1, 'num_filters_1': 20, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:36:43 WORKER: done with job (6, 0, 1), trying to register it.\n",
      "12:36:43 WORKER: registered result for job (6, 0, 1) with dispatcher\n",
      "12:36:43 DISPATCHER: job (6, 0, 1) finished\n",
      "12:36:43 DISPATCHER: register_result: lock acquired\n",
      "12:36:43 DISPATCHER: job (6, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:36:43 job_id: (6, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.0006078810364659288, 'num_conv_layers': 1, 'num_filters_1': 20, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.099609375, 'info': {'test_accuracy': 0.8864, 'train_accuracy': 0.90966796875, 'valid_accuracy': 0.900390625, 'model': 'Sequential(\\n  (0): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=12500, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:36:43 job_callback for (6, 0, 1) started\n",
      "12:36:43 DISPATCHER: Trying to submit another job.\n",
      "12:36:43 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:36:43 job_callback for (6, 0, 1) got condition\n",
      "12:36:43 done building a new model for budget 1.000000 based on 8/17 split\n",
      "Best loss for this budget:0.054688\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:36:43 HBMASTER: Trying to run another job!\n",
      "12:36:43 job_callback for (6, 0, 1) finished\n",
      "12:36:43 start sampling a new configuration.\n",
      "12:36:43 best_vector: [0.5888752428905518, 0.5501458803087967, 0.0009492772818773254, 0, 0.7301238488997215, 0.4584819156832913, 0.9001934659580714], 0.0012318942070413178, 0.1715206703960707, 0.00021129532024876272\n",
      "12:36:43 done sampling a new configuration.\n",
      "12:36:43 HBMASTER: schedule new run for iteration 6\n",
      "12:36:43 HBMASTER: trying submitting job (6, 0, 2) to dispatcher\n",
      "12:36:43 HBMASTER: submitting job (6, 0, 2) to dispatcher\n",
      "12:36:43 DISPATCHER: trying to submit job (6, 0, 2)\n",
      "12:36:43 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:36:43 HBMASTER: job (6, 0, 2) submitted to dispatcher\n",
      "12:36:43 DISPATCHER: Trying to submit another job.\n",
      "12:36:43 DISPATCHER: starting job (6, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:36:43 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:36:43 DISPATCHER: job (6, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:36:43 WORKER: start processing job (6, 0, 2)\n",
      "12:36:43 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:36:43 WORKER: args: ()\n",
      "12:36:43 WORKER: kwargs: {'config': {'lr': 0.003413905201786861, 'num_conv_layers': 2, 'num_filters_1': 4, 'optimizer': 'Adam', 'num_filters_2': 25}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:36:56 WORKER: done with job (6, 0, 2), trying to register it.\n",
      "12:36:56 WORKER: registered result for job (6, 0, 2) with dispatcher\n",
      "12:36:56 DISPATCHER: job (6, 0, 2) finished\n",
      "12:36:56 DISPATCHER: register_result: lock acquired\n",
      "12:36:56 DISPATCHER: job (6, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:36:56 job_id: (6, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.003413905201786861, 'num_conv_layers': 2, 'num_filters_1': 4, 'optimizer': 'Adam', 'num_filters_2': 25}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.068359375, 'info': {'test_accuracy': 0.9292, 'train_accuracy': 0.947509765625, 'valid_accuracy': 0.931640625, 'model': 'Sequential(\\n  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(4, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=12100, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:36:56 job_callback for (6, 0, 2) started\n",
      "12:36:56 DISPATCHER: Trying to submit another job.\n",
      "12:36:56 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:36:56 job_callback for (6, 0, 2) got condition\n",
      "12:36:56 done building a new model for budget 1.000000 based on 8/17 split\n",
      "Best loss for this budget:0.054688\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:36:56 HBMASTER: Trying to run another job!\n",
      "12:36:56 job_callback for (6, 0, 2) finished\n",
      "12:36:56 start sampling a new configuration.\n",
      "12:36:56 best_vector: [0.4164445514429569, 0.2601941771099714, 0.4079591386678796, 0, 0.8784732301183035, 0.36310248570900505, 0.23127472048068376], 5.234913812364979e-05, 1643.6085219314207, 0.08604148953579581\n",
      "12:36:56 done sampling a new configuration.\n",
      "12:36:56 HBMASTER: schedule new run for iteration 6\n",
      "12:36:56 HBMASTER: trying submitting job (6, 0, 3) to dispatcher\n",
      "12:36:56 HBMASTER: submitting job (6, 0, 3) to dispatcher\n",
      "12:36:56 DISPATCHER: trying to submit job (6, 0, 3)\n",
      "12:36:56 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:36:56 HBMASTER: job (6, 0, 3) submitted to dispatcher\n",
      "12:36:56 DISPATCHER: Trying to submit another job.\n",
      "12:36:56 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:36:56 DISPATCHER: starting job (6, 0, 3) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:36:56 DISPATCHER: job (6, 0, 3) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:36:56 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:36:56 WORKER: start processing job (6, 0, 3)\n",
      "12:36:56 WORKER: args: ()\n",
      "12:36:56 WORKER: kwargs: {'config': {'lr': 0.00031525886571814665, 'num_conv_layers': 1, 'num_filters_1': 15, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:37:05 WORKER: done with job (6, 0, 3), trying to register it.\n",
      "12:37:05 WORKER: registered result for job (6, 0, 3) with dispatcher\n",
      "12:37:05 DISPATCHER: job (6, 0, 3) finished\n",
      "12:37:05 DISPATCHER: register_result: lock acquired\n",
      "12:37:05 DISPATCHER: job (6, 0, 3) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:37:05 job_id: (6, 0, 3)\n",
      "kwargs: {'config': {'lr': 0.00031525886571814665, 'num_conv_layers': 1, 'num_filters_1': 15, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.11328125, 'info': {'test_accuracy': 0.8744, 'train_accuracy': 0.8828125, 'valid_accuracy': 0.88671875, 'model': 'Sequential(\\n  (0): Conv2d(1, 15, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=9375, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:37:05 job_callback for (6, 0, 3) started\n",
      "12:37:05 DISPATCHER: Trying to submit another job.\n",
      "12:37:05 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:37:05 job_callback for (6, 0, 3) got condition\n",
      "12:37:05 done building a new model for budget 1.000000 based on 8/18 split\n",
      "Best loss for this budget:0.054688\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:37:05 HBMASTER: Trying to run another job!\n",
      "12:37:05 job_callback for (6, 0, 3) finished\n",
      "12:37:05 start sampling a new configuration.\n",
      "12:37:05 best_vector: [0.30002718971155073, 0.19957210841284018, 0.1726876771421303, 0, 0.7416916956208082, 0.46648254621710183, 0.7700593920030864], 5.028979099797932e-07, 2193.142225985374, 0.0011029266417364758\n",
      "12:37:05 done sampling a new configuration.\n",
      "12:37:05 HBMASTER: schedule new run for iteration 6\n",
      "12:37:05 HBMASTER: trying submitting job (6, 0, 4) to dispatcher\n",
      "12:37:05 HBMASTER: submitting job (6, 0, 4) to dispatcher\n",
      "12:37:05 DISPATCHER: trying to submit job (6, 0, 4)\n",
      "12:37:05 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:37:05 HBMASTER: job (6, 0, 4) submitted to dispatcher\n",
      "12:37:05 DISPATCHER: Trying to submit another job.\n",
      "12:37:05 DISPATCHER: starting job (6, 0, 4) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:37:05 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:37:05 DISPATCHER: job (6, 0, 4) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:37:05 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:37:05 WORKER: start processing job (6, 0, 4)\n",
      "12:37:05 WORKER: args: ()\n",
      "12:37:05 WORKER: kwargs: {'config': {'lr': 6.31194401658702e-05, 'num_conv_layers': 1, 'num_filters_1': 9, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:37:11 WORKER: done with job (6, 0, 4), trying to register it.\n",
      "12:37:11 WORKER: registered result for job (6, 0, 4) with dispatcher\n",
      "12:37:11 DISPATCHER: job (6, 0, 4) finished\n",
      "12:37:11 DISPATCHER: register_result: lock acquired\n",
      "12:37:11 DISPATCHER: job (6, 0, 4) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:37:11 job_id: (6, 0, 4)\n",
      "kwargs: {'config': {'lr': 6.31194401658702e-05, 'num_conv_layers': 1, 'num_filters_1': 9, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.17578125, 'info': {'test_accuracy': 0.7885, 'train_accuracy': 0.8017578125, 'valid_accuracy': 0.82421875, 'model': 'Sequential(\\n  (0): Conv2d(1, 9, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=5625, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:37:11 job_callback for (6, 0, 4) started\n",
      "12:37:11 DISPATCHER: Trying to submit another job.\n",
      "12:37:11 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:37:11 job_callback for (6, 0, 4) got condition\n",
      "12:37:11 done building a new model for budget 1.000000 based on 8/19 split\n",
      "Best loss for this budget:0.054688\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:37:11 HBMASTER: Trying to run another job!\n",
      "12:37:11 job_callback for (6, 0, 4) finished\n",
      "12:37:11 start sampling a new configuration.\n",
      "12:37:11 done sampling a new configuration.\n",
      "12:37:11 HBMASTER: schedule new run for iteration 6\n",
      "12:37:11 HBMASTER: trying submitting job (6, 0, 5) to dispatcher\n",
      "12:37:11 HBMASTER: submitting job (6, 0, 5) to dispatcher\n",
      "12:37:11 DISPATCHER: trying to submit job (6, 0, 5)\n",
      "12:37:11 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:37:11 HBMASTER: job (6, 0, 5) submitted to dispatcher\n",
      "12:37:11 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:37:11 DISPATCHER: Trying to submit another job.\n",
      "12:37:11 DISPATCHER: starting job (6, 0, 5) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:37:11 DISPATCHER: job (6, 0, 5) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:37:11 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:37:11 WORKER: start processing job (6, 0, 5)\n",
      "12:37:11 WORKER: args: ()\n",
      "12:37:11 WORKER: kwargs: {'config': {'lr': 8.820133130931062e-06, 'num_conv_layers': 2, 'num_filters_1': 28, 'optimizer': 'SGD', 'num_filters_2': 24, 'sgd_momentum': 0.17959315571204565}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:37:30 DISPATCHER: Starting worker discovery\n",
      "12:37:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:37:30 DISPATCHER: Finished worker discovery\n",
      "12:37:35 WORKER: done with job (6, 0, 5), trying to register it.\n",
      "12:37:35 WORKER: registered result for job (6, 0, 5) with dispatcher\n",
      "12:37:35 DISPATCHER: job (6, 0, 5) finished\n",
      "12:37:35 DISPATCHER: register_result: lock acquired\n",
      "12:37:35 DISPATCHER: job (6, 0, 5) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:37:35 job_id: (6, 0, 5)\n",
      "kwargs: {'config': {'lr': 8.820133130931062e-06, 'num_conv_layers': 2, 'num_filters_1': 28, 'optimizer': 'SGD', 'num_filters_2': 24, 'sgd_momentum': 0.17959315571204565}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.93359375, 'info': {'test_accuracy': 0.0911, 'train_accuracy': 0.0791015625, 'valid_accuracy': 0.06640625, 'model': 'Sequential(\\n  (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(28, 24, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=11616, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:37:35 job_callback for (6, 0, 5) started\n",
      "12:37:35 DISPATCHER: Trying to submit another job.\n",
      "12:37:35 job_callback for (6, 0, 5) got condition\n",
      "12:37:35 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:37:35 done building a new model for budget 1.000000 based on 8/20 split\n",
      "Best loss for this budget:0.054688\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:37:35 HBMASTER: Trying to run another job!\n",
      "12:37:35 job_callback for (6, 0, 5) finished\n",
      "12:37:35 start sampling a new configuration.\n",
      "12:37:35 best_vector: [0.5398500288781559, 0.37990308924606303, 0.2126197480898075, 0, 0.7419597243415956, 0.7609046924827083, 0.7303438314195448], 2.279714687398493e-06, 2842.3912809772805, 0.006479841150577323\n",
      "12:37:35 done sampling a new configuration.\n",
      "12:37:35 HBMASTER: schedule new run for iteration 6\n",
      "12:37:35 HBMASTER: trying submitting job (6, 0, 6) to dispatcher\n",
      "12:37:35 HBMASTER: submitting job (6, 0, 6) to dispatcher\n",
      "12:37:35 DISPATCHER: trying to submit job (6, 0, 6)\n",
      "12:37:35 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:37:35 HBMASTER: job (6, 0, 6) submitted to dispatcher\n",
      "12:37:35 DISPATCHER: Trying to submit another job.\n",
      "12:37:35 DISPATCHER: starting job (6, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:37:35 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:37:35 DISPATCHER: job (6, 0, 6) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:37:35 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:37:35 WORKER: start processing job (6, 0, 6)\n",
      "12:37:35 WORKER: args: ()\n",
      "12:37:35 WORKER: kwargs: {'config': {'lr': 0.0017342039587329567, 'num_conv_layers': 2, 'num_filters_1': 10, 'optimizer': 'Adam', 'num_filters_2': 25}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:37:50 WORKER: done with job (6, 0, 6), trying to register it.\n",
      "12:37:50 DISPATCHER: job (6, 0, 6) finished\n",
      "12:37:50 WORKER: registered result for job (6, 0, 6) with dispatcher\n",
      "12:37:50 DISPATCHER: register_result: lock acquired\n",
      "12:37:50 DISPATCHER: job (6, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:37:50 job_id: (6, 0, 6)\n",
      "kwargs: {'config': {'lr': 0.0017342039587329567, 'num_conv_layers': 2, 'num_filters_1': 10, 'optimizer': 'Adam', 'num_filters_2': 25}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.046875, 'info': {'test_accuracy': 0.9478, 'train_accuracy': 0.962646484375, 'valid_accuracy': 0.953125, 'model': 'Sequential(\\n  (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(10, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=12100, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:37:50 job_callback for (6, 0, 6) started\n",
      "12:37:50 DISPATCHER: Trying to submit another job.\n",
      "12:37:50 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:37:50 job_callback for (6, 0, 6) got condition\n",
      "12:37:50 done building a new model for budget 1.000000 based on 8/21 split\n",
      "Best loss for this budget:0.046875\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:37:50 HBMASTER: Trying to run another job!\n",
      "12:37:50 job_callback for (6, 0, 6) finished\n",
      "12:37:50 start sampling a new configuration.\n",
      "12:37:50 best_vector: [0.5220604601216219, 0.2463356401202118, 0.17919517147038663, 0, 0.742966426428552, 0.8734925805578062, 0.7321883218001917], 2.0779887267644343e-06, 591.2050414846274, 0.0012285174114113554\n",
      "12:37:50 done sampling a new configuration.\n",
      "12:37:50 HBMASTER: schedule new run for iteration 6\n",
      "12:37:50 HBMASTER: trying submitting job (6, 0, 7) to dispatcher\n",
      "12:37:50 HBMASTER: submitting job (6, 0, 7) to dispatcher\n",
      "12:37:50 DISPATCHER: trying to submit job (6, 0, 7)\n",
      "12:37:50 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:37:50 HBMASTER: job (6, 0, 7) submitted to dispatcher\n",
      "12:37:50 DISPATCHER: Trying to submit another job.\n",
      "12:37:50 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:37:50 DISPATCHER: starting job (6, 0, 7) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:37:50 DISPATCHER: job (6, 0, 7) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:37:50 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:37:50 WORKER: start processing job (6, 0, 7)\n",
      "12:37:50 WORKER: args: ()\n",
      "12:37:50 WORKER: kwargs: {'config': {'lr': 0.001356321857950608, 'num_conv_layers': 1, 'num_filters_1': 9, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:37:56 WORKER: done with job (6, 0, 7), trying to register it.\n",
      "12:37:56 WORKER: registered result for job (6, 0, 7) with dispatcher\n",
      "12:37:56 DISPATCHER: job (6, 0, 7) finished\n",
      "12:37:56 DISPATCHER: register_result: lock acquired\n",
      "12:37:56 DISPATCHER: job (6, 0, 7) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:37:56 job_id: (6, 0, 7)\n",
      "kwargs: {'config': {'lr': 0.001356321857950608, 'num_conv_layers': 1, 'num_filters_1': 9, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.09765625, 'info': {'test_accuracy': 0.8989, 'train_accuracy': 0.916748046875, 'valid_accuracy': 0.90234375, 'model': 'Sequential(\\n  (0): Conv2d(1, 9, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=5625, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:37:56 job_callback for (6, 0, 7) started\n",
      "12:37:56 DISPATCHER: Trying to submit another job.\n",
      "12:37:56 job_callback for (6, 0, 7) got condition\n",
      "12:37:56 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:37:56 done building a new model for budget 1.000000 based on 8/22 split\n",
      "Best loss for this budget:0.046875\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:37:56 HBMASTER: Trying to run another job!\n",
      "12:37:56 job_callback for (6, 0, 7) finished\n",
      "12:37:56 start sampling a new configuration.\n",
      "12:37:56 best_vector: [0.5285873023403665, 0.12194880033894377, 0.5498471982244887, 0, 0.7417867220274198, 0.4300505484669994, 0.7103226956269895], 6.152401241452049e-06, 9250.672023852496, 0.05691384604381584\n",
      "12:37:56 done sampling a new configuration.\n",
      "12:37:56 HBMASTER: schedule new run for iteration 6\n",
      "12:37:56 HBMASTER: trying submitting job (6, 0, 8) to dispatcher\n",
      "12:37:56 HBMASTER: submitting job (6, 0, 8) to dispatcher\n",
      "12:37:56 DISPATCHER: trying to submit job (6, 0, 8)\n",
      "12:37:56 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:37:56 HBMASTER: job (6, 0, 8) submitted to dispatcher\n",
      "12:37:56 DISPATCHER: Trying to submit another job.\n",
      "12:37:56 DISPATCHER: starting job (6, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:37:56 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:37:56 DISPATCHER: job (6, 0, 8) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:37:56 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:37:56 WORKER: start processing job (6, 0, 8)\n",
      "12:37:56 WORKER: args: ()\n",
      "12:37:56 WORKER: kwargs: {'config': {'lr': 0.0014843072681279604, 'num_conv_layers': 1, 'num_filters_1': 19, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:38:06 WORKER: done with job (6, 0, 8), trying to register it.\n",
      "12:38:06 WORKER: registered result for job (6, 0, 8) with dispatcher\n",
      "12:38:06 DISPATCHER: job (6, 0, 8) finished\n",
      "12:38:06 DISPATCHER: register_result: lock acquired\n",
      "12:38:06 DISPATCHER: job (6, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:38:06 job_id: (6, 0, 8)\n",
      "kwargs: {'config': {'lr': 0.0014843072681279604, 'num_conv_layers': 1, 'num_filters_1': 19, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.0859375, 'info': {'test_accuracy': 0.8933, 'train_accuracy': 0.9208984375, 'valid_accuracy': 0.9140625, 'model': 'Sequential(\\n  (0): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=11875, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:38:06 job_callback for (6, 0, 8) started\n",
      "12:38:06 DISPATCHER: Trying to submit another job.\n",
      "12:38:06 job_callback for (6, 0, 8) got condition\n",
      "12:38:06 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:38:06 done building a new model for budget 1.000000 based on 8/22 split\n",
      "Best loss for this budget:0.046875\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:38:06 HBMASTER: Trying to run another job!\n",
      "12:38:06 job_callback for (6, 0, 8) finished\n",
      "12:38:06 ITERATION: Advancing config (6, 0, 0) to next budget 3.000000\n",
      "12:38:06 ITERATION: Advancing config (6, 0, 2) to next budget 3.000000\n",
      "12:38:06 ITERATION: Advancing config (6, 0, 6) to next budget 3.000000\n",
      "12:38:06 HBMASTER: schedule new run for iteration 6\n",
      "12:38:06 HBMASTER: trying submitting job (6, 0, 0) to dispatcher\n",
      "12:38:06 HBMASTER: submitting job (6, 0, 0) to dispatcher\n",
      "12:38:06 DISPATCHER: trying to submit job (6, 0, 0)\n",
      "12:38:06 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:38:06 HBMASTER: job (6, 0, 0) submitted to dispatcher\n",
      "12:38:06 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:38:06 DISPATCHER: Trying to submit another job.\n",
      "12:38:06 DISPATCHER: starting job (6, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:38:06 DISPATCHER: job (6, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:38:06 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:38:06 WORKER: start processing job (6, 0, 0)\n",
      "12:38:06 WORKER: args: ()\n",
      "12:38:06 WORKER: kwargs: {'config': {'lr': 0.07692930733079373, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'SGD', 'sgd_momentum': 0.6799487224286553}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:38:30 DISPATCHER: Starting worker discovery\n",
      "12:38:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:38:30 DISPATCHER: Finished worker discovery\n",
      "12:38:30 WORKER: done with job (6, 0, 0), trying to register it.\n",
      "12:38:30 WORKER: registered result for job (6, 0, 0) with dispatcher\n",
      "12:38:30 DISPATCHER: job (6, 0, 0) finished\n",
      "12:38:30 DISPATCHER: register_result: lock acquired\n",
      "12:38:30 DISPATCHER: job (6, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:38:30 job_id: (6, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.07692930733079373, 'num_conv_layers': 1, 'num_filters_1': 31, 'optimizer': 'SGD', 'sgd_momentum': 0.6799487224286553}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.048828125, 'info': {'test_accuracy': 0.9493, 'train_accuracy': 0.980712890625, 'valid_accuracy': 0.951171875, 'model': 'Sequential(\\n  (0): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=19375, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:38:30 job_callback for (6, 0, 0) started\n",
      "12:38:30 DISPATCHER: Trying to submit another job.\n",
      "12:38:30 job_callback for (6, 0, 0) got condition\n",
      "12:38:30 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:38:30 HBMASTER: Trying to run another job!\n",
      "12:38:30 job_callback for (6, 0, 0) finished\n",
      "12:38:30 HBMASTER: schedule new run for iteration 6\n",
      "12:38:30 HBMASTER: trying submitting job (6, 0, 2) to dispatcher\n",
      "12:38:30 HBMASTER: submitting job (6, 0, 2) to dispatcher\n",
      "12:38:30 DISPATCHER: trying to submit job (6, 0, 2)\n",
      "12:38:30 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:38:30 HBMASTER: job (6, 0, 2) submitted to dispatcher\n",
      "12:38:30 DISPATCHER: Trying to submit another job.\n",
      "12:38:30 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:38:30 DISPATCHER: starting job (6, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:38:30 DISPATCHER: job (6, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:38:30 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:38:30 WORKER: start processing job (6, 0, 2)\n",
      "12:38:30 WORKER: args: ()\n",
      "12:38:30 WORKER: kwargs: {'config': {'lr': 0.003413905201786861, 'num_conv_layers': 2, 'num_filters_1': 4, 'optimizer': 'Adam', 'num_filters_2': 25}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:38:51 WORKER: done with job (6, 0, 2), trying to register it.\n",
      "12:38:51 WORKER: registered result for job (6, 0, 2) with dispatcher\n",
      "12:38:51 DISPATCHER: job (6, 0, 2) finished\n",
      "12:38:51 DISPATCHER: register_result: lock acquired\n",
      "12:38:51 DISPATCHER: job (6, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:38:51 job_id: (6, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.003413905201786861, 'num_conv_layers': 2, 'num_filters_1': 4, 'optimizer': 'Adam', 'num_filters_2': 25}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.04296875, 'info': {'test_accuracy': 0.9641, 'train_accuracy': 0.989990234375, 'valid_accuracy': 0.95703125, 'model': 'Sequential(\\n  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(4, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=12100, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:38:51 job_callback for (6, 0, 2) started\n",
      "12:38:51 DISPATCHER: Trying to submit another job.\n",
      "12:38:51 job_callback for (6, 0, 2) got condition\n",
      "12:38:51 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:38:51 HBMASTER: Trying to run another job!\n",
      "12:38:51 job_callback for (6, 0, 2) finished\n",
      "12:38:51 HBMASTER: schedule new run for iteration 6\n",
      "12:38:51 HBMASTER: trying submitting job (6, 0, 6) to dispatcher\n",
      "12:38:51 HBMASTER: submitting job (6, 0, 6) to dispatcher\n",
      "12:38:51 DISPATCHER: trying to submit job (6, 0, 6)\n",
      "12:38:51 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:38:51 HBMASTER: job (6, 0, 6) submitted to dispatcher\n",
      "12:38:51 DISPATCHER: Trying to submit another job.\n",
      "12:38:51 DISPATCHER: starting job (6, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:38:51 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:38:51 DISPATCHER: job (6, 0, 6) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:38:51 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:38:51 WORKER: start processing job (6, 0, 6)\n",
      "12:38:51 WORKER: args: ()\n",
      "12:38:51 WORKER: kwargs: {'config': {'lr': 0.0017342039587329567, 'num_conv_layers': 2, 'num_filters_1': 10, 'optimizer': 'Adam', 'num_filters_2': 25}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:39:18 WORKER: done with job (6, 0, 6), trying to register it.\n",
      "12:39:18 WORKER: registered result for job (6, 0, 6) with dispatcher\n",
      "12:39:18 DISPATCHER: job (6, 0, 6) finished\n",
      "12:39:18 DISPATCHER: register_result: lock acquired\n",
      "12:39:18 DISPATCHER: job (6, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:39:18 job_id: (6, 0, 6)\n",
      "kwargs: {'config': {'lr': 0.0017342039587329567, 'num_conv_layers': 2, 'num_filters_1': 10, 'optimizer': 'Adam', 'num_filters_2': 25}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.041015625, 'info': {'test_accuracy': 0.9639, 'train_accuracy': 0.98974609375, 'valid_accuracy': 0.958984375, 'model': 'Sequential(\\n  (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(10, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=12100, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:39:18 job_callback for (6, 0, 6) started\n",
      "12:39:18 job_callback for (6, 0, 6) got condition\n",
      "12:39:18 DISPATCHER: Trying to submit another job.\n",
      "12:39:18 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:39:18 HBMASTER: Trying to run another job!\n",
      "12:39:18 job_callback for (6, 0, 6) finished\n",
      "12:39:18 ITERATION: Advancing config (6, 0, 6) to next budget 9.000000\n",
      "12:39:18 HBMASTER: schedule new run for iteration 6\n",
      "12:39:18 HBMASTER: trying submitting job (6, 0, 6) to dispatcher\n",
      "12:39:18 HBMASTER: submitting job (6, 0, 6) to dispatcher\n",
      "12:39:18 DISPATCHER: trying to submit job (6, 0, 6)\n",
      "12:39:18 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:39:18 HBMASTER: job (6, 0, 6) submitted to dispatcher\n",
      "12:39:18 DISPATCHER: Trying to submit another job.\n",
      "12:39:18 DISPATCHER: starting job (6, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:39:18 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:39:18 DISPATCHER: job (6, 0, 6) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:39:18 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:39:18 WORKER: start processing job (6, 0, 6)\n",
      "12:39:18 WORKER: args: ()\n",
      "12:39:18 WORKER: kwargs: {'config': {'lr': 0.0017342039587329567, 'num_conv_layers': 2, 'num_filters_1': 10, 'optimizer': 'Adam', 'num_filters_2': 25}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:39:30 DISPATCHER: Starting worker discovery\n",
      "12:39:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:39:30 DISPATCHER: Finished worker discovery\n",
      "12:40:20 WORKER: done with job (6, 0, 6), trying to register it.\n",
      "12:40:20 WORKER: registered result for job (6, 0, 6) with dispatcher\n",
      "12:40:20 DISPATCHER: job (6, 0, 6) finished\n",
      "12:40:20 DISPATCHER: register_result: lock acquired\n",
      "12:40:20 DISPATCHER: job (6, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:40:20 job_id: (6, 0, 6)\n",
      "kwargs: {'config': {'lr': 0.0017342039587329567, 'num_conv_layers': 2, 'num_filters_1': 10, 'optimizer': 'Adam', 'num_filters_2': 25}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.03125, 'info': {'test_accuracy': 0.9743, 'train_accuracy': 0.99951171875, 'valid_accuracy': 0.96875, 'model': 'Sequential(\\n  (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(10, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=12100, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:40:20 job_callback for (6, 0, 6) started\n",
      "12:40:20 DISPATCHER: Trying to submit another job.\n",
      "12:40:20 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:40:20 job_callback for (6, 0, 6) got condition\n",
      "12:40:20 HBMASTER: Trying to run another job!\n",
      "12:40:20 job_callback for (6, 0, 6) finished\n",
      "12:40:20 start sampling a new configuration.\n",
      "12:40:20 done sampling a new configuration.\n",
      "12:40:20 HBMASTER: schedule new run for iteration 7\n",
      "12:40:20 HBMASTER: trying submitting job (7, 0, 0) to dispatcher\n",
      "12:40:20 HBMASTER: submitting job (7, 0, 0) to dispatcher\n",
      "12:40:20 DISPATCHER: trying to submit job (7, 0, 0)\n",
      "12:40:20 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:40:20 HBMASTER: job (7, 0, 0) submitted to dispatcher\n",
      "12:40:20 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:40:20 DISPATCHER: Trying to submit another job.\n",
      "12:40:20 DISPATCHER: starting job (7, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:40:20 DISPATCHER: job (7, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:40:20 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:40:20 WORKER: start processing job (7, 0, 0)\n",
      "12:40:20 WORKER: args: ()\n",
      "12:40:20 WORKER: kwargs: {'config': {'lr': 0.20012222980845704, 'num_conv_layers': 1, 'num_filters_1': 18, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:40:30 DISPATCHER: Starting worker discovery\n",
      "12:40:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:40:30 DISPATCHER: Finished worker discovery\n",
      "12:40:37 WORKER: done with job (7, 0, 0), trying to register it.\n",
      "12:40:37 WORKER: registered result for job (7, 0, 0) with dispatcher\n",
      "12:40:37 DISPATCHER: job (7, 0, 0) finished\n",
      "12:40:37 DISPATCHER: register_result: lock acquired\n",
      "12:40:37 DISPATCHER: job (7, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:40:37 job_id: (7, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.20012222980845704, 'num_conv_layers': 1, 'num_filters_1': 18, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.88671875, 'info': {'test_accuracy': 0.1135, 'train_accuracy': 0.111328125, 'valid_accuracy': 0.11328125, 'model': 'Sequential(\\n  (0): Conv2d(1, 18, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=11250, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:40:37 job_callback for (7, 0, 0) started\n",
      "12:40:37 job_callback for (7, 0, 0) got condition\n",
      "12:40:37 DISPATCHER: Trying to submit another job.\n",
      "12:40:37 done building a new model for budget 3.000000 based on 8/13 split\n",
      "Best loss for this budget:0.041016\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:40:37 HBMASTER: Trying to run another job!\n",
      "12:40:37 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:40:37 job_callback for (7, 0, 0) finished\n",
      "12:40:37 start sampling a new configuration.\n",
      "12:40:37 best_vector: [0.43700619187507217, 0.3885371910921816, 0.2694198428264188, 1, 0.7736463812295259, 0.7401832755610317, 0.9840272506409635], 1.9922924409445484e-07, 15.038932917351524, 2.9961952371111584e-06\n",
      "12:40:37 done sampling a new configuration.\n",
      "12:40:37 HBMASTER: schedule new run for iteration 7\n",
      "12:40:37 HBMASTER: trying submitting job (7, 0, 1) to dispatcher\n",
      "12:40:37 HBMASTER: submitting job (7, 0, 1) to dispatcher\n",
      "12:40:37 DISPATCHER: trying to submit job (7, 0, 1)\n",
      "12:40:37 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:40:37 HBMASTER: job (7, 0, 1) submitted to dispatcher\n",
      "12:40:37 DISPATCHER: Trying to submit another job.\n",
      "12:40:37 DISPATCHER: starting job (7, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:40:37 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:40:37 DISPATCHER: job (7, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:40:37 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:40:37 WORKER: start processing job (7, 0, 1)\n",
      "12:40:37 WORKER: args: ()\n",
      "12:40:37 WORKER: kwargs: {'config': {'lr': 0.00041882939189159644, 'num_conv_layers': 2, 'num_filters_1': 11, 'optimizer': 'SGD', 'num_filters_2': 26, 'sgd_momentum': 0.9741869781345539}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:41:06 WORKER: done with job (7, 0, 1), trying to register it.\n",
      "12:41:06 WORKER: registered result for job (7, 0, 1) with dispatcher\n",
      "12:41:06 DISPATCHER: job (7, 0, 1) finished\n",
      "12:41:06 DISPATCHER: register_result: lock acquired\n",
      "12:41:06 DISPATCHER: job (7, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:41:06 job_id: (7, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.00041882939189159644, 'num_conv_layers': 2, 'num_filters_1': 11, 'optimizer': 'SGD', 'num_filters_2': 26, 'sgd_momentum': 0.9741869781345539}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.08984375, 'info': {'test_accuracy': 0.9043, 'train_accuracy': 0.919189453125, 'valid_accuracy': 0.91015625, 'model': 'Sequential(\\n  (0): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(11, 26, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=12584, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:41:06 job_callback for (7, 0, 1) started\n",
      "12:41:06 DISPATCHER: Trying to submit another job.\n",
      "12:41:06 job_callback for (7, 0, 1) got condition\n",
      "12:41:06 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:41:06 done building a new model for budget 3.000000 based on 8/14 split\n",
      "Best loss for this budget:0.041016\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:41:06 HBMASTER: Trying to run another job!\n",
      "12:41:06 job_callback for (7, 0, 1) finished\n",
      "12:41:06 start sampling a new configuration.\n",
      "12:41:06 best_vector: [0.681103218154917, 0.556370643820199, 0.9018638039264006, 1, 0.8214384427345206, 0.7418522654235424, 0.42277368400391213], 3.162775028985792e-05, 384.2748976334865, 0.012153750505012625\n",
      "12:41:06 done sampling a new configuration.\n",
      "12:41:06 HBMASTER: schedule new run for iteration 7\n",
      "12:41:06 HBMASTER: trying submitting job (7, 0, 2) to dispatcher\n",
      "12:41:06 HBMASTER: submitting job (7, 0, 2) to dispatcher\n",
      "12:41:06 DISPATCHER: trying to submit job (7, 0, 2)\n",
      "12:41:06 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:41:06 HBMASTER: job (7, 0, 2) submitted to dispatcher\n",
      "12:41:06 DISPATCHER: Trying to submit another job.\n",
      "12:41:06 DISPATCHER: starting job (7, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:41:06 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:41:06 DISPATCHER: job (7, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:41:06 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:41:06 WORKER: start processing job (7, 0, 2)\n",
      "12:41:06 WORKER: args: ()\n",
      "12:41:06 WORKER: kwargs: {'config': {'lr': 0.012207291321247232, 'num_conv_layers': 2, 'num_filters_1': 30, 'optimizer': 'SGD', 'num_filters_2': 27, 'sgd_momentum': 0.418545947163873}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:41:30 DISPATCHER: Starting worker discovery\n",
      "12:41:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:41:30 DISPATCHER: Finished worker discovery\n",
      "12:42:01 WORKER: done with job (7, 0, 2), trying to register it.\n",
      "12:42:01 WORKER: registered result for job (7, 0, 2) with dispatcher\n",
      "12:42:01 DISPATCHER: job (7, 0, 2) finished\n",
      "12:42:01 DISPATCHER: register_result: lock acquired\n",
      "12:42:01 DISPATCHER: job (7, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:42:01 job_id: (7, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.012207291321247232, 'num_conv_layers': 2, 'num_filters_1': 30, 'optimizer': 'SGD', 'num_filters_2': 27, 'sgd_momentum': 0.418545947163873}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.068359375, 'info': {'test_accuracy': 0.9203, 'train_accuracy': 0.93701171875, 'valid_accuracy': 0.931640625, 'model': 'Sequential(\\n  (0): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(30, 27, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=13068, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:42:01 job_callback for (7, 0, 2) started\n",
      "12:42:01 job_callback for (7, 0, 2) got condition\n",
      "12:42:01 done building a new model for budget 3.000000 based on 8/15 split\n",
      "Best loss for this budget:0.041016\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:42:01 HBMASTER: Trying to run another job!\n",
      "12:42:01 job_callback for (7, 0, 2) finished\n",
      "12:42:01 DISPATCHER: Trying to submit another job.\n",
      "12:42:01 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:42:01 ITERATION: Advancing config (7, 0, 2) to next budget 9.000000\n",
      "12:42:01 HBMASTER: schedule new run for iteration 7\n",
      "12:42:01 HBMASTER: trying submitting job (7, 0, 2) to dispatcher\n",
      "12:42:01 HBMASTER: submitting job (7, 0, 2) to dispatcher\n",
      "12:42:01 DISPATCHER: trying to submit job (7, 0, 2)\n",
      "12:42:01 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:42:01 HBMASTER: job (7, 0, 2) submitted to dispatcher\n",
      "12:42:01 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:42:01 DISPATCHER: Trying to submit another job.\n",
      "12:42:01 DISPATCHER: starting job (7, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:42:01 DISPATCHER: job (7, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:42:01 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:42:01 WORKER: start processing job (7, 0, 2)\n",
      "12:42:01 WORKER: args: ()\n",
      "12:42:01 WORKER: kwargs: {'config': {'lr': 0.012207291321247232, 'num_conv_layers': 2, 'num_filters_1': 30, 'optimizer': 'SGD', 'num_filters_2': 27, 'sgd_momentum': 0.418545947163873}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:42:30 DISPATCHER: Starting worker discovery\n",
      "12:42:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:42:30 DISPATCHER: Finished worker discovery\n",
      "12:43:30 DISPATCHER: Starting worker discovery\n",
      "12:43:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:43:30 DISPATCHER: Finished worker discovery\n",
      "12:44:05 WORKER: done with job (7, 0, 2), trying to register it.\n",
      "12:44:05 WORKER: registered result for job (7, 0, 2) with dispatcher\n",
      "12:44:05 DISPATCHER: job (7, 0, 2) finished\n",
      "12:44:05 DISPATCHER: register_result: lock acquired\n",
      "12:44:05 DISPATCHER: job (7, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:44:05 job_id: (7, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.012207291321247232, 'num_conv_layers': 2, 'num_filters_1': 30, 'optimizer': 'SGD', 'num_filters_2': 27, 'sgd_momentum': 0.418545947163873}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.0390625, 'info': {'test_accuracy': 0.953, 'train_accuracy': 0.98291015625, 'valid_accuracy': 0.9609375, 'model': 'Sequential(\\n  (0): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(30, 27, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=13068, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:44:05 job_callback for (7, 0, 2) started\n",
      "12:44:05 DISPATCHER: Trying to submit another job.\n",
      "12:44:05 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:44:05 job_callback for (7, 0, 2) got condition\n",
      "12:44:05 HBMASTER: Trying to run another job!\n",
      "12:44:05 job_callback for (7, 0, 2) finished\n",
      "12:44:05 start sampling a new configuration.\n",
      "12:44:05 done sampling a new configuration.\n",
      "12:44:05 HBMASTER: schedule new run for iteration 8\n",
      "12:44:05 HBMASTER: trying submitting job (8, 0, 0) to dispatcher\n",
      "12:44:05 HBMASTER: submitting job (8, 0, 0) to dispatcher\n",
      "12:44:05 DISPATCHER: trying to submit job (8, 0, 0)\n",
      "12:44:05 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:44:05 HBMASTER: job (8, 0, 0) submitted to dispatcher\n",
      "12:44:05 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:44:05 DISPATCHER: Trying to submit another job.\n",
      "12:44:05 DISPATCHER: starting job (8, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:44:05 DISPATCHER: job (8, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:44:05 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:44:05 WORKER: start processing job (8, 0, 0)\n",
      "12:44:05 WORKER: args: ()\n",
      "12:44:05 WORKER: kwargs: {'config': {'lr': 0.00010287936049608379, 'num_conv_layers': 1, 'num_filters_1': 6, 'optimizer': 'Adam'}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:44:28 WORKER: done with job (8, 0, 0), trying to register it.\n",
      "12:44:28 WORKER: registered result for job (8, 0, 0) with dispatcher\n",
      "12:44:28 DISPATCHER: job (8, 0, 0) finished\n",
      "12:44:28 DISPATCHER: register_result: lock acquired\n",
      "12:44:28 DISPATCHER: job (8, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:44:28 job_id: (8, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.00010287936049608379, 'num_conv_layers': 1, 'num_filters_1': 6, 'optimizer': 'Adam'}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.080078125, 'info': {'test_accuracy': 0.9106, 'train_accuracy': 0.9248046875, 'valid_accuracy': 0.919921875, 'model': 'Sequential(\\n  (0): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=3750, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:44:28 job_callback for (8, 0, 0) started\n",
      "12:44:28 DISPATCHER: Trying to submit another job.\n",
      "12:44:28 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:44:28 job_callback for (8, 0, 0) got condition\n",
      "12:44:28 HBMASTER: Trying to run another job!\n",
      "12:44:28 job_callback for (8, 0, 0) finished\n",
      "12:44:28 start sampling a new configuration.\n",
      "12:44:28 done sampling a new configuration.\n",
      "12:44:28 HBMASTER: schedule new run for iteration 8\n",
      "12:44:28 HBMASTER: trying submitting job (8, 0, 1) to dispatcher\n",
      "12:44:28 HBMASTER: submitting job (8, 0, 1) to dispatcher\n",
      "12:44:28 DISPATCHER: trying to submit job (8, 0, 1)\n",
      "12:44:28 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:44:28 HBMASTER: job (8, 0, 1) submitted to dispatcher\n",
      "12:44:28 DISPATCHER: Trying to submit another job.\n",
      "12:44:28 DISPATCHER: starting job (8, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:44:28 DISPATCHER: job (8, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:44:28 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:44:28 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:44:28 WORKER: start processing job (8, 0, 1)\n",
      "12:44:28 WORKER: args: ()\n",
      "12:44:28 WORKER: kwargs: {'config': {'lr': 0.003605937704175232, 'num_conv_layers': 3, 'num_filters_1': 18, 'optimizer': 'SGD', 'num_filters_2': 6, 'num_filters_3': 16, 'sgd_momentum': 0.4855462285209849}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:44:30 DISPATCHER: Starting worker discovery\n",
      "12:44:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:44:30 DISPATCHER: Finished worker discovery\n",
      "12:45:30 DISPATCHER: Starting worker discovery\n",
      "12:45:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:45:30 DISPATCHER: Finished worker discovery\n",
      "12:46:06 WORKER: done with job (8, 0, 1), trying to register it.\n",
      "12:46:06 WORKER: registered result for job (8, 0, 1) with dispatcher\n",
      "12:46:06 DISPATCHER: job (8, 0, 1) finished\n",
      "12:46:06 DISPATCHER: register_result: lock acquired\n",
      "12:46:06 DISPATCHER: job (8, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:46:06 job_id: (8, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.003605937704175232, 'num_conv_layers': 3, 'num_filters_1': 18, 'optimizer': 'SGD', 'num_filters_2': 6, 'num_filters_3': 16, 'sgd_momentum': 0.4855462285209849}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.0703125, 'info': {'test_accuracy': 0.9207, 'train_accuracy': 0.9375, 'valid_accuracy': 0.9296875, 'model': 'Sequential(\\n  (0): Conv2d(1, 18, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(18, 6, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=5776, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:46:06 job_callback for (8, 0, 1) started\n",
      "12:46:06 DISPATCHER: Trying to submit another job.\n",
      "12:46:06 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:46:06 job_callback for (8, 0, 1) got condition\n",
      "12:46:06 HBMASTER: Trying to run another job!\n",
      "12:46:06 job_callback for (8, 0, 1) finished\n",
      "12:46:06 start sampling a new configuration.\n",
      "12:46:06 best_vector: [0.8999802174742225, 0.1924907823833286, 0.6620368898705524, 0, 0.8103353138519881, 0.7406429699138737, 0.6697291266709328], 0.00023447755520775772, 2475.4505844650184, 0.5804376010829725\n",
      "12:46:06 done sampling a new configuration.\n",
      "12:46:06 HBMASTER: schedule new run for iteration 8\n",
      "12:46:06 HBMASTER: trying submitting job (8, 0, 2) to dispatcher\n",
      "12:46:06 HBMASTER: submitting job (8, 0, 2) to dispatcher\n",
      "12:46:06 DISPATCHER: trying to submit job (8, 0, 2)\n",
      "12:46:06 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:46:06 HBMASTER: job (8, 0, 2) submitted to dispatcher\n",
      "12:46:06 DISPATCHER: Trying to submit another job.\n",
      "12:46:06 DISPATCHER: starting job (8, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:46:06 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:46:06 DISPATCHER: job (8, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:46:06 WORKER: start processing job (8, 0, 2)\n",
      "12:46:06 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:46:06 WORKER: args: ()\n",
      "12:46:06 WORKER: kwargs: {'config': {'lr': 0.2511200012451207, 'num_conv_layers': 1, 'num_filters_1': 23, 'optimizer': 'Adam'}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:46:30 DISPATCHER: Starting worker discovery\n",
      "12:46:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:46:30 DISPATCHER: Finished worker discovery\n",
      "12:46:57 WORKER: done with job (8, 0, 2), trying to register it.\n",
      "12:46:57 WORKER: registered result for job (8, 0, 2) with dispatcher\n",
      "12:46:57 DISPATCHER: job (8, 0, 2) finished\n",
      "12:46:57 DISPATCHER: register_result: lock acquired\n",
      "12:46:57 DISPATCHER: job (8, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:46:57 job_id: (8, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.2511200012451207, 'num_conv_layers': 1, 'num_filters_1': 23, 'optimizer': 'Adam'}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.88671875, 'info': {'test_accuracy': 0.1135, 'train_accuracy': 0.111328125, 'valid_accuracy': 0.11328125, 'model': 'Sequential(\\n  (0): Conv2d(1, 23, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=14375, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:46:57 job_callback for (8, 0, 2) started\n",
      "12:46:57 DISPATCHER: Trying to submit another job.\n",
      "12:46:57 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:46:57 job_callback for (8, 0, 2) got condition\n",
      "12:46:57 HBMASTER: Trying to run another job!\n",
      "12:46:57 job_callback for (8, 0, 2) finished\n",
      "12:46:57 start sampling a new configuration.\n",
      "12:46:57 done sampling a new configuration.\n",
      "12:46:57 HBMASTER: schedule new run for iteration 9\n",
      "12:46:57 HBMASTER: trying submitting job (9, 0, 0) to dispatcher\n",
      "12:46:57 HBMASTER: submitting job (9, 0, 0) to dispatcher\n",
      "12:46:57 DISPATCHER: trying to submit job (9, 0, 0)\n",
      "12:46:57 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:46:57 HBMASTER: job (9, 0, 0) submitted to dispatcher\n",
      "12:46:57 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:46:57 DISPATCHER: Trying to submit another job.\n",
      "12:46:57 DISPATCHER: starting job (9, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:46:57 DISPATCHER: job (9, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:46:57 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:46:57 WORKER: start processing job (9, 0, 0)\n",
      "12:46:57 WORKER: args: ()\n",
      "12:46:57 WORKER: kwargs: {'config': {'lr': 0.06309489619956467, 'num_conv_layers': 3, 'num_filters_1': 19, 'optimizer': 'Adam', 'num_filters_2': 10, 'num_filters_3': 8}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:47:19 WORKER: done with job (9, 0, 0), trying to register it.\n",
      "12:47:19 WORKER: registered result for job (9, 0, 0) with dispatcher\n",
      "12:47:19 DISPATCHER: job (9, 0, 0) finished\n",
      "12:47:19 DISPATCHER: register_result: lock acquired\n",
      "12:47:19 DISPATCHER: job (9, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:47:19 job_id: (9, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.06309489619956467, 'num_conv_layers': 3, 'num_filters_1': 19, 'optimizer': 'Adam', 'num_filters_2': 10, 'num_filters_3': 8}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.916015625, 'info': {'test_accuracy': 0.0974, 'train_accuracy': 0.093505859375, 'valid_accuracy': 0.083984375, 'model': 'Sequential(\\n  (0): Conv2d(1, 19, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(19, 10, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=2888, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:47:19 job_callback for (9, 0, 0) started\n",
      "12:47:19 DISPATCHER: Trying to submit another job.\n",
      "12:47:19 job_callback for (9, 0, 0) got condition\n",
      "12:47:19 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:47:19 HBMASTER: Trying to run another job!\n",
      "12:47:19 job_callback for (9, 0, 0) finished\n",
      "12:47:19 start sampling a new configuration.\n",
      "12:47:19 done sampling a new configuration.\n",
      "12:47:19 HBMASTER: schedule new run for iteration 9\n",
      "12:47:19 HBMASTER: trying submitting job (9, 0, 1) to dispatcher\n",
      "12:47:19 HBMASTER: submitting job (9, 0, 1) to dispatcher\n",
      "12:47:19 DISPATCHER: trying to submit job (9, 0, 1)\n",
      "12:47:19 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:47:19 HBMASTER: job (9, 0, 1) submitted to dispatcher\n",
      "12:47:19 DISPATCHER: Trying to submit another job.\n",
      "12:47:19 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:47:19 DISPATCHER: starting job (9, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:47:19 DISPATCHER: job (9, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:47:19 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:47:19 WORKER: start processing job (9, 0, 1)\n",
      "12:47:19 WORKER: args: ()\n",
      "12:47:19 WORKER: kwargs: {'config': {'lr': 0.007004378147490428, 'num_conv_layers': 1, 'num_filters_1': 6, 'optimizer': 'SGD', 'sgd_momentum': 0.780660189394741}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:47:25 WORKER: done with job (9, 0, 1), trying to register it.\n",
      "12:47:25 WORKER: registered result for job (9, 0, 1) with dispatcher\n",
      "12:47:25 DISPATCHER: job (9, 0, 1) finished\n",
      "12:47:25 DISPATCHER: register_result: lock acquired\n",
      "12:47:25 DISPATCHER: job (9, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:47:25 job_id: (9, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.007004378147490428, 'num_conv_layers': 1, 'num_filters_1': 6, 'optimizer': 'SGD', 'sgd_momentum': 0.780660189394741}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.119140625, 'info': {'test_accuracy': 0.8568, 'train_accuracy': 0.874755859375, 'valid_accuracy': 0.880859375, 'model': 'Sequential(\\n  (0): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=3750, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:47:25 job_callback for (9, 0, 1) started\n",
      "12:47:25 DISPATCHER: Trying to submit another job.\n",
      "12:47:25 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:47:25 job_callback for (9, 0, 1) got condition\n",
      "12:47:25 HBMASTER: Trying to run another job!\n",
      "12:47:25 job_callback for (9, 0, 1) finished\n",
      "12:47:25 start sampling a new configuration.\n",
      "12:47:25 done sampling a new configuration.\n",
      "12:47:25 HBMASTER: schedule new run for iteration 9\n",
      "12:47:25 HBMASTER: trying submitting job (9, 0, 2) to dispatcher\n",
      "12:47:25 HBMASTER: submitting job (9, 0, 2) to dispatcher\n",
      "12:47:25 DISPATCHER: trying to submit job (9, 0, 2)\n",
      "12:47:25 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:47:25 HBMASTER: job (9, 0, 2) submitted to dispatcher\n",
      "12:47:25 DISPATCHER: Trying to submit another job.\n",
      "12:47:25 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:47:25 DISPATCHER: starting job (9, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:47:25 DISPATCHER: job (9, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:47:25 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:47:25 WORKER: start processing job (9, 0, 2)\n",
      "12:47:25 WORKER: args: ()\n",
      "12:47:25 WORKER: kwargs: {'config': {'lr': 0.8649863224719311, 'num_conv_layers': 2, 'num_filters_1': 31, 'optimizer': 'Adam', 'num_filters_2': 28}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:47:30 DISPATCHER: Starting worker discovery\n",
      "12:47:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:47:30 DISPATCHER: Finished worker discovery\n",
      "12:47:52 WORKER: done with job (9, 0, 2), trying to register it.\n",
      "12:47:52 WORKER: registered result for job (9, 0, 2) with dispatcher\n",
      "12:47:52 DISPATCHER: job (9, 0, 2) finished\n",
      "12:47:52 DISPATCHER: register_result: lock acquired\n",
      "12:47:52 DISPATCHER: job (9, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:47:52 job_id: (9, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.8649863224719311, 'num_conv_layers': 2, 'num_filters_1': 31, 'optimizer': 'Adam', 'num_filters_2': 28}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.916015625, 'info': {'test_accuracy': 0.0974, 'train_accuracy': 0.093505859375, 'valid_accuracy': 0.083984375, 'model': 'Sequential(\\n  (0): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(31, 28, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=13552, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:47:52 job_callback for (9, 0, 2) started\n",
      "12:47:52 DISPATCHER: Trying to submit another job.\n",
      "12:47:52 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:47:52 job_callback for (9, 0, 2) got condition\n",
      "12:47:52 HBMASTER: Trying to run another job!\n",
      "12:47:52 job_callback for (9, 0, 2) finished\n",
      "12:47:52 start sampling a new configuration.\n",
      "12:47:52 best_vector: [0.4282936711784417, 0.282048600848136, 0.61073157627768, 0, 0.37385611592293566, 0.7404242284186462, 0.5054186287136787], 0.00024891279938767813, 566.4285277738868, 0.14099131050123936\n",
      "12:47:52 done sampling a new configuration.\n",
      "12:47:52 HBMASTER: schedule new run for iteration 9\n",
      "12:47:52 HBMASTER: trying submitting job (9, 0, 3) to dispatcher\n",
      "12:47:52 HBMASTER: submitting job (9, 0, 3) to dispatcher\n",
      "12:47:52 DISPATCHER: trying to submit job (9, 0, 3)\n",
      "12:47:52 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:47:52 HBMASTER: job (9, 0, 3) submitted to dispatcher\n",
      "12:47:52 DISPATCHER: Trying to submit another job.\n",
      "12:47:52 DISPATCHER: starting job (9, 0, 3) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:47:52 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:47:52 DISPATCHER: job (9, 0, 3) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:47:53 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:47:52 WORKER: start processing job (9, 0, 3)\n",
      "12:47:53 WORKER: args: ()\n",
      "12:47:53 WORKER: kwargs: {'config': {'lr': 0.00037133170105061944, 'num_conv_layers': 1, 'num_filters_1': 21, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:48:03 WORKER: done with job (9, 0, 3), trying to register it.\n",
      "12:48:03 WORKER: registered result for job (9, 0, 3) with dispatcher\n",
      "12:48:03 DISPATCHER: job (9, 0, 3) finished\n",
      "12:48:03 DISPATCHER: register_result: lock acquired\n",
      "12:48:03 DISPATCHER: job (9, 0, 3) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:48:03 job_id: (9, 0, 3)\n",
      "kwargs: {'config': {'lr': 0.00037133170105061944, 'num_conv_layers': 1, 'num_filters_1': 21, 'optimizer': 'Adam'}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.08984375, 'info': {'test_accuracy': 0.8912, 'train_accuracy': 0.90771484375, 'valid_accuracy': 0.91015625, 'model': 'Sequential(\\n  (0): Conv2d(1, 21, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=13125, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:48:03 job_callback for (9, 0, 3) started\n",
      "12:48:03 DISPATCHER: Trying to submit another job.\n",
      "12:48:03 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:48:03 job_callback for (9, 0, 3) got condition\n",
      "12:48:03 HBMASTER: Trying to run another job!\n",
      "12:48:03 job_callback for (9, 0, 3) finished\n",
      "12:48:03 start sampling a new configuration.\n",
      "12:48:04 best_vector: [0.8839379350754915, 0.43684865213876267, 0.9756130015022726, 0, 0.6217317148193118, 0.740511529769152, 0.8730173405724965], 0.00011128812277615508, 181.40506782353094, 0.02018822946016185\n",
      "12:48:04 done sampling a new configuration.\n",
      "12:48:04 HBMASTER: schedule new run for iteration 9\n",
      "12:48:04 HBMASTER: trying submitting job (9, 0, 4) to dispatcher\n",
      "12:48:04 HBMASTER: submitting job (9, 0, 4) to dispatcher\n",
      "12:48:04 DISPATCHER: trying to submit job (9, 0, 4)\n",
      "12:48:04 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:48:04 HBMASTER: job (9, 0, 4) submitted to dispatcher\n",
      "12:48:04 DISPATCHER: Trying to submit another job.\n",
      "12:48:04 DISPATCHER: starting job (9, 0, 4) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:48:04 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:48:04 DISPATCHER: job (9, 0, 4) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:48:04 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:48:04 WORKER: start processing job (9, 0, 4)\n",
      "12:48:04 WORKER: args: ()\n",
      "12:48:04 WORKER: kwargs: {'config': {'lr': 0.20119983047154077, 'num_conv_layers': 2, 'num_filters_1': 32, 'optimizer': 'Adam', 'num_filters_2': 22}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:48:29 WORKER: done with job (9, 0, 4), trying to register it.\n",
      "12:48:29 WORKER: registered result for job (9, 0, 4) with dispatcher\n",
      "12:48:29 DISPATCHER: job (9, 0, 4) finished\n",
      "12:48:29 DISPATCHER: register_result: lock acquired\n",
      "12:48:29 DISPATCHER: job (9, 0, 4) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:48:29 job_id: (9, 0, 4)\n",
      "kwargs: {'config': {'lr': 0.20119983047154077, 'num_conv_layers': 2, 'num_filters_1': 32, 'optimizer': 'Adam', 'num_filters_2': 22}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.875, 'info': {'test_accuracy': 0.1028, 'train_accuracy': 0.108154296875, 'valid_accuracy': 0.125, 'model': 'Sequential(\\n  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(32, 22, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=10648, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:48:29 job_callback for (9, 0, 4) started\n",
      "12:48:29 DISPATCHER: Trying to submit another job.\n",
      "12:48:29 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:48:29 job_callback for (9, 0, 4) got condition\n",
      "12:48:29 HBMASTER: Trying to run another job!\n",
      "12:48:29 job_callback for (9, 0, 4) finished\n",
      "12:48:29 start sampling a new configuration.\n",
      "12:48:29 done sampling a new configuration.\n",
      "12:48:29 HBMASTER: schedule new run for iteration 9\n",
      "12:48:29 HBMASTER: trying submitting job (9, 0, 5) to dispatcher\n",
      "12:48:29 HBMASTER: submitting job (9, 0, 5) to dispatcher\n",
      "12:48:29 DISPATCHER: trying to submit job (9, 0, 5)\n",
      "12:48:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:48:29 HBMASTER: job (9, 0, 5) submitted to dispatcher\n",
      "12:48:29 DISPATCHER: Trying to submit another job.\n",
      "12:48:29 DISPATCHER: starting job (9, 0, 5) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:48:29 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:48:29 DISPATCHER: job (9, 0, 5) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:48:29 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:48:29 WORKER: start processing job (9, 0, 5)\n",
      "12:48:29 WORKER: args: ()\n",
      "12:48:29 WORKER: kwargs: {'config': {'lr': 0.28970048676684795, 'num_conv_layers': 3, 'num_filters_1': 21, 'optimizer': 'SGD', 'num_filters_2': 26, 'num_filters_3': 10, 'sgd_momentum': 0.3256367371449705}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:48:30 DISPATCHER: Starting worker discovery\n",
      "12:48:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:48:30 DISPATCHER: Finished worker discovery\n",
      "12:48:54 WORKER: done with job (9, 0, 5), trying to register it.\n",
      "12:48:54 WORKER: registered result for job (9, 0, 5) with dispatcher\n",
      "12:48:54 DISPATCHER: job (9, 0, 5) finished\n",
      "12:48:54 DISPATCHER: register_result: lock acquired\n",
      "12:48:54 DISPATCHER: job (9, 0, 5) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:48:54 job_id: (9, 0, 5)\n",
      "kwargs: {'config': {'lr': 0.28970048676684795, 'num_conv_layers': 3, 'num_filters_1': 21, 'optimizer': 'SGD', 'num_filters_2': 26, 'num_filters_3': 10, 'sgd_momentum': 0.3256367371449705}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.875, 'info': {'test_accuracy': 0.1028, 'train_accuracy': 0.108154296875, 'valid_accuracy': 0.125, 'model': 'Sequential(\\n  (0): Conv2d(1, 21, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(21, 26, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(26, 10, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=3610, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:48:54 job_callback for (9, 0, 5) started\n",
      "12:48:54 DISPATCHER: Trying to submit another job.\n",
      "12:48:54 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:48:54 job_callback for (9, 0, 5) got condition\n",
      "12:48:54 HBMASTER: Trying to run another job!\n",
      "12:48:54 job_callback for (9, 0, 5) finished\n",
      "12:48:54 start sampling a new configuration.\n",
      "12:48:54 best_vector: [0.9236991282522296, 0.6956179953025261, 0.14667537256370644, 1, 0.7518757446806256, 0.7415456493383173, 0.673169609352363], 0.00021316001689460537, 208.76832834062535, 0.04450106039614622\n",
      "12:48:54 done sampling a new configuration.\n",
      "12:48:54 HBMASTER: schedule new run for iteration 9\n",
      "12:48:54 HBMASTER: trying submitting job (9, 0, 6) to dispatcher\n",
      "12:48:54 HBMASTER: submitting job (9, 0, 6) to dispatcher\n",
      "12:48:54 DISPATCHER: trying to submit job (9, 0, 6)\n",
      "12:48:54 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:48:54 HBMASTER: job (9, 0, 6) submitted to dispatcher\n",
      "12:48:54 DISPATCHER: Trying to submit another job.\n",
      "12:48:54 DISPATCHER: starting job (9, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:48:54 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:48:54 DISPATCHER: job (9, 0, 6) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:48:54 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:48:54 WORKER: start processing job (9, 0, 6)\n",
      "12:48:54 WORKER: args: ()\n",
      "12:48:54 WORKER: kwargs: {'config': {'lr': 0.3484935700838901, 'num_conv_layers': 3, 'num_filters_1': 8, 'optimizer': 'SGD', 'num_filters_2': 25, 'num_filters_3': 25, 'sgd_momentum': 0.6664379132588394}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:49:14 WORKER: done with job (9, 0, 6), trying to register it.\n",
      "12:49:14 WORKER: registered result for job (9, 0, 6) with dispatcher\n",
      "12:49:14 DISPATCHER: job (9, 0, 6) finished\n",
      "12:49:14 DISPATCHER: register_result: lock acquired\n",
      "12:49:14 DISPATCHER: job (9, 0, 6) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:49:14 job_id: (9, 0, 6)\n",
      "kwargs: {'config': {'lr': 0.3484935700838901, 'num_conv_layers': 3, 'num_filters_1': 8, 'optimizer': 'SGD', 'num_filters_2': 25, 'num_filters_3': 25, 'sgd_momentum': 0.6664379132588394}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.88671875, 'info': {'test_accuracy': 0.1135, 'train_accuracy': 0.111328125, 'valid_accuracy': 0.11328125, 'model': 'Sequential(\\n  (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(8, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=9025, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:49:14 job_callback for (9, 0, 6) started\n",
      "12:49:14 DISPATCHER: Trying to submit another job.\n",
      "12:49:14 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:49:14 job_callback for (9, 0, 6) got condition\n",
      "12:49:14 HBMASTER: Trying to run another job!\n",
      "12:49:14 job_callback for (9, 0, 6) finished\n",
      "12:49:14 start sampling a new configuration.\n",
      "12:49:14 best_vector: [0.6435961001124738, 0.049619293852794136, 0.8919033509874783, 1, 0.7356001737323965, 0.7420241675420504, 0.6878813813445361], 2.0541520149690455e-05, 3265.8070452507136, 0.06708464122501859\n",
      "12:49:14 done sampling a new configuration.\n",
      "12:49:14 HBMASTER: schedule new run for iteration 9\n",
      "12:49:14 HBMASTER: trying submitting job (9, 0, 7) to dispatcher\n",
      "12:49:14 HBMASTER: submitting job (9, 0, 7) to dispatcher\n",
      "12:49:14 DISPATCHER: trying to submit job (9, 0, 7)\n",
      "12:49:14 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:49:14 HBMASTER: job (9, 0, 7) submitted to dispatcher\n",
      "12:49:14 DISPATCHER: Trying to submit another job.\n",
      "12:49:14 DISPATCHER: starting job (9, 0, 7) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:49:14 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:49:14 DISPATCHER: job (9, 0, 7) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:49:14 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:49:14 WORKER: start processing job (9, 0, 7)\n",
      "12:49:14 WORKER: args: ()\n",
      "12:49:14 WORKER: kwargs: {'config': {'lr': 0.007270706283115313, 'num_conv_layers': 1, 'num_filters_1': 29, 'optimizer': 'SGD', 'sgd_momentum': 0.6810025675310908}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:49:26 WORKER: done with job (9, 0, 7), trying to register it.\n",
      "12:49:26 WORKER: registered result for job (9, 0, 7) with dispatcher\n",
      "12:49:26 DISPATCHER: job (9, 0, 7) finished\n",
      "12:49:26 DISPATCHER: register_result: lock acquired\n",
      "12:49:26 DISPATCHER: job (9, 0, 7) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:49:26 job_id: (9, 0, 7)\n",
      "kwargs: {'config': {'lr': 0.007270706283115313, 'num_conv_layers': 1, 'num_filters_1': 29, 'optimizer': 'SGD', 'sgd_momentum': 0.6810025675310908}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.099609375, 'info': {'test_accuracy': 0.8884, 'train_accuracy': 0.905517578125, 'valid_accuracy': 0.900390625, 'model': 'Sequential(\\n  (0): Conv2d(1, 29, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=18125, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:49:26 job_callback for (9, 0, 7) started\n",
      "12:49:26 DISPATCHER: Trying to submit another job.\n",
      "12:49:26 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:49:26 job_callback for (9, 0, 7) got condition\n",
      "12:49:26 HBMASTER: Trying to run another job!\n",
      "12:49:26 job_callback for (9, 0, 7) finished\n",
      "12:49:26 start sampling a new configuration.\n",
      "12:49:26 best_vector: [0.6835554064138027, 0.21865531549651085, 0.7868975083837914, 1, 0.8812415937549606, 0.7425247718367045, 0.8137734668208222], 0.00018017026501271067, 552.5803719913758, 0.09955855206250844\n",
      "12:49:26 done sampling a new configuration.\n",
      "12:49:26 HBMASTER: schedule new run for iteration 9\n",
      "12:49:26 HBMASTER: trying submitting job (9, 0, 8) to dispatcher\n",
      "12:49:26 HBMASTER: submitting job (9, 0, 8) to dispatcher\n",
      "12:49:26 DISPATCHER: trying to submit job (9, 0, 8)\n",
      "12:49:26 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:49:26 HBMASTER: job (9, 0, 8) submitted to dispatcher\n",
      "12:49:26 DISPATCHER: Trying to submit another job.\n",
      "12:49:26 DISPATCHER: starting job (9, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:49:26 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:49:26 DISPATCHER: job (9, 0, 8) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:49:26 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:49:26 WORKER: start processing job (9, 0, 8)\n",
      "12:49:26 WORKER: args: ()\n",
      "12:49:26 WORKER: kwargs: {'config': {'lr': 0.012627937928299122, 'num_conv_layers': 1, 'num_filters_1': 26, 'optimizer': 'SGD', 'sgd_momentum': 0.805635732152614}, 'budget': 1.0, 'working_directory': '.'}\n",
      "12:49:30 DISPATCHER: Starting worker discovery\n",
      "12:49:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:49:30 DISPATCHER: Finished worker discovery\n",
      "12:49:36 WORKER: done with job (9, 0, 8), trying to register it.\n",
      "12:49:36 WORKER: registered result for job (9, 0, 8) with dispatcher\n",
      "12:49:36 DISPATCHER: job (9, 0, 8) finished\n",
      "12:49:36 DISPATCHER: register_result: lock acquired\n",
      "12:49:36 DISPATCHER: job (9, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:49:36 job_id: (9, 0, 8)\n",
      "kwargs: {'config': {'lr': 0.012627937928299122, 'num_conv_layers': 1, 'num_filters_1': 26, 'optimizer': 'SGD', 'sgd_momentum': 0.805635732152614}, 'budget': 1.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.083984375, 'info': {'test_accuracy': 0.8995, 'train_accuracy': 0.91748046875, 'valid_accuracy': 0.916015625, 'model': 'Sequential(\\n  (0): Conv2d(1, 26, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=16250, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:49:36 job_callback for (9, 0, 8) started\n",
      "12:49:36 DISPATCHER: Trying to submit another job.\n",
      "12:49:36 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:49:36 job_callback for (9, 0, 8) got condition\n",
      "12:49:36 HBMASTER: Trying to run another job!\n",
      "12:49:36 job_callback for (9, 0, 8) finished\n",
      "12:49:36 ITERATION: Advancing config (9, 0, 3) to next budget 3.000000\n",
      "12:49:36 ITERATION: Advancing config (9, 0, 7) to next budget 3.000000\n",
      "12:49:36 ITERATION: Advancing config (9, 0, 8) to next budget 3.000000\n",
      "12:49:36 HBMASTER: schedule new run for iteration 9\n",
      "12:49:36 HBMASTER: trying submitting job (9, 0, 3) to dispatcher\n",
      "12:49:36 HBMASTER: submitting job (9, 0, 3) to dispatcher\n",
      "12:49:36 DISPATCHER: trying to submit job (9, 0, 3)\n",
      "12:49:36 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:49:36 HBMASTER: job (9, 0, 3) submitted to dispatcher\n",
      "12:49:36 DISPATCHER: Trying to submit another job.\n",
      "12:49:36 DISPATCHER: starting job (9, 0, 3) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:49:36 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:49:36 DISPATCHER: job (9, 0, 3) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:49:36 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:49:36 WORKER: start processing job (9, 0, 3)\n",
      "12:49:36 WORKER: args: ()\n",
      "12:49:36 WORKER: kwargs: {'config': {'lr': 0.00037133170105061944, 'num_conv_layers': 1, 'num_filters_1': 21, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:49:51 WORKER: done with job (9, 0, 3), trying to register it.\n",
      "12:49:51 WORKER: registered result for job (9, 0, 3) with dispatcher\n",
      "12:49:51 DISPATCHER: job (9, 0, 3) finished\n",
      "12:49:51 DISPATCHER: register_result: lock acquired\n",
      "12:49:51 DISPATCHER: job (9, 0, 3) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:49:51 job_id: (9, 0, 3)\n",
      "kwargs: {'config': {'lr': 0.00037133170105061944, 'num_conv_layers': 1, 'num_filters_1': 21, 'optimizer': 'Adam'}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.07421875, 'info': {'test_accuracy': 0.9149, 'train_accuracy': 0.949462890625, 'valid_accuracy': 0.92578125, 'model': 'Sequential(\\n  (0): Conv2d(1, 21, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=13125, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:49:51 job_callback for (9, 0, 3) started\n",
      "12:49:51 DISPATCHER: Trying to submit another job.\n",
      "12:49:51 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:49:51 job_callback for (9, 0, 3) got condition\n",
      "12:49:51 done building a new model for budget 3.000000 based on 8/16 split\n",
      "Best loss for this budget:0.041016\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:49:51 HBMASTER: Trying to run another job!\n",
      "12:49:51 job_callback for (9, 0, 3) finished\n",
      "12:49:51 HBMASTER: schedule new run for iteration 9\n",
      "12:49:51 HBMASTER: trying submitting job (9, 0, 7) to dispatcher\n",
      "12:49:51 HBMASTER: submitting job (9, 0, 7) to dispatcher\n",
      "12:49:51 DISPATCHER: trying to submit job (9, 0, 7)\n",
      "12:49:51 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:49:51 HBMASTER: job (9, 0, 7) submitted to dispatcher\n",
      "12:49:52 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:49:51 DISPATCHER: Trying to submit another job.\n",
      "12:49:52 DISPATCHER: starting job (9, 0, 7) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:49:52 DISPATCHER: job (9, 0, 7) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:49:52 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:49:52 WORKER: start processing job (9, 0, 7)\n",
      "12:49:52 WORKER: args: ()\n",
      "12:49:52 WORKER: kwargs: {'config': {'lr': 0.007270706283115313, 'num_conv_layers': 1, 'num_filters_1': 29, 'optimizer': 'SGD', 'sgd_momentum': 0.6810025675310908}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:50:11 WORKER: done with job (9, 0, 7), trying to register it.\n",
      "12:50:11 WORKER: registered result for job (9, 0, 7) with dispatcher\n",
      "12:50:11 DISPATCHER: job (9, 0, 7) finished\n",
      "12:50:11 DISPATCHER: register_result: lock acquired\n",
      "12:50:11 DISPATCHER: job (9, 0, 7) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:50:11 job_id: (9, 0, 7)\n",
      "kwargs: {'config': {'lr': 0.007270706283115313, 'num_conv_layers': 1, 'num_filters_1': 29, 'optimizer': 'SGD', 'sgd_momentum': 0.6810025675310908}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.09375, 'info': {'test_accuracy': 0.9007, 'train_accuracy': 0.927001953125, 'valid_accuracy': 0.90625, 'model': 'Sequential(\\n  (0): Conv2d(1, 29, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=18125, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:50:11 job_callback for (9, 0, 7) started\n",
      "12:50:11 DISPATCHER: Trying to submit another job.\n",
      "12:50:11 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:50:11 job_callback for (9, 0, 7) got condition\n",
      "12:50:11 done building a new model for budget 3.000000 based on 8/17 split\n",
      "Best loss for this budget:0.041016\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:50:11 HBMASTER: Trying to run another job!\n",
      "12:50:11 job_callback for (9, 0, 7) finished\n",
      "12:50:11 HBMASTER: schedule new run for iteration 9\n",
      "12:50:11 HBMASTER: trying submitting job (9, 0, 8) to dispatcher\n",
      "12:50:11 HBMASTER: submitting job (9, 0, 8) to dispatcher\n",
      "12:50:11 DISPATCHER: trying to submit job (9, 0, 8)\n",
      "12:50:11 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:50:11 HBMASTER: job (9, 0, 8) submitted to dispatcher\n",
      "12:50:11 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:50:11 DISPATCHER: Trying to submit another job.\n",
      "12:50:11 DISPATCHER: starting job (9, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:50:11 DISPATCHER: job (9, 0, 8) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:50:11 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:50:11 WORKER: start processing job (9, 0, 8)\n",
      "12:50:11 WORKER: args: ()\n",
      "12:50:11 WORKER: kwargs: {'config': {'lr': 0.012627937928299122, 'num_conv_layers': 1, 'num_filters_1': 26, 'optimizer': 'SGD', 'sgd_momentum': 0.805635732152614}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:50:28 WORKER: done with job (9, 0, 8), trying to register it.\n",
      "12:50:28 WORKER: registered result for job (9, 0, 8) with dispatcher\n",
      "12:50:28 DISPATCHER: job (9, 0, 8) finished\n",
      "12:50:28 DISPATCHER: register_result: lock acquired\n",
      "12:50:28 DISPATCHER: job (9, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:50:28 job_id: (9, 0, 8)\n",
      "kwargs: {'config': {'lr': 0.012627937928299122, 'num_conv_layers': 1, 'num_filters_1': 26, 'optimizer': 'SGD', 'sgd_momentum': 0.805635732152614}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.072265625, 'info': {'test_accuracy': 0.9213, 'train_accuracy': 0.947265625, 'valid_accuracy': 0.927734375, 'model': 'Sequential(\\n  (0): Conv2d(1, 26, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=16250, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:50:28 job_callback for (9, 0, 8) started\n",
      "12:50:28 DISPATCHER: Trying to submit another job.\n",
      "12:50:28 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:50:28 job_callback for (9, 0, 8) got condition\n",
      "12:50:29 done building a new model for budget 3.000000 based on 8/17 split\n",
      "Best loss for this budget:0.041016\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:50:29 HBMASTER: Trying to run another job!\n",
      "12:50:29 job_callback for (9, 0, 8) finished\n",
      "12:50:29 ITERATION: Advancing config (9, 0, 8) to next budget 9.000000\n",
      "12:50:29 HBMASTER: schedule new run for iteration 9\n",
      "12:50:29 HBMASTER: trying submitting job (9, 0, 8) to dispatcher\n",
      "12:50:29 HBMASTER: submitting job (9, 0, 8) to dispatcher\n",
      "12:50:29 DISPATCHER: trying to submit job (9, 0, 8)\n",
      "12:50:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:50:29 HBMASTER: job (9, 0, 8) submitted to dispatcher\n",
      "12:50:29 DISPATCHER: Trying to submit another job.\n",
      "12:50:29 DISPATCHER: starting job (9, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:50:29 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:50:29 DISPATCHER: job (9, 0, 8) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:50:29 WORKER: start processing job (9, 0, 8)\n",
      "12:50:29 WORKER: args: ()\n",
      "12:50:29 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:50:29 WORKER: kwargs: {'config': {'lr': 0.012627937928299122, 'num_conv_layers': 1, 'num_filters_1': 26, 'optimizer': 'SGD', 'sgd_momentum': 0.805635732152614}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:50:30 DISPATCHER: Starting worker discovery\n",
      "12:50:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:50:30 DISPATCHER: Finished worker discovery\n",
      "12:51:08 WORKER: done with job (9, 0, 8), trying to register it.\n",
      "12:51:08 WORKER: registered result for job (9, 0, 8) with dispatcher\n",
      "12:51:08 DISPATCHER: job (9, 0, 8) finished\n",
      "12:51:08 DISPATCHER: register_result: lock acquired\n",
      "12:51:08 DISPATCHER: job (9, 0, 8) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:51:08 job_id: (9, 0, 8)\n",
      "kwargs: {'config': {'lr': 0.012627937928299122, 'num_conv_layers': 1, 'num_filters_1': 26, 'optimizer': 'SGD', 'sgd_momentum': 0.805635732152614}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.044921875, 'info': {'test_accuracy': 0.9492, 'train_accuracy': 0.993408203125, 'valid_accuracy': 0.955078125, 'model': 'Sequential(\\n  (0): Conv2d(1, 26, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Flatten()\\n  (4): Linear(in_features=16250, out_features=10, bias=True)\\n  (5): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:51:08 job_callback for (9, 0, 8) started\n",
      "12:51:08 DISPATCHER: Trying to submit another job.\n",
      "12:51:08 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:51:08 job_callback for (9, 0, 8) got condition\n",
      "12:51:08 done building a new model for budget 9.000000 based on 8/13 split\n",
      "Best loss for this budget:0.031250\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:51:08 HBMASTER: Trying to run another job!\n",
      "12:51:08 job_callback for (9, 0, 8) finished\n",
      "12:51:08 start sampling a new configuration.\n",
      "12:51:08 best_vector: [0.7230625334750258, 0.7863586908692479, 0.2967177993401609, 1, 0.9489618919034579, 0.9820144947728098, 0.7032863978623469], 6.664197463122564e-09, 0.04562644935613513, 3.0406366805044587e-10\n",
      "12:51:08 done sampling a new configuration.\n",
      "12:51:08 HBMASTER: schedule new run for iteration 10\n",
      "12:51:08 HBMASTER: trying submitting job (10, 0, 0) to dispatcher\n",
      "12:51:08 HBMASTER: submitting job (10, 0, 0) to dispatcher\n",
      "12:51:08 DISPATCHER: trying to submit job (10, 0, 0)\n",
      "12:51:08 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:51:08 HBMASTER: job (10, 0, 0) submitted to dispatcher\n",
      "12:51:08 DISPATCHER: Trying to submit another job.\n",
      "12:51:08 DISPATCHER: starting job (10, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:51:08 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:51:08 DISPATCHER: job (10, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:51:08 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:51:08 WORKER: start processing job (10, 0, 0)\n",
      "12:51:08 WORKER: args: ()\n",
      "12:51:08 WORKER: kwargs: {'config': {'lr': 0.021795919781923134, 'num_conv_layers': 3, 'num_filters_1': 12, 'optimizer': 'SGD', 'num_filters_2': 31, 'num_filters_3': 32, 'sgd_momentum': 0.6962535338837235}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:51:30 DISPATCHER: Starting worker discovery\n",
      "12:51:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:51:30 DISPATCHER: Finished worker discovery\n",
      "12:52:01 WORKER: done with job (10, 0, 0), trying to register it.\n",
      "12:52:01 WORKER: registered result for job (10, 0, 0) with dispatcher\n",
      "12:52:01 DISPATCHER: job (10, 0, 0) finished\n",
      "12:52:01 DISPATCHER: register_result: lock acquired\n",
      "12:52:01 DISPATCHER: job (10, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:52:01 job_id: (10, 0, 0)\n",
      "kwargs: {'config': {'lr': 0.021795919781923134, 'num_conv_layers': 3, 'num_filters_1': 12, 'optimizer': 'SGD', 'num_filters_2': 31, 'num_filters_3': 32, 'sgd_momentum': 0.6962535338837235}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.05078125, 'info': {'test_accuracy': 0.9438, 'train_accuracy': 0.965576171875, 'valid_accuracy': 0.94921875, 'model': 'Sequential(\\n  (0): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(12, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=11552, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:52:01 job_callback for (10, 0, 0) started\n",
      "12:52:01 DISPATCHER: Trying to submit another job.\n",
      "12:52:01 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:52:01 job_callback for (10, 0, 0) got condition\n",
      "12:52:01 HBMASTER: Trying to run another job!\n",
      "12:52:01 job_callback for (10, 0, 0) finished\n",
      "12:52:01 start sampling a new configuration.\n",
      "12:52:01 best_vector: [0.3070754254875264, 0.8691471387838205, 0.9204031925503855, 0, 0.7019953309051722, 0.9765902918891239, 0.33647635101428314], 1.127707027405964e-09, 0.04483077738981729, 5.0555982706569354e-11\n",
      "12:52:01 done sampling a new configuration.\n",
      "12:52:01 HBMASTER: schedule new run for iteration 10\n",
      "12:52:01 HBMASTER: trying submitting job (10, 0, 1) to dispatcher\n",
      "12:52:01 HBMASTER: submitting job (10, 0, 1) to dispatcher\n",
      "12:52:01 DISPATCHER: trying to submit job (10, 0, 1)\n",
      "12:52:01 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:52:01 HBMASTER: job (10, 0, 1) submitted to dispatcher\n",
      "12:52:01 DISPATCHER: Trying to submit another job.\n",
      "12:52:01 DISPATCHER: starting job (10, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:52:01 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:52:01 DISPATCHER: job (10, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:52:01 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:52:01 WORKER: start processing job (10, 0, 1)\n",
      "12:52:01 WORKER: args: ()\n",
      "12:52:01 WORKER: kwargs: {'config': {'lr': 6.957489393315608e-05, 'num_conv_layers': 3, 'num_filters_1': 30, 'optimizer': 'Adam', 'num_filters_2': 24, 'num_filters_3': 32}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:52:30 DISPATCHER: Starting worker discovery\n",
      "12:52:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:52:30 DISPATCHER: Finished worker discovery\n",
      "12:53:10 WORKER: done with job (10, 0, 1), trying to register it.\n",
      "12:53:10 WORKER: registered result for job (10, 0, 1) with dispatcher\n",
      "12:53:10 DISPATCHER: job (10, 0, 1) finished\n",
      "12:53:10 DISPATCHER: register_result: lock acquired\n",
      "12:53:10 DISPATCHER: job (10, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:53:10 job_id: (10, 0, 1)\n",
      "kwargs: {'config': {'lr': 6.957489393315608e-05, 'num_conv_layers': 3, 'num_filters_1': 30, 'optimizer': 'Adam', 'num_filters_2': 24, 'num_filters_3': 32}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.095703125, 'info': {'test_accuracy': 0.9007, 'train_accuracy': 0.911865234375, 'valid_accuracy': 0.904296875, 'model': 'Sequential(\\n  (0): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(30, 24, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=11552, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:53:10 job_callback for (10, 0, 1) started\n",
      "12:53:10 DISPATCHER: Trying to submit another job.\n",
      "12:53:10 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:53:10 job_callback for (10, 0, 1) got condition\n",
      "12:53:10 HBMASTER: Trying to run another job!\n",
      "12:53:10 job_callback for (10, 0, 1) finished\n",
      "12:53:10 start sampling a new configuration.\n",
      "12:53:10 best_vector: [0.5998212302812115, 0.7197162033219577, 0.835684476596875, 0, 0.75957723829456, 0.9338655739824424, 0.5243424447618658], 1.298326744897958e-07, 0.14956760044024617, 1.941876158217832e-08\n",
      "12:53:10 done sampling a new configuration.\n",
      "12:53:10 HBMASTER: schedule new run for iteration 10\n",
      "12:53:10 HBMASTER: trying submitting job (10, 0, 2) to dispatcher\n",
      "12:53:10 HBMASTER: submitting job (10, 0, 2) to dispatcher\n",
      "12:53:10 DISPATCHER: trying to submit job (10, 0, 2)\n",
      "12:53:10 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:53:10 HBMASTER: job (10, 0, 2) submitted to dispatcher\n",
      "12:53:10 DISPATCHER: Trying to submit another job.\n",
      "12:53:10 DISPATCHER: starting job (10, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:53:10 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:53:10 DISPATCHER: job (10, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:53:10 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:53:10 WORKER: start processing job (10, 0, 2)\n",
      "12:53:10 WORKER: args: ()\n",
      "12:53:10 WORKER: kwargs: {'config': {'lr': 0.003971251406845232, 'num_conv_layers': 3, 'num_filters_1': 28, 'optimizer': 'Adam', 'num_filters_2': 26, 'num_filters_3': 31}, 'budget': 3.0, 'working_directory': '.'}\n",
      "12:53:30 DISPATCHER: Starting worker discovery\n",
      "12:53:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:53:30 DISPATCHER: Finished worker discovery\n",
      "12:54:15 WORKER: done with job (10, 0, 2), trying to register it.\n",
      "12:54:15 WORKER: registered result for job (10, 0, 2) with dispatcher\n",
      "12:54:15 DISPATCHER: job (10, 0, 2) finished\n",
      "12:54:15 DISPATCHER: register_result: lock acquired\n",
      "12:54:15 DISPATCHER: job (10, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:54:15 job_id: (10, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.003971251406845232, 'num_conv_layers': 3, 'num_filters_1': 28, 'optimizer': 'Adam', 'num_filters_2': 26, 'num_filters_3': 31}, 'budget': 3.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.03515625, 'info': {'test_accuracy': 0.9599, 'train_accuracy': 0.9814453125, 'valid_accuracy': 0.96484375, 'model': 'Sequential(\\n  (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(28, 26, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(26, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=11191, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:54:15 job_callback for (10, 0, 2) started\n",
      "12:54:15 DISPATCHER: Trying to submit another job.\n",
      "12:54:15 job_callback for (10, 0, 2) got condition\n",
      "12:54:15 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:54:15 HBMASTER: Trying to run another job!\n",
      "12:54:15 job_callback for (10, 0, 2) finished\n",
      "12:54:15 ITERATION: Advancing config (10, 0, 2) to next budget 9.000000\n",
      "12:54:15 HBMASTER: schedule new run for iteration 10\n",
      "12:54:15 HBMASTER: trying submitting job (10, 0, 2) to dispatcher\n",
      "12:54:15 HBMASTER: submitting job (10, 0, 2) to dispatcher\n",
      "12:54:15 DISPATCHER: trying to submit job (10, 0, 2)\n",
      "12:54:15 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:54:15 HBMASTER: job (10, 0, 2) submitted to dispatcher\n",
      "12:54:15 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:54:15 DISPATCHER: Trying to submit another job.\n",
      "12:54:15 DISPATCHER: starting job (10, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:54:15 DISPATCHER: job (10, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:54:15 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:54:15 WORKER: start processing job (10, 0, 2)\n",
      "12:54:15 WORKER: args: ()\n",
      "12:54:15 WORKER: kwargs: {'config': {'lr': 0.003971251406845232, 'num_conv_layers': 3, 'num_filters_1': 28, 'optimizer': 'Adam', 'num_filters_2': 26, 'num_filters_3': 31}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:54:30 DISPATCHER: Starting worker discovery\n",
      "12:54:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:54:30 DISPATCHER: Finished worker discovery\n",
      "12:55:30 DISPATCHER: Starting worker discovery\n",
      "12:55:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:55:30 DISPATCHER: Finished worker discovery\n",
      "12:56:30 DISPATCHER: Starting worker discovery\n",
      "12:56:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:56:30 DISPATCHER: Finished worker discovery\n",
      "12:56:51 WORKER: done with job (10, 0, 2), trying to register it.\n",
      "12:56:51 WORKER: registered result for job (10, 0, 2) with dispatcher\n",
      "12:56:51 DISPATCHER: job (10, 0, 2) finished\n",
      "12:56:51 DISPATCHER: register_result: lock acquired\n",
      "12:56:51 DISPATCHER: job (10, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:56:51 job_id: (10, 0, 2)\n",
      "kwargs: {'config': {'lr': 0.003971251406845232, 'num_conv_layers': 3, 'num_filters_1': 28, 'optimizer': 'Adam', 'num_filters_2': 26, 'num_filters_3': 31}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.037109375, 'info': {'test_accuracy': 0.97, 'train_accuracy': 0.992919921875, 'valid_accuracy': 0.962890625, 'model': 'Sequential(\\n  (0): Conv2d(1, 28, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(28, 26, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(26, 31, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=11191, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:56:51 job_callback for (10, 0, 2) started\n",
      "12:56:51 DISPATCHER: Trying to submit another job.\n",
      "12:56:51 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:56:51 job_callback for (10, 0, 2) got condition\n",
      "12:56:51 done building a new model for budget 9.000000 based on 8/14 split\n",
      "Best loss for this budget:0.031250\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:56:51 HBMASTER: Trying to run another job!\n",
      "12:56:51 job_callback for (10, 0, 2) finished\n",
      "12:56:51 start sampling a new configuration.\n",
      "12:56:51 done sampling a new configuration.\n",
      "12:56:51 HBMASTER: schedule new run for iteration 11\n",
      "12:56:51 HBMASTER: trying submitting job (11, 0, 0) to dispatcher\n",
      "12:56:51 HBMASTER: submitting job (11, 0, 0) to dispatcher\n",
      "12:56:51 DISPATCHER: trying to submit job (11, 0, 0)\n",
      "12:56:51 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:56:52 HBMASTER: job (11, 0, 0) submitted to dispatcher\n",
      "12:56:52 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:56:52 DISPATCHER: Trying to submit another job.\n",
      "12:56:52 DISPATCHER: starting job (11, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:56:52 DISPATCHER: job (11, 0, 0) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:56:52 WORKER: start processing job (11, 0, 0)\n",
      "12:56:52 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:56:52 WORKER: args: ()\n",
      "12:56:52 WORKER: kwargs: {'config': {'lr': 3.3784842654557494e-05, 'num_conv_layers': 2, 'num_filters_1': 7, 'optimizer': 'Adam', 'num_filters_2': 14}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:57:29 WORKER: done with job (11, 0, 0), trying to register it.\n",
      "12:57:29 WORKER: registered result for job (11, 0, 0) with dispatcher\n",
      "12:57:29 DISPATCHER: job (11, 0, 0) finished\n",
      "12:57:29 DISPATCHER: register_result: lock acquired\n",
      "12:57:29 DISPATCHER: job (11, 0, 0) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:57:29 job_id: (11, 0, 0)\n",
      "kwargs: {'config': {'lr': 3.3784842654557494e-05, 'num_conv_layers': 2, 'num_filters_1': 7, 'optimizer': 'Adam', 'num_filters_2': 14}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.111328125, 'info': {'test_accuracy': 0.8865, 'train_accuracy': 0.89599609375, 'valid_accuracy': 0.888671875, 'model': 'Sequential(\\n  (0): Conv2d(1, 7, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(7, 14, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=6776, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:57:29 job_callback for (11, 0, 0) started\n",
      "12:57:29 DISPATCHER: Trying to submit another job.\n",
      "12:57:29 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:57:29 job_callback for (11, 0, 0) got condition\n",
      "12:57:29 done building a new model for budget 9.000000 based on 8/15 split\n",
      "Best loss for this budget:0.031250\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:57:29 HBMASTER: Trying to run another job!\n",
      "12:57:29 job_callback for (11, 0, 0) finished\n",
      "12:57:29 start sampling a new configuration.\n",
      "12:57:29 best_vector: [0.4426346794261408, 0.6051560864017682, 0.557642650054914, 0, 0.77899276759051, 0.9553018304103701, 0.5318060891160874], 3.1349330468934517e-07, 4.586832735319964, 1.437941352252724e-06\n",
      "12:57:29 done sampling a new configuration.\n",
      "12:57:29 HBMASTER: schedule new run for iteration 11\n",
      "12:57:29 HBMASTER: trying submitting job (11, 0, 1) to dispatcher\n",
      "12:57:29 HBMASTER: submitting job (11, 0, 1) to dispatcher\n",
      "12:57:29 DISPATCHER: trying to submit job (11, 0, 1)\n",
      "12:57:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:57:29 HBMASTER: job (11, 0, 1) submitted to dispatcher\n",
      "12:57:29 DISPATCHER: Trying to submit another job.\n",
      "12:57:29 DISPATCHER: starting job (11, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:57:29 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:57:29 DISPATCHER: job (11, 0, 1) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:57:29 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:57:29 WORKER: start processing job (11, 0, 1)\n",
      "12:57:29 WORKER: args: ()\n",
      "12:57:29 WORKER: kwargs: {'config': {'lr': 0.0004526974795957226, 'num_conv_layers': 2, 'num_filters_1': 20, 'optimizer': 'Adam', 'num_filters_2': 26}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:57:30 DISPATCHER: Starting worker discovery\n",
      "12:57:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:57:30 DISPATCHER: Finished worker discovery\n",
      "12:58:30 DISPATCHER: Starting worker discovery\n",
      "12:58:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:58:30 DISPATCHER: Finished worker discovery\n",
      "12:58:53 WORKER: done with job (11, 0, 1), trying to register it.\n",
      "12:58:53 WORKER: registered result for job (11, 0, 1) with dispatcher\n",
      "12:58:53 DISPATCHER: job (11, 0, 1) finished\n",
      "12:58:53 DISPATCHER: register_result: lock acquired\n",
      "12:58:53 DISPATCHER: job (11, 0, 1) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "12:58:53 job_id: (11, 0, 1)\n",
      "kwargs: {'config': {'lr': 0.0004526974795957226, 'num_conv_layers': 2, 'num_filters_1': 20, 'optimizer': 'Adam', 'num_filters_2': 26}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.029296875, 'info': {'test_accuracy': 0.9674, 'train_accuracy': 0.99658203125, 'valid_accuracy': 0.970703125, 'model': 'Sequential(\\n  (0): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(20, 26, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Flatten()\\n  (7): Linear(in_features=12584, out_features=10, bias=True)\\n  (8): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "12:58:53 job_callback for (11, 0, 1) started\n",
      "12:58:53 DISPATCHER: Trying to submit another job.\n",
      "12:58:53 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "12:58:53 job_callback for (11, 0, 1) got condition\n",
      "12:58:53 done building a new model for budget 9.000000 based on 8/16 split\n",
      "Best loss for this budget:0.029297\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12:58:53 HBMASTER: Trying to run another job!\n",
      "12:58:53 job_callback for (11, 0, 1) finished\n",
      "12:58:53 start sampling a new configuration.\n",
      "12:58:53 done sampling a new configuration.\n",
      "12:58:53 HBMASTER: schedule new run for iteration 11\n",
      "12:58:53 HBMASTER: trying submitting job (11, 0, 2) to dispatcher\n",
      "12:58:53 HBMASTER: submitting job (11, 0, 2) to dispatcher\n",
      "12:58:53 DISPATCHER: trying to submit job (11, 0, 2)\n",
      "12:58:53 DISPATCHER: trying to notify the job_runner thread.\n",
      "12:58:53 HBMASTER: job (11, 0, 2) submitted to dispatcher\n",
      "12:58:53 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "12:58:53 DISPATCHER: Trying to submit another job.\n",
      "12:58:53 DISPATCHER: starting job (11, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:58:53 DISPATCHER: job (11, 0, 2) dispatched on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720\n",
      "12:58:53 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "12:58:53 WORKER: start processing job (11, 0, 2)\n",
      "12:58:53 WORKER: args: ()\n",
      "12:58:53 WORKER: kwargs: {'config': {'lr': 7.966198791234604e-05, 'num_conv_layers': 3, 'num_filters_1': 6, 'optimizer': 'SGD', 'num_filters_2': 32, 'num_filters_3': 22, 'sgd_momentum': 0.9834184909517262}, 'budget': 9.0, 'working_directory': '.'}\n",
      "12:59:30 DISPATCHER: Starting worker discovery\n",
      "12:59:30 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "12:59:30 DISPATCHER: Finished worker discovery\n",
      "13:00:29 WORKER: done with job (11, 0, 2), trying to register it.\n",
      "13:00:29 WORKER: registered result for job (11, 0, 2) with dispatcher\n",
      "13:00:29 DISPATCHER: job (11, 0, 2) finished\n",
      "13:00:29 DISPATCHER: register_result: lock acquired\n",
      "13:00:29 DISPATCHER: job (11, 0, 2) on hpbandster.run_bohb_run_1.worker.529c0b5055b4.65140272243406720 finished\n",
      "13:00:29 job_id: (11, 0, 2)\n",
      "kwargs: {'config': {'lr': 7.966198791234604e-05, 'num_conv_layers': 3, 'num_filters_1': 6, 'optimizer': 'SGD', 'num_filters_2': 32, 'num_filters_3': 22, 'sgd_momentum': 0.9834184909517262}, 'budget': 9.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.11328125, 'info': {'test_accuracy': 0.8714, 'train_accuracy': 0.89404296875, 'valid_accuracy': 0.88671875, 'model': 'Sequential(\\n  (0): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\\n  (1): ReLU(inplace)\\n  (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (3): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1))\\n  (4): ReLU(inplace)\\n  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (6): Conv2d(32, 22, kernel_size=(3, 3), stride=(1, 1))\\n  (7): ReLU(inplace)\\n  (8): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\\n  (9): Flatten()\\n  (10): Linear(in_features=7942, out_features=10, bias=True)\\n  (11): LogSoftmax()\\n)'}}\n",
      "exception: None\n",
      "\n",
      "13:00:29 job_callback for (11, 0, 2) started\n",
      "13:00:29 DISPATCHER: Trying to submit another job.\n",
      "13:00:29 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:00:29 job_callback for (11, 0, 2) got condition\n",
      "13:00:29 done building a new model for budget 9.000000 based on 8/17 split\n",
      "Best loss for this budget:0.029297\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13:00:29 HBMASTER: Trying to run another job!\n",
      "13:00:29 job_callback for (11, 0, 2) finished\n",
      "13:00:29 HBMASTER: shutdown initiated, shutdown_workers = True\n",
      "13:00:29 WORKER: shutting down now!\n",
      "13:00:30 DISPATCHER: Dispatcher shutting down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write result to file ./bohb_result.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:00:30 DISPATCHER: Trying to submit another job.\n",
      "13:00:30 DISPATCHER: job_runner shutting down\n",
      "13:00:30 DISPATCHER: discover_workers shutting down\n",
      "13:00:30 DISPATCHER: 'discover_worker' thread exited\n",
      "13:00:30 DISPATCHER: 'job_runner' thread exited\n",
      "13:00:30 DISPATCHER: shut down complete\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Start a nameserver\n",
    "    host = hpns.nic_name_to_host(nic_name)\n",
    "    ns = hpns.NameServer(run_id=run_id, host=host, port=port,\n",
    "                         working_directory=working_dir)\n",
    "    ns_host, ns_port = ns.start()\n",
    "\n",
    "    # Start local worker\n",
    "    w = PyTorchWorker(run_id=run_id, host=host, nameserver=ns_host,\n",
    "                      nameserver_port=ns_port, timeout=120)\n",
    "    w.run(background=True)\n",
    "\n",
    "    # Run an optimizer\n",
    "    bohb = BOHB(configspace=worker.get_configspace(),\n",
    "                run_id=run_id,\n",
    "                host=host,\n",
    "                nameserver=ns_host,\n",
    "                nameserver_port=ns_port,\n",
    "                min_budget=min_budget, max_budget=max_budget)\n",
    "    \n",
    "    result = bohb.run(n_iterations=n_bohb_iterations)\n",
    "    print(\"Write result to file {}\".format(result_file))\n",
    "    with open(result_file, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "finally:\n",
    "    bohb.shutdown(shutdown_workers=True)\n",
    "    ns.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0jOi0nDj6Bz"
   },
   "source": [
    "### Evaluate result\n",
    "\n",
    "The result object which we dumped to disk contains all the runs with the different configurations.\n",
    "Here we will analyse it further. The [HpBandSter Analysis Example](https://automl.github.io/HpBandSter/build/html/auto_examples/plot_example_6_analysis.html) is there, if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nt5Ymo1Hj6B0"
   },
   "outputs": [],
   "source": [
    "#  load a saved result object if necessary\n",
    "with open(result_file, 'rb') as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBqDTwMASjYt"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('bohb_result.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBTK8lAvj6B2"
   },
   "source": [
    "**Task:** Print the model of the best run, evaluated on the largest budget, with it's final validation error. *Tipp:* Have a look at the [HpBandSter Result Docs](https://automl.github.io/HpBandSter/build/html/core/result.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "M_aEoYX-j6B3",
    "outputId": "c271f4c1-c6ca-42a9-fe13-7366f731b1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best found configuration:\n",
      "{'lr': 0.0004526974795957226, 'num_conv_layers': 2, 'num_filters_1': 20, 'optimizer': 'Adam', 'num_filters_2': 26}\n",
      "It achieved accuracies of 0.970703 (validation) and 0.967400 (test).\n"
     ]
    }
   ],
   "source": [
    "# Code is taken taken from the example\n",
    "\n",
    "# START TODO ################\n",
    "\n",
    "# get all executed runs\n",
    "all_runs = result.get_all_runs()\n",
    "\n",
    "# get the 'dict' that translates config ids to the actual configurations\n",
    "id2conf = result.get_id2config_mapping()\n",
    "\n",
    "\n",
    "# Here is how you get he incumbent (best configuration)\n",
    "inc_id = result.get_incumbent_id()\n",
    "\n",
    "# let's grab the run on the highest budget\n",
    "inc_runs = result.get_runs_by_id(inc_id)\n",
    "inc_run = inc_runs[-1]\n",
    "\n",
    "# We have access to all information: the config, the loss observed during\n",
    "#optimization, and all the additional information\n",
    "inc_loss = inc_run.loss\n",
    "inc_config = id2conf[inc_id]['config']\n",
    "inc_test_loss = inc_run.info['test_accuracy']\n",
    "\n",
    "print('Best found configuration:')\n",
    "print(inc_config)\n",
    "print('It achieved accuracies of %f (validation) and %f (test).'%(1-inc_loss, inc_test_loss))\n",
    "\n",
    "\n",
    "# END TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bmqBD9_j6B4"
   },
   "source": [
    "We can gain deeper insight through plotting results. Thanks to the [HpBandSter Visualization Module](https://automl.github.io/HpBandSter/build/html/core/visualization.html) plotting is a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1t7_Z0R9j6B5"
   },
   "outputs": [],
   "source": [
    "import hpbandster.visualization as hpvis\n",
    "\n",
    "all_runs = result.get_all_runs()\n",
    "id2conf = result.get_id2config_mapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eoSUUiUvj6B7"
   },
   "source": [
    "First let's see, if we really can evaluate more configurations when makeing use of low budget runs.\n",
    "\n",
    "**Task:** Plot the finished runs over time. How many runs per minute did finish for the individual budgets (only approximately)? \n",
    "\n",
    "**Answer:**  Approximately, \n",
    "\n",
    "budget 1 = 1 runs per minute\n",
    "\n",
    "budget 3 = 0.7 runs per minute \n",
    "\n",
    "budget 9 = 0.3 runs per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "x3vkSXDzj6B7",
    "outputId": "37a7252f-fdab-43fa-a1aa-45cb6a635b82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.figure.Figure at 0x7f934b4dbeb8>,\n",
       " <matplotlib.axes._subplots.AxesSubplot at 0x7f934de182b0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEGCAYAAACNaZVuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkn9ERAMl+UdENFCSf0REA/0DleQufnJ+xn4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f934b4dbeb8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# START TODO ##################\n",
    "\n",
    "hpvis.finished_runs_over_time(all_runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VyrrtwzFj6B9"
   },
   "source": [
    "Evaluating configurations on lower budgets doesn't make sense - even if they are faster - if the performance ranking isn't consistent from low to high budget. This means, that the loss rankings for configurations should correlate. In simplified terms: The best configuration after one epoch should also be the best after nine epochs, the second best should stay the second best and so on.\n",
    "\n",
    "**Task:** Plot correlations of rankings across budgets. Are the correlations high enough?\n",
    "\n",
    "**Answer:**  Yes. There is a high correlation as we can understand from p values. The P values are less than or around 0.05, so the correlation is siginificant. Thus, the loss rankings are correlated between the confiigurations with budgets 9, 3 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "id": "fhViRqB1j6B-",
    "outputId": "b5daa5ed-ae05-4b9b-a1dd-8cded6e1cf29"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEHCAYAAAA6U1oSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f934dd61fd0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<matplotlib.figure.Figure at 0x7f934dd61fd0>,\n",
       " <matplotlib.axes._subplots.AxesSubplot at 0x7f934de18cc0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# START TODO ##################\n",
    "hpvis.correlation_across_budgets(result,show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXWtWk-Nj6B_"
   },
   "source": [
    "We usually assume, that training on a higher budget (number of epochs) and sampling more configurations can lead to better results. Let's check this.\n",
    "\n",
    "**Task:** Plot the losses over time. Do our assumptions hold true? Why?\n",
    "\n",
    "**Answer:**  For our case, high budget (orange dots) shows the best performance in the overall system. However, for some samples it shows very poor performance, so it also depends on different configuration variables as well. Additionally, sampling more will lead to the optimal solution in theory because of the law of large numbers; however in practice, since the system is stochastic, we can find the best solution even in the first iterations. We can also see it in our example, as our model performs best for the earlier configurations. So essentially it can said that sampling more increases the change of finding better solutions, but it doesn't guarantee it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "-qhr3zXqj6CA",
    "outputId": "5eaba1da-74f9-4b95-eb43-3b0adb6129a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.figure.Figure at 0x7f934870e518>,\n",
       " <matplotlib.axes._subplots.AxesSubplot at 0x7f934874b6d8>)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUnlKA4BSSuUpDQBKKZWn/j/epVB7HCKk+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f934870e518>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# START TODO ##################\n",
    "hpvis.losses_over_time(all_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i31xlKxXj6CC"
   },
   "source": [
    "Remember that BOHB uses a model after some time to improve the configuation sampling. We can check, if the BO-sampled configurations work better than the random-sampled.\n",
    "\n",
    "**Task:** Plot loss histograms for all budgets only with BO-sampled and only with random-sampled configurations (6 histograms). Is the BO-sampling useful?\n",
    "\n",
    "**Answer:** Yes. As we can see from the histogram, the values are differentiated more, i.e. the models with high performance and low performance have larger difference in frequency. So it produces more meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "colab_type": "code",
    "id": "fJL37hgYj6CC",
    "outputId": "429e795e-0eb6-45e1-d192-425501dd7442"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.figure.Figure at 0x7f93487e3278>,\n",
       " array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f933e854518>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f93487e3198>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f93487eaeb8>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f9348744518>],\n",
       "        [<matplotlib.axes._subplots.AxesSubplot object at 0x7f93486548d0>,\n",
       "         <matplotlib.axes._subplots.AxesSubplot object at 0x7f93486016d8>]],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEjCAYAAAAVCvdtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f93487e3278>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# START TODO ##################\n",
    "hpvis.performance_histogram_model_vs_random(all_runs, id2conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "My4a0RKjj6CF"
   },
   "source": [
    "### Your Feedback on Exercise 6.2\n",
    "\n",
    "A very informative exercise to learn BOHB, glad that I did it. It took 4 - 6 hours to do it, such a short time to learn such a powerful toolbox, hopefully the next exercises will be also like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ-ZxA5RBWuC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercise06_hpo_bohb.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
